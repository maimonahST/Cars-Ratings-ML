{"cells":[{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","import seaborn as sns\n","import matplotlib.pylab as plt\n","from imblearn.over_sampling import SMOTE\n","#%matplotlib inline\n","\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn import preprocessing\n","from sklearn.preprocessing import OrdinalEncoder\n","from sklearn.impute import SimpleImputer\n","from sklearn.model_selection import StratifiedKFold\n","\n","from scipy.stats import norm\n","from scipy import stats\n","from sklearn.impute import KNNImputer\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import precision_recall_fscore_support as score\n","\n","#DS\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import GridSearchCV, cross_val_score\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import classification_report\n","\n","#KNN\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","#GradientBoostingClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","\n","#LogisticRegressionCV\n","from sklearn.linear_model import LogisticRegressionCV\n","\n","#RandomForestClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","\n","def warn(*args, **kwargs):\n","    pass\n","import warnings\n","warnings.warn = warn"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 1. Loading the dataset"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["#loading the data from CSV file \n","data=pd.read_csv('final_Binary.csv')\n","data.head()\n","\n","'''dataFeatures= ['Side Chest Airbag-Driver', 'Side Chest Airbag-Passenger',\n","       'AEB Vulnerable Road Users', 'Side Head Airbag-Driver',\n","       'Side Head Airbag-Passenger', 'Seatbelt Reminder-Passenger',\n","       'AEB Car-to-Car', 'Belt Loadlimiter-Rear', 'Belt Pretensioner-Rear',\n","       'Side Head Airbag-Rear', 'Lane Assist System', 'Seatbelt Reminder-Rear',\n","       'Safety Assist', 'Speed Assistance', 'Adult Occupant',\n","       'Centre Airbag-Driver', 'Child Occupant', 'Tested Model',\n","       'Isofix/i-Size-Passenger'] '''\n","\n","dataFeatures= ['Adult Occupant', 'Belt Pretensioner-Rear', 'Safety Assist',\n","       'Child Occupant', 'Class', 'Side Pelvis Airbag-Rear',\n","       'AEB Vulnerable Road Users']\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/plain":["Index(['Adult Occupant', 'Belt Pretensioner-Rear', 'Safety Assist',\n","       'Child Occupant', 'Class', 'Side Pelvis Airbag-Rear',\n","       'AEB Vulnerable Road Users', 'Rate'],\n","      dtype='object')"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# print the columns in the dataset\n","data.columns"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["#Data presentation \n","\n","sns.countplot(data[\"Rate\"])\n","plt.xlabel(\"Rate of safety\")\n","plt.ylabel(\"Count of rates\")\n","plt.title(\"Car satety ratings\")\n","plt.show()"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":["'# Create a MinMaxScaler object for numrical data\\nscaler = MinMaxScaler()\\n\\n# Scaling the raw input features \\nfeature_cols=data.columns[:-1]\\nX= scaler.fit_transform(data[feature_cols])\\n\\nprint(f\"The range of feature inputs are within {X.min()} to {X.max()}\")'"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["'''# Create a MinMaxScaler object for numrical data\n","scaler = MinMaxScaler()\n","\n","# Scaling the raw input features \n","feature_cols=data.columns[:-1]\n","X= scaler.fit_transform(data[feature_cols])\n","\n","print(f\"The range of feature inputs are within {X.min()} to {X.max()}\")'''"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 2. Split the dataset "]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Training dataset shape, X_train: (217, 7), y_train: (217,)\n","Testing dataset shape, X_test: (94, 7), y_test: (94,)\n"]}],"source":["\n","from sklearn.model_selection import StratifiedShuffleSplit\n","\n","feature_cols=data.columns[:-1]\n","# Get the split indexes\n","strat_shuf_split = StratifiedShuffleSplit(n_splits=1, \n","                                          test_size=0.3, random_state=0)\n","\n","train_idx, test_idx = next(strat_shuf_split.split(data[feature_cols], data['Rate']))\n","\n","# Create the dataframes\n","\n","\n","X_train = data.loc[train_idx, dataFeatures]\n","y_train = data.loc[train_idx, 'Rate']\n","\n","X_test  = data.loc[test_idx, dataFeatures]\n","y_test  = data.loc[test_idx, 'Rate']\n","\n","print(f\"Training dataset shape, X_train: {X_train.shape}, y_train: {y_train.shape}\")\n","print(f\"Testing dataset shape, X_test: {X_test.shape}, y_test: {y_test.shape}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 3. Smoot "]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["X= data.loc[:,dataFeatures]\n","y= data.loc[:,[\"Rate\"]]\n","\n","def splitSmote (model):\n","    # Initialize the Stratified K-fold Cross-validator with 5 splits\n","    sk=StratifiedKFold(n_splits=5)\n","\n","    # Initialize the array to store the accuracy scores\n","    tr_accuracy_scores = []\n","    tst_accuracy_scores = []\n","\n","    # Perform cross-validation\n","    for train_index, test_index in sk.split(X, y):\n","        # Split the data into training and test sets\n","        x_train_fold, x_test_fold = X.loc[train_index,:], X.loc[test_index,:]\n","        y_train_fold, y_test_fold = y.loc[train_index,:], y.loc[test_index,:]\n","\n","        smote = SMOTE(sampling_strategy='minority')\n","        # smote = SMOTE(sampling_strategy=0.5)\n","        x_sm, y_sm = smote.fit_resample(x_train_fold, y_train_fold)\n","        #Fit the model to the training data\n","        model.fit(x_sm, np.ravel(y_sm))\n","        # Make predictions on the test data\n","        yTrain_pred = model.predict(x_train_fold)\n","        yTest_pred = model.predict(x_test_fold)\n","\n","        # Calculate the accuracy score and append it to the list\n","        tr_accuracy_scores.append(accuracy_score(y_train_fold, yTrain_pred))\n","        tst_accuracy_scores.append(accuracy_score(y_test_fold, yTest_pred))\n","        #Data presentation \n","\n","        print(\"before smote\")\n","        sns.countplot(y_train_fold[\"Rate\"])\n","        plt.xlabel(\"Rate of safety\")\n","        plt.ylabel(\"Count of rates\")\n","        plt.title(\"Car satety ratings\")\n","        plt.show()\n","\n","        print(\"after smote\")\n","        sns.countplot(y_sm[\"Rate\"])\n","        plt.xlabel(\"Rate of safety\")\n","        plt.ylabel(\"Count of rates\")\n","        plt.title(\"Car satety ratings\")\n","        plt.show()\n","\n","\n","\n","\n","    # Print the accuracy scores for each fold\n","    print(\"Accuracy scores for each training fold: \", tr_accuracy_scores)\n","    print(\"Accuracy scores for each testing fold: \", tst_accuracy_scores)\n","\n","    # Calculate the mean accuracy scores\n","    print(\"Traning Mean accuracy score: \", np.mean(tr_accuracy_scores))\n","    print(\"Testing Mean accuracy score: \", np.mean(tst_accuracy_scores))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 4. Data normalization "]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":["1    0.718894\n","0    0.281106\n","Name: Rate, dtype: float64"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["y_train.value_counts(normalize=True)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/plain":["1    0.723404\n","0    0.276596\n","Name: Rate, dtype: float64"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["y_test.value_counts(normalize=True)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 4. Models "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 4.1 Decision Tree "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["before optimization"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["before smote\n"]},{"ename":"KeyError","evalue":"0","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2263\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2273\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 0","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 17\u001b[0m\n\u001b[1;32m      5\u001b[0m clf\u001b[39m=\u001b[39mDecisionTreeClassifier(random_state\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[39m#clf2=clf.fit(X_train,y_train)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m# accuracy = accuracy_score(y_test, clf2.predict(X_test))\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39m# metrics.append(pd.Series({'precision':precision, 'recall':recall,'fscore':fscore, 'accuracy':accuracy}))\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m splitSmote (clf)\n","Cell \u001b[0;32mIn[8], line 33\u001b[0m, in \u001b[0;36msplitSmote\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39m#Data presentation \u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mbefore smote\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m sns\u001b[39m.\u001b[39;49mcountplot(y_train_fold[\u001b[39m\"\u001b[39;49m\u001b[39mRate\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m     34\u001b[0m plt\u001b[39m.\u001b[39mxlabel(\u001b[39m\"\u001b[39m\u001b[39mRate of safety\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m plt\u001b[39m.\u001b[39mylabel(\u001b[39m\"\u001b[39m\u001b[39mCount of rates\u001b[39m\u001b[39m\"\u001b[39m)\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/seaborn/categorical.py:2943\u001b[0m, in \u001b[0;36mcountplot\u001b[0;34m(data, x, y, hue, order, hue_order, orient, color, palette, saturation, width, dodge, ax, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m \u001b[39melif\u001b[39;00m x \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2941\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot pass values for both `x` and `y`\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 2943\u001b[0m plotter \u001b[39m=\u001b[39m _CountPlotter(\n\u001b[1;32m   2944\u001b[0m     x, y, hue, data, order, hue_order,\n\u001b[1;32m   2945\u001b[0m     estimator, errorbar, n_boot, units, seed,\n\u001b[1;32m   2946\u001b[0m     orient, color, palette, saturation,\n\u001b[1;32m   2947\u001b[0m     width, errcolor, errwidth, capsize, dodge\n\u001b[1;32m   2948\u001b[0m )\n\u001b[1;32m   2950\u001b[0m plotter\u001b[39m.\u001b[39mvalue_label \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcount\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2952\u001b[0m \u001b[39mif\u001b[39;00m ax \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/seaborn/categorical.py:1530\u001b[0m, in \u001b[0;36m_BarPlotter.__init__\u001b[0;34m(self, x, y, hue, data, order, hue_order, estimator, errorbar, n_boot, units, seed, orient, color, palette, saturation, width, errcolor, errwidth, capsize, dodge)\u001b[0m\n\u001b[1;32m   1525\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, x, y, hue, data, order, hue_order,\n\u001b[1;32m   1526\u001b[0m              estimator, errorbar, n_boot, units, seed,\n\u001b[1;32m   1527\u001b[0m              orient, color, palette, saturation, width,\n\u001b[1;32m   1528\u001b[0m              errcolor, errwidth, capsize, dodge):\n\u001b[1;32m   1529\u001b[0m     \u001b[39m\"\"\"Initialize the plotter.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1530\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mestablish_variables(x, y, hue, data, orient,\n\u001b[1;32m   1531\u001b[0m                              order, hue_order, units)\n\u001b[1;32m   1532\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestablish_colors(color, palette, saturation)\n\u001b[1;32m   1533\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimate_statistic(estimator, errorbar, n_boot, seed)\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/seaborn/categorical.py:486\u001b[0m, in \u001b[0;36m_CategoricalPlotter.establish_variables\u001b[0;34m(self, x, y, hue, data, orient, order, hue_order, units)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(data, \u001b[39m\"\u001b[39m\u001b[39mshape\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    485\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 486\u001b[0m         \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39misscalar(data[\u001b[39m0\u001b[39;49m]):\n\u001b[1;32m    487\u001b[0m             plot_data \u001b[39m=\u001b[39m [data]\n\u001b[1;32m    488\u001b[0m         \u001b[39melse\u001b[39;00m:\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/series.py:981\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    978\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[key]\n\u001b[1;32m    980\u001b[0m \u001b[39melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m--> 981\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_value(key)\n\u001b[1;32m    983\u001b[0m \u001b[39mif\u001b[39;00m is_hashable(key):\n\u001b[1;32m    984\u001b[0m     \u001b[39m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[1;32m    985\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    986\u001b[0m         \u001b[39m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/series.py:1089\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1086\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[label]\n\u001b[1;32m   1088\u001b[0m \u001b[39m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1089\u001b[0m loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mget_loc(label)\n\u001b[1;32m   1090\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39m_get_values_for_loc(\u001b[39mself\u001b[39m, loc, label)\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n","\u001b[0;31mKeyError\u001b[0m: 0"]}],"source":["# DecisionTreeClassifier\n","#importing the classfier\n","metrics=[]\n","\n","clf=DecisionTreeClassifier(random_state=0)\n","#clf2=clf.fit(X_train,y_train)\n","\n","\n","#y_pred.append(pd.Series(clf2.predict(X_test), name='DecisionTreeClassifier'))\n","# Preciision, recall, f-score from the multi-class support function\n","\n","# precision, recall, fscore, _ = score(y_test, clf2.predict(X_test), average='weighted')\n","# accuracy = accuracy_score(y_test, clf2.predict(X_test))\n","# metrics.append(pd.Series({'precision':precision, 'recall':recall,'fscore':fscore, 'accuracy':accuracy}))\n","\n","\n","splitSmote (clf)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["After optimization"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# DecisionTree opt\n","\n","\n","#optimization\n","param_grid={\n","\"max_depth\":[2,4,6],\n","\"min_samples_split\":[2,5,10],\n","\"min_samples_leaf\":[1,2,4]}\n","grid_search= GridSearchCV(estimator=clf,param_grid=param_grid,cv=5)\n","\n","# grid_search.fit(X_train,y_train)\n","# print(\"Best hyper-param: \",grid_search.best_params_ )\n","# print(\"Best estimator: \",grid_search.best_estimator_ )\n","# print(\"Best score: \",grid_search.best_score_ )\n","\n","\n","# #precision, recall, fscore, _ = score(y_test, grid_search.predict(X_test), average='weighted')\n","# accuracy = accuracy_score(y_test, grid_search.predict(X_test))\n","# metrics.append(pd.Series({'precision':precision, 'recall':recall,'fscore':fscore, 'accuracy':accuracy}))\n","\n","\n","#splitSmote (kCls)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["grid_search\n","before smote\n"]},{"ename":"KeyError","evalue":"0","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2263\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2273\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 0","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[0;32mIn[13], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#DT aftar opt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mgrid_search\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m splitSmote(grid_search)\n","Cell \u001b[0;32mIn[8], line 33\u001b[0m, in \u001b[0;36msplitSmote\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39m#Data presentation \u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mbefore smote\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m sns\u001b[39m.\u001b[39;49mcountplot(y_train_fold[\u001b[39m\"\u001b[39;49m\u001b[39mRate\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m     34\u001b[0m plt\u001b[39m.\u001b[39mxlabel(\u001b[39m\"\u001b[39m\u001b[39mRate of safety\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m plt\u001b[39m.\u001b[39mylabel(\u001b[39m\"\u001b[39m\u001b[39mCount of rates\u001b[39m\u001b[39m\"\u001b[39m)\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/seaborn/categorical.py:2943\u001b[0m, in \u001b[0;36mcountplot\u001b[0;34m(data, x, y, hue, order, hue_order, orient, color, palette, saturation, width, dodge, ax, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m \u001b[39melif\u001b[39;00m x \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2941\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot pass values for both `x` and `y`\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 2943\u001b[0m plotter \u001b[39m=\u001b[39m _CountPlotter(\n\u001b[1;32m   2944\u001b[0m     x, y, hue, data, order, hue_order,\n\u001b[1;32m   2945\u001b[0m     estimator, errorbar, n_boot, units, seed,\n\u001b[1;32m   2946\u001b[0m     orient, color, palette, saturation,\n\u001b[1;32m   2947\u001b[0m     width, errcolor, errwidth, capsize, dodge\n\u001b[1;32m   2948\u001b[0m )\n\u001b[1;32m   2950\u001b[0m plotter\u001b[39m.\u001b[39mvalue_label \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcount\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2952\u001b[0m \u001b[39mif\u001b[39;00m ax \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/seaborn/categorical.py:1530\u001b[0m, in \u001b[0;36m_BarPlotter.__init__\u001b[0;34m(self, x, y, hue, data, order, hue_order, estimator, errorbar, n_boot, units, seed, orient, color, palette, saturation, width, errcolor, errwidth, capsize, dodge)\u001b[0m\n\u001b[1;32m   1525\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, x, y, hue, data, order, hue_order,\n\u001b[1;32m   1526\u001b[0m              estimator, errorbar, n_boot, units, seed,\n\u001b[1;32m   1527\u001b[0m              orient, color, palette, saturation, width,\n\u001b[1;32m   1528\u001b[0m              errcolor, errwidth, capsize, dodge):\n\u001b[1;32m   1529\u001b[0m     \u001b[39m\"\"\"Initialize the plotter.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1530\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mestablish_variables(x, y, hue, data, orient,\n\u001b[1;32m   1531\u001b[0m                              order, hue_order, units)\n\u001b[1;32m   1532\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestablish_colors(color, palette, saturation)\n\u001b[1;32m   1533\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimate_statistic(estimator, errorbar, n_boot, seed)\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/seaborn/categorical.py:486\u001b[0m, in \u001b[0;36m_CategoricalPlotter.establish_variables\u001b[0;34m(self, x, y, hue, data, orient, order, hue_order, units)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(data, \u001b[39m\"\u001b[39m\u001b[39mshape\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    485\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 486\u001b[0m         \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39misscalar(data[\u001b[39m0\u001b[39;49m]):\n\u001b[1;32m    487\u001b[0m             plot_data \u001b[39m=\u001b[39m [data]\n\u001b[1;32m    488\u001b[0m         \u001b[39melse\u001b[39;00m:\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/series.py:981\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    978\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[key]\n\u001b[1;32m    980\u001b[0m \u001b[39melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m--> 981\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_value(key)\n\u001b[1;32m    983\u001b[0m \u001b[39mif\u001b[39;00m is_hashable(key):\n\u001b[1;32m    984\u001b[0m     \u001b[39m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[1;32m    985\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    986\u001b[0m         \u001b[39m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/series.py:1089\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1086\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[label]\n\u001b[1;32m   1088\u001b[0m \u001b[39m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1089\u001b[0m loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mget_loc(label)\n\u001b[1;32m   1090\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39m_get_values_for_loc(\u001b[39mself\u001b[39m, loc, label)\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n","\u001b[0;31mKeyError\u001b[0m: 0"]}],"source":["#DT aftar opt\n","\n","print(\"grid_search\")\n","\n","splitSmote(grid_search)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 4.2 KNN"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Before optimization"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["before smote\n"]},{"ename":"KeyError","evalue":"0","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2263\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2273\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 0","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[0;32mIn[14], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# KNN\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#importing the classfier\u001b[39;00m\n\u001b[1;32m      4\u001b[0m kCls\u001b[39m=\u001b[39mKNeighborsClassifier()\n\u001b[0;32m----> 5\u001b[0m splitSmote (kCls)\n","Cell \u001b[0;32mIn[8], line 33\u001b[0m, in \u001b[0;36msplitSmote\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39m#Data presentation \u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mbefore smote\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m sns\u001b[39m.\u001b[39;49mcountplot(y_train_fold[\u001b[39m\"\u001b[39;49m\u001b[39mRate\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m     34\u001b[0m plt\u001b[39m.\u001b[39mxlabel(\u001b[39m\"\u001b[39m\u001b[39mRate of safety\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m plt\u001b[39m.\u001b[39mylabel(\u001b[39m\"\u001b[39m\u001b[39mCount of rates\u001b[39m\u001b[39m\"\u001b[39m)\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/seaborn/categorical.py:2943\u001b[0m, in \u001b[0;36mcountplot\u001b[0;34m(data, x, y, hue, order, hue_order, orient, color, palette, saturation, width, dodge, ax, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m \u001b[39melif\u001b[39;00m x \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2941\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot pass values for both `x` and `y`\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 2943\u001b[0m plotter \u001b[39m=\u001b[39m _CountPlotter(\n\u001b[1;32m   2944\u001b[0m     x, y, hue, data, order, hue_order,\n\u001b[1;32m   2945\u001b[0m     estimator, errorbar, n_boot, units, seed,\n\u001b[1;32m   2946\u001b[0m     orient, color, palette, saturation,\n\u001b[1;32m   2947\u001b[0m     width, errcolor, errwidth, capsize, dodge\n\u001b[1;32m   2948\u001b[0m )\n\u001b[1;32m   2950\u001b[0m plotter\u001b[39m.\u001b[39mvalue_label \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcount\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2952\u001b[0m \u001b[39mif\u001b[39;00m ax \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/seaborn/categorical.py:1530\u001b[0m, in \u001b[0;36m_BarPlotter.__init__\u001b[0;34m(self, x, y, hue, data, order, hue_order, estimator, errorbar, n_boot, units, seed, orient, color, palette, saturation, width, errcolor, errwidth, capsize, dodge)\u001b[0m\n\u001b[1;32m   1525\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, x, y, hue, data, order, hue_order,\n\u001b[1;32m   1526\u001b[0m              estimator, errorbar, n_boot, units, seed,\n\u001b[1;32m   1527\u001b[0m              orient, color, palette, saturation, width,\n\u001b[1;32m   1528\u001b[0m              errcolor, errwidth, capsize, dodge):\n\u001b[1;32m   1529\u001b[0m     \u001b[39m\"\"\"Initialize the plotter.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1530\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mestablish_variables(x, y, hue, data, orient,\n\u001b[1;32m   1531\u001b[0m                              order, hue_order, units)\n\u001b[1;32m   1532\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestablish_colors(color, palette, saturation)\n\u001b[1;32m   1533\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimate_statistic(estimator, errorbar, n_boot, seed)\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/seaborn/categorical.py:486\u001b[0m, in \u001b[0;36m_CategoricalPlotter.establish_variables\u001b[0;34m(self, x, y, hue, data, orient, order, hue_order, units)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(data, \u001b[39m\"\u001b[39m\u001b[39mshape\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    485\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 486\u001b[0m         \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39misscalar(data[\u001b[39m0\u001b[39;49m]):\n\u001b[1;32m    487\u001b[0m             plot_data \u001b[39m=\u001b[39m [data]\n\u001b[1;32m    488\u001b[0m         \u001b[39melse\u001b[39;00m:\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/series.py:981\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    978\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[key]\n\u001b[1;32m    980\u001b[0m \u001b[39melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m--> 981\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_value(key)\n\u001b[1;32m    983\u001b[0m \u001b[39mif\u001b[39;00m is_hashable(key):\n\u001b[1;32m    984\u001b[0m     \u001b[39m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[1;32m    985\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    986\u001b[0m         \u001b[39m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/series.py:1089\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1086\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[label]\n\u001b[1;32m   1088\u001b[0m \u001b[39m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1089\u001b[0m loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mget_loc(label)\n\u001b[1;32m   1090\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39m_get_values_for_loc(\u001b[39mself\u001b[39m, loc, label)\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n","\u001b[0;31mKeyError\u001b[0m: 0"]}],"source":["# KNN\n","#importing the classfier\n","\n","kCls=KNeighborsClassifier()\n","splitSmote (kCls)\n","#kCls.fit(X_train,y_train)\n","\n","#y_pred.append(pd.Series(kCls.predict(X_test), name='KNeighborsClassifier'))\n","\n","#precision, recall, fscore, _ = score(y_test, kCls.predict(X_test), average='weighted')\n","#accuracy = accuracy_score(y_test, kCls.predict(X_test))\n","#metrics.append(pd.Series({'precision':precision, 'recall':recall,'fscore':fscore, 'accuracy':accuracy}))\n","\n","#kCls=KNeighborsClassifier(n_neighbors=9)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["After optimization"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["#KNN after Optimization \n","\n","kCls=KNeighborsClassifier(algorithm=\"brute\")\n","splitSmote (kCls)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 4.3 GradientBoosting"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Before optimization"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["before smote\n"]},{"ename":"KeyError","evalue":"0","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2263\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2273\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 0","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[0;32mIn[16], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# GradientBoostingClassifier\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#importing the classfier\u001b[39;00m\n\u001b[1;32m      4\u001b[0m gb_clf \u001b[39m=\u001b[39m GradientBoostingClassifier()\n\u001b[0;32m----> 5\u001b[0m splitSmote(gb_clf)\n","Cell \u001b[0;32mIn[8], line 33\u001b[0m, in \u001b[0;36msplitSmote\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39m#Data presentation \u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mbefore smote\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m sns\u001b[39m.\u001b[39;49mcountplot(y_train_fold[\u001b[39m\"\u001b[39;49m\u001b[39mRate\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m     34\u001b[0m plt\u001b[39m.\u001b[39mxlabel(\u001b[39m\"\u001b[39m\u001b[39mRate of safety\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m plt\u001b[39m.\u001b[39mylabel(\u001b[39m\"\u001b[39m\u001b[39mCount of rates\u001b[39m\u001b[39m\"\u001b[39m)\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/seaborn/categorical.py:2943\u001b[0m, in \u001b[0;36mcountplot\u001b[0;34m(data, x, y, hue, order, hue_order, orient, color, palette, saturation, width, dodge, ax, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m \u001b[39melif\u001b[39;00m x \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2941\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot pass values for both `x` and `y`\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 2943\u001b[0m plotter \u001b[39m=\u001b[39m _CountPlotter(\n\u001b[1;32m   2944\u001b[0m     x, y, hue, data, order, hue_order,\n\u001b[1;32m   2945\u001b[0m     estimator, errorbar, n_boot, units, seed,\n\u001b[1;32m   2946\u001b[0m     orient, color, palette, saturation,\n\u001b[1;32m   2947\u001b[0m     width, errcolor, errwidth, capsize, dodge\n\u001b[1;32m   2948\u001b[0m )\n\u001b[1;32m   2950\u001b[0m plotter\u001b[39m.\u001b[39mvalue_label \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcount\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2952\u001b[0m \u001b[39mif\u001b[39;00m ax \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/seaborn/categorical.py:1530\u001b[0m, in \u001b[0;36m_BarPlotter.__init__\u001b[0;34m(self, x, y, hue, data, order, hue_order, estimator, errorbar, n_boot, units, seed, orient, color, palette, saturation, width, errcolor, errwidth, capsize, dodge)\u001b[0m\n\u001b[1;32m   1525\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, x, y, hue, data, order, hue_order,\n\u001b[1;32m   1526\u001b[0m              estimator, errorbar, n_boot, units, seed,\n\u001b[1;32m   1527\u001b[0m              orient, color, palette, saturation, width,\n\u001b[1;32m   1528\u001b[0m              errcolor, errwidth, capsize, dodge):\n\u001b[1;32m   1529\u001b[0m     \u001b[39m\"\"\"Initialize the plotter.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1530\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mestablish_variables(x, y, hue, data, orient,\n\u001b[1;32m   1531\u001b[0m                              order, hue_order, units)\n\u001b[1;32m   1532\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestablish_colors(color, palette, saturation)\n\u001b[1;32m   1533\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimate_statistic(estimator, errorbar, n_boot, seed)\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/seaborn/categorical.py:486\u001b[0m, in \u001b[0;36m_CategoricalPlotter.establish_variables\u001b[0;34m(self, x, y, hue, data, orient, order, hue_order, units)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(data, \u001b[39m\"\u001b[39m\u001b[39mshape\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    485\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 486\u001b[0m         \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39misscalar(data[\u001b[39m0\u001b[39;49m]):\n\u001b[1;32m    487\u001b[0m             plot_data \u001b[39m=\u001b[39m [data]\n\u001b[1;32m    488\u001b[0m         \u001b[39melse\u001b[39;00m:\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/series.py:981\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    978\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[key]\n\u001b[1;32m    980\u001b[0m \u001b[39melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m--> 981\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_value(key)\n\u001b[1;32m    983\u001b[0m \u001b[39mif\u001b[39;00m is_hashable(key):\n\u001b[1;32m    984\u001b[0m     \u001b[39m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[1;32m    985\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    986\u001b[0m         \u001b[39m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/series.py:1089\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1086\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[label]\n\u001b[1;32m   1088\u001b[0m \u001b[39m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1089\u001b[0m loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mget_loc(label)\n\u001b[1;32m   1090\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39m_get_values_for_loc(\u001b[39mself\u001b[39m, loc, label)\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n","\u001b[0;31mKeyError\u001b[0m: 0"]}],"source":["# GradientBoostingClassifier\n","#importing the classfier\n","\n","gb_clf = GradientBoostingClassifier()\n","splitSmote(gb_clf)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["After optimization"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[17], line 16\u001b[0m\n\u001b[1;32m      3\u001b[0m parameters \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mn_estimators\u001b[39m\u001b[39m'\u001b[39m : [\u001b[39m20\u001b[39m,\u001b[39m50\u001b[39m,\u001b[39m70\u001b[39m,\u001b[39m100\u001b[39m], \n\u001b[1;32m      4\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39mlearning_rate\u001b[39m\u001b[39m'\u001b[39m : [\u001b[39m0.01\u001b[39m, \u001b[39m0.02\u001b[39m, \u001b[39m0.03\u001b[39m, \u001b[39m0.04\u001b[39m], \n\u001b[1;32m      5\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39mmax_depth\u001b[39m\u001b[39m'\u001b[39m : [\u001b[39m4\u001b[39m,\u001b[39m6\u001b[39m,\u001b[39m8\u001b[39m,\u001b[39m10\u001b[39m],\n\u001b[1;32m      6\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39msubsample\u001b[39m\u001b[39m'\u001b[39m : [\u001b[39m0.9\u001b[39m, \u001b[39m0.7\u001b[39m ,\u001b[39m0.5\u001b[39m , \u001b[39m0.2\u001b[39m ],\n\u001b[1;32m      7\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39mmax_features\u001b[39m\u001b[39m'\u001b[39m :[\u001b[39m3\u001b[39m,\u001b[39m5\u001b[39m,\u001b[39m7\u001b[39m]\n\u001b[1;32m      8\u001b[0m                 }\n\u001b[1;32m     10\u001b[0m grid \u001b[39m=\u001b[39m GridSearchCV(estimator\u001b[39m=\u001b[39m gb_clf, \n\u001b[1;32m     11\u001b[0m                     param_grid\u001b[39m=\u001b[39m parameters, \n\u001b[1;32m     12\u001b[0m                     cv\u001b[39m=\u001b[39m \u001b[39m2\u001b[39m,\n\u001b[1;32m     13\u001b[0m                     n_jobs\u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m grid\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[1;32m     17\u001b[0m splitSmote(grid)\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    868\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[1;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    870\u001b[0m     )\n\u001b[1;32m    872\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[0;32m--> 874\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[1;32m    876\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    878\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1386\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1387\u001b[0m     \u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1388\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    814\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m    815\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[1;32m    818\u001b[0m         )\n\u001b[1;32m    819\u001b[0m     )\n\u001b[0;32m--> 821\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    822\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m         clone(base_estimator),\n\u001b[1;32m    824\u001b[0m         X,\n\u001b[1;32m    825\u001b[0m         y,\n\u001b[1;32m    826\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[1;32m    827\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[1;32m    828\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[1;32m    829\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[1;32m    830\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[1;32m    831\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[1;32m    832\u001b[0m     )\n\u001b[1;32m    833\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[1;32m    834\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[1;32m    835\u001b[0m     )\n\u001b[1;32m    836\u001b[0m )\n\u001b[1;32m    838\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    839\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    843\u001b[0m     )\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/joblib/parallel.py:1061\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1058\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1061\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[1;32m   1062\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/joblib/parallel.py:938\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    936\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    937\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 938\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[1;32m    939\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    940\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/joblib/_parallel_backends.py:542\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[39m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[39mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 542\u001b[0m     \u001b[39mreturn\u001b[39;00m future\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    543\u001b[0m \u001b[39mexcept\u001b[39;00m CfTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    544\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/lib/python3.10/concurrent/futures/_base.py:453\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m    451\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_result()\n\u001b[0;32m--> 453\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_condition\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    456\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n","File \u001b[0;32m~/anaconda3/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# # optimizing the GradientBoostingClassifier using RandomizedSearchCV\n","from sklearn.model_selection import GridSearchCV\n","parameters = {'n_estimators' : [20,50,70,100], \n","                'learning_rate' : [0.01, 0.02, 0.03, 0.04], \n","                'max_depth' : [4,6,8,10],\n","                'subsample' : [0.9, 0.7 ,0.5 , 0.2 ],\n","                'max_features' :[3,5,7]\n","                }\n","\n","grid = GridSearchCV(estimator= gb_clf, \n","                    param_grid= parameters, \n","                    cv= 2,\n","                    n_jobs= -1)\n","\n","\n","grid.fit(X_train, y_train)\n","splitSmote(grid)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["GradientBoostingClassifier(learning_rate=0.03, max_depth=10, max_features=7,\n","                           n_estimators=50, subsample=0.5)\n","0.9555555555555555\n","{'learning_rate': 0.03, 'max_depth': 10, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.5}\n","{'mean_fit_time': array([0.03366113, 0.03366113, 0.03366137, 0.03282058, 0.08698392,\n","       0.09165919, 0.08392   , 0.10865557, 0.1333636 , 0.12502873,\n","       0.18311083, 0.1080035 , 0.22033167, 0.21404874, 0.21664703,\n","       0.15798306, 0.04161191, 0.03761387, 0.03843236, 0.03364956,\n","       0.08798349, 0.08682549, 0.1039784 , 0.074736  , 0.1290133 ,\n","       0.1586473 , 0.1462847 , 0.13832605, 0.17935586, 0.18728554,\n","       0.17457378, 0.23766184, 0.03341353, 0.04157186, 0.06230128,\n","       0.03730798, 0.10393441, 0.07867301, 0.10792136, 0.07870579,\n","       0.15336263, 0.10895145, 0.11232948, 0.16425502, 0.16823101,\n","       0.18632019, 0.27920461, 0.24617243, 0.06559062, 0.07358515,\n","       0.07994664, 0.04796791, 0.21806026, 0.16100621, 0.18208361,\n","       0.11635172, 0.33365035, 0.20930088, 0.2936765 , 0.16133726,\n","       0.46682811, 0.31423879, 0.37311065, 0.32913756, 0.04796863,\n","       0.05596054, 0.05697024, 0.05596256, 0.13759851, 0.15953898,\n","       0.20205414, 0.17357409, 0.17336667, 0.17872131, 0.23808062,\n","       0.15910494, 0.23606157, 0.31567705, 0.31272757, 0.2549051 ,\n","       0.0421195 , 0.03798652, 0.04185724, 0.03333521, 0.09206092,\n","       0.11197579, 0.09998369, 0.11666429, 0.13729262, 0.1562351 ,\n","       0.15564716, 0.13498235, 0.22494948, 0.23294449, 0.28529453,\n","       0.2498095 , 0.05074   , 0.0700264 , 0.06919253, 0.04255295,\n","       0.22724152, 0.2026211 , 0.25596726, 0.1325413 , 0.39362884,\n","       0.29257214, 0.32566929, 0.17028868, 0.41592586, 0.56798816,\n","       0.44889951, 0.27990162, 0.0456773 , 0.05773878, 0.05666065,\n","       0.04796827, 0.14721322, 0.22834516, 0.23296058, 0.1192323 ,\n","       0.18508983, 0.2087388 , 0.20835912, 0.15977716, 0.38200867,\n","       0.34884501, 0.38819861, 0.24671352, 0.04596806, 0.03725648,\n","       0.03741527, 0.06531906, 0.17959023, 0.1292758 , 0.14764738,\n","       0.15563226, 0.14363945, 0.16586196, 0.19605041, 0.17953753,\n","       0.32462323, 0.31263173, 0.36382401, 0.23032773, 0.0733037 ,\n","       0.07281065, 0.10470331, 0.05996001, 0.1833663 , 0.22329926,\n","       0.25667262, 0.15061522, 0.29152012, 0.47953129, 0.33500135,\n","       0.20122242, 0.65353525, 0.72885275, 0.46811604, 0.3222816 ,\n","       0.06182504, 0.06274188, 0.08794153, 0.04396987, 0.12854004,\n","       0.15892291, 0.21256518, 0.12941909, 0.18798518, 0.30528355,\n","       0.2352246 , 0.16917896, 0.28329599, 0.45820463, 0.36225772,\n","       0.32094073, 0.03331625, 0.04126823, 0.04526484, 0.05401075,\n","       0.11596906, 0.13398707, 0.12455058, 0.16298354, 0.1449182 ,\n","       0.22902417, 0.19169807, 0.15827608, 0.22459972, 0.26660383,\n","       0.29195917, 0.29595661, 0.05408144, 0.03737462, 0.03330541,\n","       0.04459441, 0.08392513, 0.11202562, 0.08326781, 0.09534228,\n","       0.14130521, 0.12066734, 0.128631  , 0.14110029, 0.22243679,\n","       0.18366802, 0.17643857, 0.17310846, 0.0366993 , 0.04785943,\n","       0.03622723, 0.03272843, 0.1235044 , 0.09134722, 0.09956324,\n","       0.07866716, 0.1224612 , 0.12910652, 0.12151384, 0.11311734,\n","       0.24429405, 0.19564533, 0.1817857 , 0.17778635, 0.03703988,\n","       0.03733861, 0.03671062, 0.02639985, 0.07959366, 0.0858444 ,\n","       0.12128294, 0.07731283, 0.11259043, 0.15465486, 0.11326873,\n","       0.13643169, 0.15999103, 0.1726588 , 0.20228255, 0.20760632,\n","       0.05067241, 0.05436361, 0.08360338, 0.05433106, 0.15398228,\n","       0.14161849, 0.14060342, 0.10024655, 0.25822783, 0.25796139,\n","       0.20427942, 0.19229996, 0.28730977, 0.29130745, 0.34202373,\n","       0.20497036, 0.04596603, 0.06197464, 0.03799117, 0.04999471,\n","       0.10294473, 0.1290586 , 0.14638901, 0.1250664 , 0.17971194,\n","       0.176754  , 0.17416549, 0.16283584, 0.32538247, 0.27665234,\n","       0.25983453, 0.24732876, 0.06630814, 0.06616867, 0.03737688,\n","       0.05368328, 0.13066518, 0.13330817, 0.13330793, 0.12935877,\n","       0.14895844, 0.17794371, 0.21120977, 0.17794514, 0.2291882 ,\n","       0.24554706, 0.31247711, 0.22482646, 0.06665742, 0.06657052,\n","       0.09168863, 0.0580107 , 0.25973213, 0.23535073, 0.20737064,\n","       0.14194942, 0.28407061, 0.29606187, 0.27339888, 0.21314514,\n","       0.43316066, 0.39716959, 0.40192819, 0.26456773, 0.04478562,\n","       0.0457654 , 0.05811036, 0.04176962, 0.13722348, 0.15796494,\n","       0.16666448, 0.12931824, 0.20394218, 0.24595392, 0.23719323,\n","       0.1666292 , 0.27894139, 0.35843325, 0.32184029, 0.24233377,\n","       0.0376581 , 0.04996765, 0.04600739, 0.04167628, 0.09604943,\n","       0.1252197 , 0.12789643, 0.10827136, 0.14150465, 0.22123003,\n","       0.237679  , 0.16543889, 0.2540338 , 0.28272963, 0.31139386,\n","       0.2521168 , 0.06911922, 0.08140492, 0.08516729, 0.05399883,\n","       0.18302667, 0.26226389, 0.21694851, 0.14636767, 0.29404902,\n","       0.33064151, 0.29353762, 0.19556725, 0.5082531 , 0.63717914,\n","       0.45573556, 0.27856755, 0.04405439, 0.05675125, 0.06244707,\n","       0.0421164 , 0.15820718, 0.19190907, 0.17890394, 0.12920284,\n","       0.30437839, 0.47137034, 0.31302094, 0.22049952, 0.46707094,\n","       0.71534681, 0.5227443 , 0.34723067, 0.07897246, 0.08762157,\n","       0.05432022, 0.06194162, 0.14132631, 0.24625123, 0.20866704,\n","       0.22208381, 0.20856178, 0.24906337, 0.27014315, 0.28368211,\n","       0.28328419, 0.51226223, 0.47924352, 0.28481078, 0.03997171,\n","       0.05829883, 0.06631565, 0.03727353, 0.11223447, 0.12025571,\n","       0.14128625, 0.07562268, 0.12930083, 0.13798618, 0.17102659,\n","       0.12532938, 0.23802543, 0.26293242, 0.18371642, 0.17867291,\n","       0.03370297, 0.04636443, 0.03302813, 0.03770077, 0.11299002,\n","       0.09127748, 0.11258638, 0.09190905, 0.11260951, 0.12129283,\n","       0.14965093, 0.14566755, 0.22040033, 0.22860456, 0.17964697,\n","       0.20795059, 0.0373261 , 0.03733528, 0.04095566, 0.03328741,\n","       0.08730888, 0.09997964, 0.10863435, 0.07932866, 0.10332274,\n","       0.13299274, 0.1169734 , 0.14567125, 0.19957829, 0.23070276,\n","       0.1629442 , 0.18773127, 0.04568148, 0.05399537, 0.04999816,\n","       0.04134727, 0.18034363, 0.13609385, 0.12637031, 0.09979916,\n","       0.22500551, 0.27730286, 0.27519643, 0.21121478, 0.30392313,\n","       0.28402686, 0.27137303, 0.3123014 , 0.05401599, 0.0521574 ,\n","       0.06396198, 0.05278671, 0.13341427, 0.12476075, 0.15831137,\n","       0.12121868, 0.18326187, 0.26260221, 0.22066844, 0.14496148,\n","       0.29156148, 0.39182484, 0.38244987, 0.32913411, 0.04410672,\n","       0.05535614, 0.05449855, 0.03780258, 0.10791695, 0.16617775,\n","       0.11657357, 0.09963858, 0.14558589, 0.16288614, 0.20018315,\n","       0.14199305, 0.2329489 , 0.32557023, 0.38428628, 0.32729435,\n","       0.06265414, 0.06477618, 0.06664228, 0.04999077, 0.27861547,\n","       0.19528604, 0.18772817, 0.11829042, 0.30356574, 0.2752372 ,\n","       0.32720304, 0.16361284, 0.37575221, 0.47738111, 0.43740475,\n","       0.31179166, 0.03997326, 0.04397047, 0.05196595, 0.03997362,\n","       0.12791729, 0.16098297, 0.144328  , 0.14559996, 0.15759158,\n","       0.21355426, 0.2673099 , 0.16661382, 0.29162991, 0.42707276,\n","       0.42707312, 0.33513296, 0.03597546, 0.04397142, 0.05047846,\n","       0.03863883, 0.10172319, 0.11776042, 0.1140703 , 0.11748743,\n","       0.14546824, 0.21741998, 0.17267489, 0.1479013 , 0.28780615,\n","       0.33177936, 0.26782227, 0.21985364, 0.05596244, 0.06795418,\n","       0.07195318, 0.05596304, 0.22385073, 0.25982738, 0.22784805,\n","       0.15589595, 0.25974464, 0.3064121 , 0.27443361, 0.21795702,\n","       0.52974117, 0.45174885, 0.50778937, 0.31282604, 0.04619122,\n","       0.066957  , 0.04115272, 0.0413543 , 0.12099624, 0.1582706 ,\n","       0.22892988, 0.14559948, 0.19094193, 0.30494523, 0.21601248,\n","       0.18331981, 0.27173424, 0.46402287, 0.35363233, 0.2356801 ,\n","       0.04998338, 0.04868257, 0.04402483, 0.03197885, 0.13301742,\n","       0.12901962, 0.15300429, 0.14769232, 0.13191211, 0.19033456,\n","       0.24229991, 0.15435803, 0.27610469, 0.36173809, 0.30808246,\n","       0.2401284 , 0.04397082, 0.03197849, 0.03597689, 0.03197849,\n","       0.09223211, 0.08577013, 0.08976746, 0.07377744, 0.16846955,\n","       0.12449884, 0.12050128, 0.11650407, 0.21643722, 0.20444489,\n","       0.16447258, 0.2384938 , 0.03597665, 0.03597569, 0.03597629,\n","       0.05996001, 0.10513842, 0.0879426 , 0.08394408, 0.09475815,\n","       0.11678147, 0.11678314, 0.11678433, 0.18074107, 0.22471225,\n","       0.17298567, 0.24783528, 0.15189898, 0.03597605, 0.03997314,\n","       0.03597605, 0.03197861, 0.07594907, 0.10393059, 0.07994676,\n","       0.09593582, 0.10393047, 0.11592245, 0.15334177, 0.10453594,\n","       0.22325408, 0.16933215, 0.22929251, 0.16533458, 0.07587016,\n","       0.05188656, 0.05425489, 0.07195175, 0.15589607, 0.16788805,\n","       0.13590968, 0.10792816, 0.22535813, 0.18938243, 0.19737637,\n","       0.14540803, 0.27858019, 0.27458227, 0.25059855, 0.25059831,\n","       0.03723001, 0.07320929, 0.06921101, 0.06121647, 0.11192608,\n","       0.15589631, 0.15189862, 0.13990641, 0.17588305, 0.17588282,\n","       0.1798805 , 0.17588305, 0.37590241, 0.25982714, 0.30925405,\n","       0.21185899, 0.03597748, 0.05995989, 0.03597522, 0.03997326,\n","       0.09593523, 0.16135168, 0.16073132, 0.16534972, 0.12966907,\n","       0.19762361, 0.17012894, 0.15069175, 0.21884584, 0.23421335,\n","       0.28145623, 0.20986509, 0.05596185, 0.07802188, 0.06130838,\n","       0.05355036, 0.22401059, 0.22905493, 0.19187236, 0.12791574,\n","       0.30617976, 0.29230654, 0.32616591, 0.19824982, 0.38822436,\n","       0.4307121 , 0.42970562, 0.28308797, 0.04397058, 0.05196571,\n","       0.06720197, 0.05128908, 0.13001287, 0.15405977, 0.14580524,\n","       0.13963342, 0.18147635, 0.24868357, 0.2414366 , 0.16389394,\n","       0.29180574, 0.32378399, 0.29180539, 0.2198534 , 0.03197908,\n","       0.04397106, 0.04397023, 0.0399735 , 0.11992049, 0.12791514,\n","       0.12391758, 0.09993315, 0.17588353, 0.19986665, 0.22901487,\n","       0.1638912 , 0.2464962 , 0.34174597, 0.33374953, 0.27546847,\n","       0.05581796, 0.08990538, 0.07711625, 0.06145346, 0.18523932,\n","       0.2891705 , 0.28037715, 0.1397754 , 0.27901816, 0.36473727,\n","       0.39999342, 0.1801033 , 0.49748456, 0.70154333, 0.68201983,\n","       0.46976054, 0.04950774, 0.09787285, 0.07963741, 0.05393088,\n","       0.14563978, 0.21708918, 0.19172108, 0.23348737, 0.21607959,\n","       0.43237126, 0.39549959, 0.19217229, 0.44226515, 0.57138193,\n","       0.49984193, 0.28731775, 0.04970026, 0.07096064, 0.0623455 ,\n","       0.04971194, 0.10431087, 0.16260576, 0.13328338, 0.15854502,\n","       0.14933884, 0.20401883, 0.20176399, 0.16606462, 0.23468137,\n","       0.29138422, 0.30462325, 0.25196433]), 'std_fit_time': array([0.00000000e+00, 0.00000000e+00, 2.38418579e-07, 8.83638859e-03,\n","       3.99708748e-03, 8.67235661e-03, 0.00000000e+00, 6.69014454e-03,\n","       6.61730766e-04, 1.00266933e-03, 1.72257423e-04, 6.83188438e-04,\n","       3.71265411e-03, 1.12904310e-02, 1.59612894e-02, 1.24955177e-03,\n","       3.09228897e-04, 4.30607796e-03, 4.13656235e-03, 6.46948814e-04,\n","       4.71818447e-03, 4.43542004e-03, 1.26848221e-02, 3.44514847e-05,\n","       1.24146938e-02, 8.80217552e-03, 1.30976439e-02, 3.75568867e-03,\n","       3.34143639e-03, 3.99625301e-03, 7.20858574e-04, 2.97563076e-02,\n","       7.52210617e-05, 2.84910202e-04, 3.65436077e-03, 3.97896767e-03,\n","       3.96573544e-03, 4.64165211e-03, 2.00271606e-05, 4.03380394e-03,\n","       5.24628162e-03, 7.60185719e-03, 4.39620018e-03, 1.48948431e-02,\n","       2.04229355e-03, 2.44128704e-03, 2.25181580e-02, 3.03161144e-02,\n","       7.24363327e-03, 8.74674320e-03, 7.99429417e-03, 0.00000000e+00,\n","       2.17804909e-02, 1.48758888e-02, 2.57775784e-02, 1.15638971e-02,\n","       1.82533264e-02, 7.41159916e-03, 1.89656019e-02, 1.38820410e-02,\n","       1.57384872e-02, 1.50589943e-02, 7.39301443e-02, 2.19633579e-02,\n","       2.38418579e-07, 7.99691677e-03, 1.00815296e-03, 7.99465179e-03,\n","       6.30569458e-03, 1.11985207e-02, 9.18447971e-03, 2.16752291e-02,\n","       3.44947577e-02, 1.31486654e-02, 1.40200853e-02, 2.58442163e-02,\n","       4.40623760e-02, 8.85283947e-03, 1.27915144e-02, 3.64983082e-03,\n","       8.56685638e-03, 4.61411476e-03, 8.30173492e-04, 3.71932983e-05,\n","       7.89940357e-03, 3.96239758e-03, 9.35530663e-03, 5.22971153e-04,\n","       1.33848190e-02, 2.24745274e-03, 2.03131437e-02, 1.68728828e-03,\n","       2.62964964e-02, 5.02815247e-02, 2.11396217e-02, 1.23624802e-02,\n","       5.22279739e-03, 1.79147720e-02, 2.75886059e-03, 2.57968903e-03,\n","       2.05938816e-02, 2.75552273e-03, 1.13264322e-02, 1.13629103e-02,\n","       5.88736534e-02, 3.20154428e-02, 2.29008198e-02, 1.03954077e-02,\n","       7.98094273e-03, 5.93698025e-02, 7.82158375e-02, 1.11782551e-03,\n","       4.94825840e-03, 8.92806053e-03, 7.29775429e-03, 1.19209290e-07,\n","       1.38847828e-02, 5.80878258e-02, 2.14933157e-02, 1.89316273e-03,\n","       4.41744328e-02, 1.81345940e-02, 8.53359699e-03, 1.15132332e-03,\n","       4.67602015e-02, 2.95855999e-02, 2.89690495e-02, 1.48233175e-02,\n","       1.27294064e-02, 4.01926041e-03, 3.81755829e-03, 1.73511505e-02,\n","       3.09953690e-02, 1.73518658e-02, 5.35631180e-03, 2.02503204e-02,\n","       1.57252550e-02, 2.89686918e-02, 2.54619122e-02, 9.53912735e-04,\n","       8.58052969e-02, 2.58461237e-02, 6.25947714e-02, 4.21822071e-03,\n","       8.36253166e-03, 8.85272026e-03, 2.32115984e-02, 3.99768353e-03,\n","       1.87582970e-02, 2.81071663e-03, 1.86610222e-03, 1.47058964e-02,\n","       2.11281776e-02, 2.85768509e-03, 3.64896059e-02, 3.06241512e-02,\n","       1.53629661e-01, 7.03182220e-02, 1.70707703e-04, 6.51981831e-02,\n","       1.65700912e-03, 2.17688084e-03, 3.57627869e-07, 3.99827957e-03,\n","       7.36784935e-03, 1.82099342e-02, 3.40468884e-02, 9.49859619e-03,\n","       3.88731956e-02, 4.67021465e-02, 9.82773304e-03, 1.05712414e-02,\n","       2.47195959e-02, 3.20792198e-04, 1.23419762e-02, 1.23548508e-02,\n","       2.72989273e-05, 7.90560246e-03, 3.90899181e-03, 4.83548641e-03,\n","       8.03387165e-03, 6.76989555e-04, 8.70609283e-03, 1.29593611e-02,\n","       1.23271942e-02, 1.29537582e-02, 2.49328613e-02, 7.60602951e-03,\n","       8.77177715e-03, 5.86982965e-02, 4.13380861e-02, 2.13518143e-02,\n","       4.60982323e-03, 4.05824184e-03, 1.12056732e-04, 1.25735998e-02,\n","       3.06367874e-05, 1.19866133e-02, 6.66499138e-04, 1.27137899e-02,\n","       8.03303719e-03, 4.71103191e-03, 3.32820415e-03, 7.79879093e-03,\n","       3.75574827e-02, 9.98616219e-04, 1.09181404e-02, 1.43996477e-02,\n","       4.60243225e-03, 8.21948051e-03, 7.69162178e-03, 6.71482086e-03,\n","       7.46893883e-03, 6.98804855e-04, 8.47208500e-03, 3.98564339e-03,\n","       1.05365515e-02, 9.18602943e-03, 8.78310204e-03, 9.18567181e-03,\n","       5.00396490e-02, 2.75301933e-03, 2.21225023e-02, 1.89709663e-03,\n","       2.93195248e-03, 5.35380840e-03, 3.26931477e-03, 2.41374969e-03,\n","       4.91547585e-03, 5.89764118e-03, 6.63077831e-03, 2.63392925e-03,\n","       7.32910633e-03, 1.32342577e-02, 6.65056705e-03, 1.28931999e-02,\n","       1.05860233e-02, 2.08175182e-03, 1.43460035e-02, 9.00244713e-03,\n","       8.01408291e-03, 4.42230701e-03, 7.67660141e-03, 4.31036949e-03,\n","       3.99839878e-03, 8.40044022e-03, 7.05122948e-04, 3.46779823e-04,\n","       2.15945244e-02, 1.99373960e-02, 2.48749256e-02, 1.82327032e-02,\n","       2.12754011e-02, 1.72786713e-02, 9.26029682e-03, 5.06424904e-03,\n","       4.65929508e-03, 1.26422644e-02, 4.64737415e-03, 8.69131088e-03,\n","       5.03957272e-03, 4.35447693e-03, 2.16848850e-02, 2.43281126e-02,\n","       2.90769339e-02, 1.04241371e-02, 1.57356262e-04, 3.17668915e-03,\n","       7.12704659e-03, 1.73842907e-02, 1.02188587e-02, 2.55060196e-03,\n","       1.70165300e-02, 2.40496397e-02, 3.25250626e-03, 1.30538940e-02,\n","       4.40589190e-02, 7.88998604e-03, 1.01804733e-04, 1.33695602e-02,\n","       2.31366158e-02, 1.12071037e-02, 3.30018997e-03, 1.12085342e-02,\n","       2.93562412e-02, 2.08754539e-02, 6.18016720e-02, 2.14695930e-04,\n","       9.39452648e-03, 7.30633736e-03, 2.51281261e-02, 8.08632374e-03,\n","       2.68791914e-02, 1.00213289e-02, 9.96506214e-03, 4.59432602e-04,\n","       4.04633284e-02, 2.04759836e-02, 1.80816650e-03, 9.50992107e-03,\n","       8.33972692e-02, 2.13208199e-02, 2.72531509e-02, 6.61671162e-03,\n","       3.49748135e-03, 3.12900543e-03, 5.27501106e-04, 7.82072544e-03,\n","       3.02393436e-02, 8.02588463e-03, 1.67042017e-02, 3.98755074e-03,\n","       3.72501612e-02, 3.89587879e-03, 3.94570827e-03, 8.08250904e-03,\n","       3.76293659e-02, 8.25071335e-03, 1.09601021e-02, 9.07480717e-03,\n","       4.34839725e-03, 2.02655792e-06, 4.01139259e-03, 3.59773636e-04,\n","       1.19308233e-02, 1.60001516e-02, 1.77919865e-03, 9.18579102e-03,\n","       8.53097439e-03, 5.16080856e-03, 3.35316658e-02, 8.48197937e-03,\n","       2.51276493e-02, 2.03003883e-02, 5.36090136e-02, 1.42487288e-02,\n","       2.11515427e-02, 6.17909431e-03, 9.14776325e-03, 4.66620922e-03,\n","       3.61489058e-02, 2.03121901e-02, 8.63552094e-04, 7.48622417e-03,\n","       2.25977898e-02, 2.62951851e-03, 6.17289543e-03, 4.52935696e-03,\n","       4.16183472e-02, 7.93113708e-02, 2.24884748e-02, 4.72235680e-03,\n","       4.08208370e-03, 7.20334053e-03, 4.45652008e-03, 1.15156174e-04,\n","       1.70822144e-02, 1.59263611e-04, 3.14295292e-03, 4.18925285e-03,\n","       1.63375139e-02, 1.19134188e-02, 1.76303387e-02, 8.50915909e-04,\n","       4.16241884e-02, 1.37591362e-03, 2.65175104e-02, 5.46698570e-02,\n","       4.31430340e-03, 3.62765789e-03, 3.69942188e-03, 4.66871262e-03,\n","       8.08727741e-03, 4.43974733e-02, 7.19606876e-02, 5.73537350e-02,\n","       3.37358713e-02, 6.00956678e-02, 1.07270479e-02, 2.05855370e-02,\n","       2.81829834e-02, 1.03635669e-01, 2.13103294e-02, 3.25918198e-03,\n","       8.34465027e-07, 9.21440125e-03, 1.19328499e-03, 3.85987759e-03,\n","       1.22641325e-02, 1.23655796e-02, 8.66472721e-03, 2.85983086e-04,\n","       3.36217880e-03, 4.05359268e-03, 3.84186506e-02, 7.15851784e-04,\n","       5.47344685e-02, 5.37663698e-02, 8.40079784e-03, 3.88205051e-03,\n","       8.37886333e-03, 1.29774809e-02, 3.59058380e-04, 5.03170490e-03,\n","       4.39274311e-03, 6.22391701e-04, 2.06938982e-02, 1.73867941e-02,\n","       1.26100779e-02, 3.98492813e-03, 8.41486454e-03, 4.35638428e-03,\n","       4.22620773e-03, 2.07154751e-02, 4.54974174e-03, 1.60531998e-02,\n","       3.98659706e-03, 4.18937206e-03, 1.84893608e-04, 1.77621841e-05,\n","       1.26290321e-02, 4.12464142e-05, 1.67104006e-02, 4.71746922e-03,\n","       1.26872063e-02, 1.63316727e-02, 3.27348709e-04, 1.24474764e-02,\n","       7.72094727e-03, 3.66624594e-02, 3.68177891e-03, 4.87303734e-03,\n","       3.70287895e-03, 3.27968597e-03, 7.68542290e-04, 1.10387802e-04,\n","       8.28218460e-03, 6.01649284e-03, 4.45544720e-03, 3.66210938e-04,\n","       3.58939171e-04, 3.52381468e-02, 4.26040888e-02, 3.00879478e-02,\n","       4.14109230e-03, 7.94172287e-03, 1.27398968e-02, 5.43918610e-02,\n","       4.58657742e-03, 1.21784210e-02, 1.46126747e-02, 1.34776831e-02,\n","       2.48353481e-02, 7.99286366e-03, 1.62742138e-02, 2.93834209e-02,\n","       1.13725662e-04, 2.87600756e-02, 2.95242071e-02, 3.70991230e-03,\n","       1.63853168e-03, 6.74306154e-02, 3.39853764e-02, 4.65881824e-03,\n","       3.69524956e-03, 3.01396847e-03, 3.87251377e-03, 4.57775593e-03,\n","       7.94851780e-03, 5.03268242e-02, 8.70418549e-03, 2.44498253e-04,\n","       2.02735662e-02, 1.96139812e-02, 1.77333355e-02, 4.52995300e-06,\n","       4.16611433e-02, 8.30423832e-03, 1.76881552e-02, 1.00340843e-02,\n","       2.88116932e-03, 6.80017471e-03, 5.00679016e-06, 1.13248825e-05,\n","       7.19127655e-02, 3.99613380e-03, 2.88200378e-03, 1.65796280e-03,\n","       4.77373600e-02, 1.14127398e-02, 3.13987732e-02, 3.72028351e-03,\n","       7.99655914e-03, 2.62888670e-02, 1.02978945e-02, 1.59893036e-02,\n","       7.99489021e-03, 1.19925737e-02, 3.99732590e-03, 1.19209290e-07,\n","       1.59866810e-02, 1.08969212e-03, 7.57014751e-03, 1.69527531e-03,\n","       2.16822624e-02, 2.16821432e-02, 1.58941746e-02, 1.27363205e-03,\n","       2.42246389e-02, 2.42106915e-02, 2.37575769e-02, 2.02130079e-02,\n","       3.99756432e-03, 1.19920969e-02, 5.48553467e-03, 1.33371353e-03,\n","       1.37825012e-02, 1.26404762e-02, 1.50045156e-02, 5.56206703e-03,\n","       9.56010818e-03, 4.15366888e-02, 1.27820969e-02, 3.99732590e-03,\n","       4.79692221e-02, 6.79544210e-02, 1.99871063e-02, 3.99708748e-03,\n","       7.99310207e-03, 3.99768353e-03, 2.26497650e-06, 2.38418579e-07,\n","       1.59888268e-02, 1.19926929e-02, 1.99871063e-02, 1.99863911e-02,\n","       6.00426197e-02, 2.24682093e-02, 7.80963898e-03, 8.32414627e-03,\n","       2.05310583e-02, 3.45664024e-02, 7.20524788e-03, 1.21909380e-02,\n","       4.53281403e-03, 8.98170471e-03, 9.17315483e-03, 2.34842300e-05,\n","       2.90079117e-02, 8.35204124e-03, 3.66318226e-03, 4.31931019e-03,\n","       3.36238146e-02, 2.76696682e-02, 1.73325539e-02, 8.66842270e-03,\n","       4.55627441e-02, 4.33819294e-02, 4.98350859e-02, 6.37102127e-03,\n","       8.63301754e-03, 8.70859623e-03, 4.05251980e-03, 2.38418579e-07,\n","       2.10911036e-02, 2.50910521e-02, 9.09984112e-03, 1.17839575e-02,\n","       3.99863720e-03, 2.55236626e-02, 1.35287046e-02, 5.53548336e-03,\n","       8.85756016e-02, 3.69385481e-02, 3.26129198e-02, 3.36349010e-03,\n","       3.99732590e-03, 1.19209290e-07, 3.99780273e-03, 5.96046448e-07,\n","       1.11206770e-02, 5.81789017e-03, 6.16788864e-03, 1.82676315e-03,\n","       1.65711641e-02, 4.57930565e-03, 5.81741333e-04, 3.41546535e-03,\n","       2.45648623e-02, 4.57978249e-03, 4.57859039e-03, 4.92727757e-03,\n","       3.99684906e-03, 3.99589539e-03, 3.99744511e-03, 1.99869871e-02,\n","       1.47813559e-02, 4.76837158e-07, 3.99732590e-03, 9.55307484e-03,\n","       3.13854218e-03, 4.85825539e-03, 3.13544273e-03, 2.71208286e-02,\n","       1.68496370e-02, 2.89666653e-03, 3.99721861e-02, 7.99453259e-03,\n","       3.99649143e-03, 7.99548626e-03, 3.99720669e-03, 2.38418579e-07,\n","       1.19911432e-02, 7.99500942e-03, 9.53674316e-07, 7.99441338e-03,\n","       7.99465179e-03, 3.99708748e-03, 1.44338608e-03, 6.04748726e-04,\n","       2.09912062e-02, 1.44469738e-03, 1.05491877e-02, 2.55334377e-03,\n","       1.19062662e-02, 3.91268730e-03, 6.28650188e-03, 2.39850283e-02,\n","       2.79811621e-02, 3.99721861e-02, 8.34465027e-07, 1.19916201e-02,\n","       1.44823790e-02, 2.48944759e-03, 5.50448895e-03, 1.50334835e-03,\n","       1.22423172e-02, 2.49743462e-04, 2.50697136e-04, 2.37332582e-02,\n","       2.74395943e-03, 9.25230980e-03, 2.74109840e-03, 2.74097919e-03,\n","       2.39844322e-02, 1.19918585e-02, 2.39839554e-02, 3.99661064e-03,\n","       1.59883499e-02, 7.99369812e-03, 1.19909048e-02, 1.59883499e-02,\n","       1.61426067e-02, 3.99684906e-03, 2.14437246e-02, 1.19946003e-02,\n","       3.99744511e-03, 1.19922161e-02, 3.99827957e-03, 2.38418579e-07,\n","       1.59896612e-02, 6.24980927e-02, 1.68285370e-02, 3.45177650e-02,\n","       1.59116983e-02, 1.37475729e-02, 1.26683712e-03, 1.95732117e-02,\n","       6.83832169e-03, 7.19213486e-03, 1.32346153e-03, 2.35447884e-02,\n","       7.99441338e-03, 2.97299623e-02, 5.02562523e-03, 5.25724888e-03,\n","       1.61491632e-02, 1.07851028e-02, 4.76837158e-07, 7.99500942e-03,\n","       2.16023922e-02, 1.14918947e-02, 2.80127525e-02, 2.00157166e-02,\n","       4.02562618e-02, 7.31337070e-03, 2.67732143e-02, 7.07244873e-03,\n","       3.99756432e-03, 3.99756432e-03, 3.24356556e-03, 4.67395782e-03,\n","       2.09891796e-03, 1.00301504e-02, 2.26937532e-02, 1.54569149e-02,\n","       2.95779705e-02, 3.28255892e-02, 2.55800486e-02, 3.99923325e-03,\n","       5.99606037e-02, 1.99867487e-02, 1.19916201e-02, 3.99804115e-03,\n","       7.99369812e-03, 3.99827957e-03, 1.19916201e-02, 2.38418579e-07,\n","       2.38418579e-07, 7.99489021e-03, 3.99780273e-03, 3.99684906e-03,\n","       3.19786072e-02, 3.19784880e-02, 5.16152382e-03, 3.99696827e-03,\n","       1.73275471e-02, 1.23494864e-02, 1.23519897e-02, 2.76335478e-02,\n","       1.43647194e-04, 8.36074352e-03, 5.16462326e-03, 5.49471378e-03,\n","       1.40330791e-02, 2.26204395e-02, 2.34203339e-02, 1.01822615e-02,\n","       4.71730232e-02, 2.82976627e-02, 1.51402950e-02, 1.17673874e-02,\n","       9.72889662e-02, 4.39703465e-02, 9.03908014e-02, 1.35313272e-02,\n","       5.56588173e-04, 9.19950008e-03, 2.91606188e-02, 1.18998289e-02,\n","       1.23692751e-02, 2.50611305e-02, 3.09348106e-04, 7.87258148e-04,\n","       2.63725519e-02, 6.55194521e-02, 4.64450121e-02, 1.70302391e-02,\n","       1.41543269e-01, 8.72968435e-02, 1.07924461e-01, 2.86700726e-02,\n","       8.08668137e-03, 4.31025028e-03, 1.29604340e-02, 2.38180161e-04,\n","       5.16140461e-03, 3.73005867e-02, 1.66521072e-02, 3.45706940e-05,\n","       1.72601938e-02, 2.92935371e-02, 3.88704538e-02, 8.05103779e-03,\n","       4.72388268e-02, 4.42774296e-02, 2.87185907e-02, 7.53617287e-03]), 'mean_score_time': array([0.00399661, 0.        , 0.00399661, 0.02903068, 0.        ,\n","       0.01078963, 0.00399733, 0.00226009, 0.        , 0.        ,\n","       0.00399745, 0.00435483, 0.00867116, 0.00399816, 0.00833452,\n","       0.00399756, 0.        , 0.00399768, 0.00399792, 0.0039978 ,\n","       0.        , 0.00838459, 0.00399816, 0.00069571, 0.00399816,\n","       0.        , 0.00403357, 0.00399745, 0.00399745, 0.00461733,\n","       0.00399733, 0.        , 0.00434756, 0.        , 0.        ,\n","       0.0039978 , 0.        , 0.00863862, 0.01264596, 0.        ,\n","       0.00799513, 0.00399733, 0.00399685, 0.00063336, 0.00399756,\n","       0.00519001, 0.00799477, 0.00799513, 0.00399768, 0.00399756,\n","       0.00399756, 0.00799477, 0.00399733, 0.        , 0.        ,\n","       0.00399745, 0.00399745, 0.00800157, 0.00399733, 0.00399709,\n","       0.00798953, 0.00399756, 0.01199019, 0.00799489, 0.00407875,\n","       0.00399959, 0.00298977, 0.00399745, 0.00399792, 0.00153387,\n","       0.00803089, 0.00399876, 0.00399756, 0.00399721, 0.        ,\n","       0.00494289, 0.00399745, 0.00437975, 0.0086627 , 0.        ,\n","       0.00399768, 0.00399768, 0.        , 0.00799537, 0.        ,\n","       0.00402176, 0.        , 0.00799406, 0.00367725, 0.00399745,\n","       0.        , 0.        , 0.00399745, 0.00399745, 0.00628424,\n","       0.00809252, 0.00399768, 0.00154245, 0.00152338, 0.00554144,\n","       0.        , 0.00399745, 0.        , 0.00399697, 0.01198971,\n","       0.00799453, 0.01998711, 0.00399733, 0.00399804, 0.00399578,\n","       0.00799513, 0.00399768, 0.        , 0.        , 0.00441861,\n","       0.01199186, 0.00399733, 0.        , 0.00399756, 0.        ,\n","       0.00399745, 0.        , 0.00466454, 0.        , 0.0119915 ,\n","       0.00399756, 0.00399852, 0.0015403 , 0.00399709, 0.00799453,\n","       0.00799441, 0.00399733, 0.00552213, 0.00399816, 0.00603354,\n","       0.00799489, 0.00799525, 0.00399745, 0.00399733, 0.00399745,\n","       0.00799477, 0.00799477, 0.00876725, 0.00399745, 0.0039978 ,\n","       0.00799429, 0.00399673, 0.0039978 , 0.        , 0.00445998,\n","       0.00399756, 0.        , 0.        , 0.00399768, 0.00399745,\n","       0.00399709, 0.01199222, 0.00799465, 0.00399971, 0.        ,\n","       0.00399745, 0.00604367, 0.00799501, 0.0079962 , 0.00799453,\n","       0.00608337, 0.00603318, 0.        , 0.00466156, 0.02067769,\n","       0.00399709, 0.00399745, 0.        , 0.01265764, 0.        ,\n","       0.00399733, 0.00405455, 0.00874472, 0.        , 0.        ,\n","       0.00465393, 0.00401986, 0.00399756, 0.00399733, 0.00465333,\n","       0.        , 0.        , 0.        , 0.        , 0.00399721,\n","       0.00799477, 0.        , 0.00064754, 0.00799465, 0.00399673,\n","       0.0053463 , 0.00399709, 0.00466859, 0.        , 0.00861025,\n","       0.00867164, 0.00399756, 0.        , 0.01714444, 0.00204945,\n","       0.00399721, 0.0067023 , 0.01853526, 0.        , 0.00539804,\n","       0.00608528, 0.00153148, 0.00101221, 0.        , 0.0067513 ,\n","       0.        , 0.00799823, 0.        , 0.00552309, 0.00399697,\n","       0.00399721, 0.0119915 , 0.00204289, 0.02934372, 0.00152934,\n","       0.        , 0.00062895, 0.00853062, 0.00399768, 0.00799394,\n","       0.00400269, 0.00799394, 0.00399923, 0.00399733, 0.00799525,\n","       0.00223303, 0.        , 0.00399733, 0.00399745, 0.01236463,\n","       0.00399733, 0.00799584, 0.01637745, 0.0083245 , 0.00799417,\n","       0.00399685, 0.00433159, 0.01199198, 0.00869024, 0.        ,\n","       0.00399756, 0.02465045, 0.00799453, 0.00103092, 0.00399804,\n","       0.        , 0.        , 0.00402009, 0.01199174, 0.        ,\n","       0.00505292, 0.        , 0.00399733, 0.00813937, 0.00799561,\n","       0.00656343, 0.00868487, 0.        , 0.        , 0.00399709,\n","       0.00799429, 0.0039978 , 0.        , 0.        , 0.00399756,\n","       0.00871027, 0.        , 0.00399733, 0.00399721, 0.        ,\n","       0.        , 0.01665092, 0.01666558, 0.        , 0.        ,\n","       0.00799406, 0.00399733, 0.00399721, 0.00430918, 0.00399745,\n","       0.        , 0.00399804, 0.00891113, 0.01199234, 0.        ,\n","       0.00399745, 0.00799549, 0.00799596, 0.00616789, 0.00799489,\n","       0.00438392, 0.00399792, 0.00435281, 0.00869107, 0.00436163,\n","       0.00799596, 0.00399804, 0.00399709, 0.00437868, 0.00435281,\n","       0.        , 0.00399816, 0.00835824, 0.00838053, 0.00876069,\n","       0.00399828, 0.        , 0.00833189, 0.02027631, 0.00399792,\n","       0.00399792, 0.        , 0.0039978 , 0.        , 0.        ,\n","       0.        , 0.00604904, 0.        , 0.0045923 , 0.01199174,\n","       0.00399721, 0.006042  , 0.00858462, 0.00152004, 0.00867712,\n","       0.        , 0.00399697, 0.00399697, 0.        , 0.        ,\n","       0.00427282, 0.        , 0.        , 0.00399816, 0.00217557,\n","       0.00833321, 0.00433409, 0.00636518, 0.00801861, 0.03769028,\n","       0.00433958, 0.00799513, 0.00399745, 0.00399721, 0.        ,\n","       0.0039978 , 0.00799561, 0.00399768, 0.        , 0.00399816,\n","       0.00399745, 0.00214183, 0.        , 0.00450134, 0.00399745,\n","       0.0058794 , 0.00865376, 0.00399637, 0.        , 0.00399685,\n","       0.00399792, 0.02131784, 0.00399792, 0.00399745, 0.00399768,\n","       0.00399637, 0.00799513, 0.00399792, 0.00424421, 0.00799525,\n","       0.00463164, 0.0086695 , 0.00469887, 0.00255537, 0.00399816,\n","       0.00399756, 0.00471544, 0.00399733, 0.        , 0.0039978 ,\n","       0.        , 0.00799477, 0.00799394, 0.0039978 , 0.00799441,\n","       0.00399745, 0.00399733, 0.        , 0.00399721, 0.        ,\n","       0.00799417, 0.0039978 , 0.00867033, 0.0119921 , 0.00399697,\n","       0.        , 0.        , 0.00799572, 0.00399685, 0.00799489,\n","       0.00464284, 0.00799489, 0.00458086, 0.00839198, 0.00399745,\n","       0.00434875, 0.00477242, 0.00437176, 0.00062156, 0.00399816,\n","       0.00399697, 0.        , 0.0079962 , 0.00399768, 0.00866723,\n","       0.00062656, 0.        , 0.        , 0.        , 0.00399709,\n","       0.        , 0.00858414, 0.00399756, 0.00399721, 0.        ,\n","       0.00399709, 0.00208318, 0.00552166, 0.00259137, 0.00858355,\n","       0.0040009 , 0.00259519, 0.        , 0.00976825, 0.00867677,\n","       0.00799501, 0.00799501, 0.00799537, 0.        , 0.00203204,\n","       0.00399756, 0.        , 0.0039978 , 0.00465667, 0.        ,\n","       0.02510083, 0.00799429, 0.00399745, 0.        , 0.        ,\n","       0.00399733, 0.00399733, 0.00399709, 0.00799429, 0.00604916,\n","       0.00399768, 0.00399685, 0.00399733, 0.00399804, 0.        ,\n","       0.00399685, 0.        , 0.        , 0.00420451, 0.0080198 ,\n","       0.00399709, 0.00402093, 0.00799513, 0.00799441, 0.00433898,\n","       0.        , 0.00866914, 0.        , 0.        , 0.00399733,\n","       0.01003063, 0.00203109, 0.00799406, 0.00399733, 0.00399745,\n","       0.0079937 , 0.00799477, 0.00799465, 0.0119921 , 0.00399756,\n","       0.00399768, 0.00399721, 0.00799537, 0.        , 0.        ,\n","       0.        , 0.00203407, 0.0055238 , 0.00399697, 0.00399768,\n","       0.        , 0.00799537, 0.0039978 , 0.00799477, 0.00399745,\n","       0.        , 0.00399792, 0.00399721, 0.00799453, 0.00152624,\n","       0.00552487, 0.00399745, 0.        , 0.00399745, 0.01598859,\n","       0.00399721, 0.01199174, 0.00399745, 0.00399709, 0.00799692,\n","       0.00399745, 0.00799465, 0.        , 0.01598978, 0.00399768,\n","       0.00399673, 0.00399733, 0.00799417, 0.01598907, 0.00399745,\n","       0.        , 0.00799489, 0.00799477, 0.00399745, 0.00203359,\n","       0.00799489, 0.00399804, 0.00399673, 0.00399756, 0.        ,\n","       0.00467527, 0.00900018, 0.01699579, 0.00399709, 0.00799429,\n","       0.00468254, 0.        , 0.00870895, 0.01296198, 0.00799489,\n","       0.        , 0.00799358, 0.        , 0.00799716, 0.00399685,\n","       0.        , 0.        , 0.00399756, 0.00799441, 0.00799501,\n","       0.00399816, 0.        , 0.00399685, 0.00399733, 0.00399733,\n","       0.        , 0.00399733, 0.00799477, 0.00356889, 0.00799513,\n","       0.00399768, 0.        , 0.00399733, 0.00399697, 0.00399756,\n","       0.00153279, 0.00399709, 0.00399721, 0.00399768, 0.        ,\n","       0.00799501, 0.00399721, 0.        , 0.00799477, 0.00399756,\n","       0.00399685, 0.00799501, 0.00399661, 0.00399697, 0.00399721,\n","       0.00399685, 0.01003361, 0.00399685, 0.00799453, 0.00152826,\n","       0.00400031, 0.00399745, 0.        , 0.00399733, 0.00399745,\n","       0.        , 0.00399745, 0.00399721, 0.        , 0.0039978 ,\n","       0.        , 0.        , 0.00399756, 0.00399721, 0.        ,\n","       0.00399768, 0.00399733, 0.        , 0.00399804, 0.00203753,\n","       0.00204062, 0.00399721, 0.00799429, 0.        , 0.00552022,\n","       0.0055207 , 0.        , 0.        , 0.00399721, 0.00399733,\n","       0.        , 0.        , 0.        , 0.00399649, 0.00399745,\n","       0.        , 0.00399733, 0.        , 0.        , 0.00799477,\n","       0.00799716, 0.        , 0.00799513, 0.        , 0.        ,\n","       0.00799441, 0.00799465, 0.        , 0.00399733, 0.00399745,\n","       0.00399721, 0.00799465, 0.        , 0.00399733, 0.00799477,\n","       0.00399709, 0.00399709, 0.        , 0.00799525, 0.        ,\n","       0.00799429, 0.01998723, 0.        , 0.00799465, 0.00399745,\n","       0.        , 0.0039978 , 0.02001035, 0.00799274, 0.00399709,\n","       0.00472188, 0.008358  , 0.        , 0.00399733, 0.00472188,\n","       0.00399685, 0.00399756, 0.        , 0.        , 0.        ,\n","       0.00399768, 0.00537026, 0.00399792, 0.00799704, 0.00799477,\n","       0.00307965, 0.00758016, 0.00799417, 0.        , 0.00799417,\n","       0.00152731, 0.00607359, 0.00799501, 0.00153089, 0.00799549,\n","       0.00799513, 0.00799465, 0.        , 0.        , 0.00399661,\n","       0.00399756, 0.0039978 , 0.00799501, 0.00399745, 0.00799441,\n","       0.        , 0.00399768, 0.00399709, 0.        , 0.00399709,\n","       0.00799477, 0.00799537, 0.00399685, 0.00399768, 0.00799394,\n","       0.        , 0.02464795, 0.00399768, 0.00832784, 0.00152576,\n","       0.00479412, 0.00399709, 0.0039953 , 0.00799298, 0.00399745,\n","       0.        , 0.00399733, 0.        , 0.00399768, 0.00799441,\n","       0.        , 0.00799489, 0.00402176, 0.00874484, 0.00399745,\n","       0.        , 0.00799394, 0.00203514, 0.0079937 , 0.        ,\n","       0.        , 0.        , 0.00399601, 0.00399709, 0.00799263,\n","       0.00045145, 0.        , 0.        , 0.        , 0.0034883 ,\n","       0.01233959, 0.01660001, 0.00431621, 0.00404799, 0.        ,\n","       0.00399828, 0.00399816, 0.016626  , 0.00433266, 0.00399721,\n","       0.00495183, 0.        , 0.00833106, 0.00058734, 0.00246406,\n","       0.00665319, 0.00399733, 0.00799549]), 'std_score_time': array([3.99661064e-03, 0.00000000e+00, 3.99661064e-03, 2.06137896e-02,\n","       0.00000000e+00, 1.43957138e-03, 3.99732590e-03, 2.26008892e-03,\n","       0.00000000e+00, 0.00000000e+00, 3.99744511e-03, 4.35483456e-03,\n","       8.67116451e-03, 3.99816036e-03, 3.39388847e-04, 3.99756432e-03,\n","       0.00000000e+00, 3.99768353e-03, 3.99792194e-03, 3.99780273e-03,\n","       0.00000000e+00, 3.90172005e-04, 3.99816036e-03, 6.95705414e-04,\n","       3.99816036e-03, 0.00000000e+00, 4.03356552e-03, 3.99744511e-03,\n","       3.99744511e-03, 4.61733341e-03, 3.99732590e-03, 0.00000000e+00,\n","       4.34756279e-03, 0.00000000e+00, 0.00000000e+00, 3.99780273e-03,\n","       0.00000000e+00, 5.96046448e-04, 1.26459599e-02, 0.00000000e+00,\n","       9.53674316e-07, 3.99732590e-03, 3.99684906e-03, 6.33358955e-04,\n","       3.99756432e-03, 2.80463696e-03, 5.96046448e-07, 2.38418579e-07,\n","       3.99768353e-03, 3.99756432e-03, 3.99756432e-03, 5.96046448e-07,\n","       3.99732590e-03, 0.00000000e+00, 0.00000000e+00, 3.99744511e-03,\n","       3.99744511e-03, 6.91413879e-06, 3.99732590e-03, 3.99708748e-03,\n","       4.64916229e-06, 3.99756432e-03, 1.19901896e-02, 7.15255737e-07,\n","       4.07874584e-03, 3.99959087e-03, 2.98976898e-03, 3.99744511e-03,\n","       3.99792194e-03, 1.53386593e-03, 3.64780426e-05, 3.99875641e-03,\n","       3.99756432e-03, 3.99720669e-03, 0.00000000e+00, 4.94289398e-03,\n","       3.99744511e-03, 4.37974930e-03, 6.68048859e-04, 0.00000000e+00,\n","       3.99768353e-03, 3.99768353e-03, 0.00000000e+00, 7.15255737e-07,\n","       0.00000000e+00, 4.02176380e-03, 0.00000000e+00, 5.96046448e-07,\n","       3.67724895e-03, 3.99744511e-03, 0.00000000e+00, 0.00000000e+00,\n","       3.99744511e-03, 3.99744511e-03, 1.71184540e-03, 9.73939896e-05,\n","       3.99768353e-03, 1.54244900e-03, 1.52337551e-03, 2.45511532e-03,\n","       0.00000000e+00, 3.99744511e-03, 0.00000000e+00, 3.99696827e-03,\n","       3.99529934e-03, 3.57627869e-07, 1.19912624e-02, 3.99732590e-03,\n","       3.99804115e-03, 3.99577618e-03, 2.38418579e-07, 3.99768353e-03,\n","       0.00000000e+00, 0.00000000e+00, 4.41861153e-03, 3.99696827e-03,\n","       3.99732590e-03, 0.00000000e+00, 3.99756432e-03, 0.00000000e+00,\n","       3.99744511e-03, 0.00000000e+00, 4.66454029e-03, 0.00000000e+00,\n","       3.99756432e-03, 3.99756432e-03, 3.99851799e-03, 1.54030323e-03,\n","       3.99708748e-03, 3.57627869e-07, 0.00000000e+00, 3.99732590e-03,\n","       2.47228146e-03, 3.99816036e-03, 1.96182728e-03, 7.99489021e-03,\n","       1.19209290e-07, 3.99744511e-03, 3.99732590e-03, 3.99744511e-03,\n","       3.57627869e-07, 1.19209290e-07, 7.73072243e-04, 3.99744511e-03,\n","       3.99780273e-03, 3.57627869e-07, 3.99672985e-03, 3.99780273e-03,\n","       0.00000000e+00, 2.53081322e-04, 3.99756432e-03, 0.00000000e+00,\n","       0.00000000e+00, 3.99768353e-03, 3.99744511e-03, 3.99708748e-03,\n","       3.99780273e-03, 4.76837158e-07, 3.99971008e-03, 0.00000000e+00,\n","       3.99744511e-03, 1.95097923e-03, 3.57627869e-07, 1.19209290e-07,\n","       3.57627869e-07, 1.91247463e-03, 1.96146965e-03, 0.00000000e+00,\n","       4.66156006e-03, 1.26835108e-02, 3.99708748e-03, 3.99744511e-03,\n","       0.00000000e+00, 4.66275215e-03, 0.00000000e+00, 3.99732590e-03,\n","       4.05454636e-03, 7.51495361e-04, 0.00000000e+00, 0.00000000e+00,\n","       3.34095955e-03, 4.01985645e-03, 3.99756432e-03, 3.99732590e-03,\n","       4.65333462e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n","       0.00000000e+00, 3.99720669e-03, 1.19209290e-07, 0.00000000e+00,\n","       6.47544861e-04, 2.38418579e-07, 3.99672985e-03, 4.09650803e-03,\n","       3.99708748e-03, 4.66859341e-03, 0.00000000e+00, 8.61024857e-03,\n","       6.76512718e-04, 3.99756432e-03, 0.00000000e+00, 1.61378384e-02,\n","       2.04944611e-03, 3.99720669e-03, 2.62463093e-03, 1.05413198e-02,\n","       0.00000000e+00, 2.59709358e-03, 1.91414356e-03, 1.53148174e-03,\n","       1.01220608e-03, 0.00000000e+00, 2.61783600e-03, 0.00000000e+00,\n","       3.09944153e-06, 0.00000000e+00, 2.47204304e-03, 3.99696827e-03,\n","       3.99720669e-03, 1.19915009e-02, 2.04288960e-03, 2.13493109e-02,\n","       1.52933598e-03, 0.00000000e+00, 6.28948212e-04, 5.37395477e-04,\n","       3.99768353e-03, 4.76837158e-07, 4.00269032e-03, 4.76837158e-07,\n","       3.99923325e-03, 3.99732590e-03, 5.96046448e-07, 2.23302841e-03,\n","       0.00000000e+00, 3.99732590e-03, 3.99744511e-03, 4.36782837e-03,\n","       3.99732590e-03, 7.15255737e-07, 1.63774490e-02, 8.32450390e-03,\n","       4.76837158e-07, 3.99684906e-03, 4.33158875e-03, 3.99684906e-03,\n","       6.96063042e-04, 0.00000000e+00, 3.99756432e-03, 7.32719898e-03,\n","       1.19209290e-07, 1.03092194e-03, 3.99804115e-03, 0.00000000e+00,\n","       0.00000000e+00, 4.02009487e-03, 3.99637222e-03, 0.00000000e+00,\n","       2.99108028e-03, 0.00000000e+00, 3.99732590e-03, 1.45196915e-04,\n","       7.99560547e-03, 2.42018700e-03, 6.35385513e-04, 0.00000000e+00,\n","       0.00000000e+00, 3.99708748e-03, 1.19209290e-07, 3.99780273e-03,\n","       0.00000000e+00, 0.00000000e+00, 3.99756432e-03, 8.71026516e-03,\n","       0.00000000e+00, 3.99732590e-03, 3.99720669e-03, 0.00000000e+00,\n","       0.00000000e+00, 8.65435600e-03, 1.66655779e-02, 0.00000000e+00,\n","       0.00000000e+00, 3.57627869e-07, 3.99732590e-03, 3.99720669e-03,\n","       3.68285179e-03, 3.99744511e-03, 0.00000000e+00, 3.99804115e-03,\n","       9.16957855e-04, 3.99792194e-03, 0.00000000e+00, 3.99744511e-03,\n","       3.57627869e-07, 1.19209290e-07, 1.82652473e-03, 0.00000000e+00,\n","       4.38392162e-03, 3.99792194e-03, 4.35280800e-03, 9.53674316e-06,\n","       4.36162949e-03, 1.19209290e-07, 3.99804115e-03, 3.99708748e-03,\n","       4.37867641e-03, 4.35280800e-03, 0.00000000e+00, 3.99816036e-03,\n","       3.62396240e-04, 3.84926796e-04, 4.98294830e-05, 3.99827957e-03,\n","       0.00000000e+00, 3.38196754e-04, 1.17080212e-02, 3.99792194e-03,\n","       3.99792194e-03, 0.00000000e+00, 3.99780273e-03, 0.00000000e+00,\n","       0.00000000e+00, 0.00000000e+00, 1.94537640e-03, 0.00000000e+00,\n","       4.59229946e-03, 1.19917393e-02, 3.99720669e-03, 1.95336342e-03,\n","       5.87105751e-04, 1.52003765e-03, 6.83188438e-04, 0.00000000e+00,\n","       3.99696827e-03, 3.99696827e-03, 0.00000000e+00, 0.00000000e+00,\n","       4.27281857e-03, 0.00000000e+00, 0.00000000e+00, 3.99816036e-03,\n","       2.17556953e-03, 3.37600708e-04, 4.33409214e-03, 2.30562687e-03,\n","       2.22921371e-05, 2.90111303e-02, 4.33957577e-03, 4.76837158e-07,\n","       3.99744511e-03, 3.99720669e-03, 0.00000000e+00, 3.99780273e-03,\n","       1.19209290e-06, 3.99768353e-03, 0.00000000e+00, 3.99816036e-03,\n","       3.99744511e-03, 2.14183331e-03, 0.00000000e+00, 3.49569321e-03,\n","       3.99744511e-03, 2.11644173e-03, 6.58392906e-04, 3.99637222e-03,\n","       0.00000000e+00, 3.99684906e-03, 3.99792194e-03, 2.13178396e-02,\n","       3.99792194e-03, 3.99744511e-03, 3.99768353e-03, 3.99637222e-03,\n","       2.38418579e-07, 3.99792194e-03, 4.24420834e-03, 1.19209290e-07,\n","       4.63163853e-03, 6.75082207e-04, 4.69887257e-03, 2.55537033e-03,\n","       3.99816036e-03, 3.99756432e-03, 4.71544266e-03, 3.99732590e-03,\n","       0.00000000e+00, 3.99780273e-03, 0.00000000e+00, 3.57627869e-07,\n","       0.00000000e+00, 3.99780273e-03, 4.76837158e-07, 3.99744511e-03,\n","       3.99732590e-03, 0.00000000e+00, 3.99720669e-03, 0.00000000e+00,\n","       2.38418579e-07, 3.99780273e-03, 6.74962997e-04, 1.19920969e-02,\n","       3.99696827e-03, 0.00000000e+00, 0.00000000e+00, 1.19209290e-07,\n","       3.99684906e-03, 9.53674316e-07, 4.64284420e-03, 1.43051147e-06,\n","       4.58085537e-03, 3.51548195e-04, 3.99744511e-03, 4.34875488e-03,\n","       4.77242470e-03, 4.37176228e-03, 6.21557236e-04, 3.99816036e-03,\n","       3.99696827e-03, 0.00000000e+00, 1.19209290e-07, 3.99768353e-03,\n","       6.25133514e-04, 6.26564026e-04, 0.00000000e+00, 0.00000000e+00,\n","       0.00000000e+00, 3.99708748e-03, 0.00000000e+00, 5.88297844e-04,\n","       3.99756432e-03, 3.99720669e-03, 0.00000000e+00, 3.99708748e-03,\n","       2.08318233e-03, 2.47156620e-03, 2.59137154e-03, 5.88893890e-04,\n","       4.00090218e-03, 2.59518623e-03, 0.00000000e+00, 1.72567368e-03,\n","       6.83307648e-04, 5.96046448e-07, 5.96046448e-07, 2.38418579e-07,\n","       0.00000000e+00, 2.03204155e-03, 3.99756432e-03, 0.00000000e+00,\n","       3.99780273e-03, 4.65667248e-03, 0.00000000e+00, 1.55919790e-02,\n","       5.96046448e-07, 3.99744511e-03, 0.00000000e+00, 0.00000000e+00,\n","       3.99732590e-03, 3.99732590e-03, 3.99708748e-03, 5.96046448e-07,\n","       1.94692612e-03, 3.99768353e-03, 3.99684906e-03, 3.99732590e-03,\n","       3.99804115e-03, 0.00000000e+00, 3.99684906e-03, 0.00000000e+00,\n","       0.00000000e+00, 4.20451164e-03, 2.39610672e-05, 3.99708748e-03,\n","       4.02092934e-03, 1.43051147e-06, 4.76837158e-07, 4.33897972e-03,\n","       0.00000000e+00, 2.86102295e-06, 0.00000000e+00, 0.00000000e+00,\n","       3.99732590e-03, 5.95939159e-03, 2.03108788e-03, 1.19209290e-07,\n","       3.99732590e-03, 3.99744511e-03, 2.38418579e-07, 5.96046448e-07,\n","       7.15255737e-07, 3.99768353e-03, 3.99756432e-03, 3.99768353e-03,\n","       3.99720669e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n","       0.00000000e+00, 2.03406811e-03, 2.47013569e-03, 3.99696827e-03,\n","       3.99768353e-03, 0.00000000e+00, 0.00000000e+00, 3.99780273e-03,\n","       1.31130219e-06, 3.99744511e-03, 0.00000000e+00, 3.99792194e-03,\n","       3.99720669e-03, 3.57627869e-07, 1.52623653e-03, 2.47073174e-03,\n","       3.99744511e-03, 0.00000000e+00, 3.99744511e-03, 7.99512863e-03,\n","       3.99720669e-03, 3.99684906e-03, 3.99744511e-03, 3.99708748e-03,\n","       8.34465027e-07, 3.99744511e-03, 4.76837158e-07, 0.00000000e+00,\n","       7.99632072e-03, 3.99768353e-03, 3.99672985e-03, 3.99732590e-03,\n","       4.76837158e-07, 7.99465179e-03, 3.99744511e-03, 0.00000000e+00,\n","       9.53674316e-07, 1.19209290e-07, 3.99744511e-03, 2.03359127e-03,\n","       7.99489021e-03, 3.99804115e-03, 3.99672985e-03, 3.99756432e-03,\n","       0.00000000e+00, 3.31795216e-03, 1.00576878e-03, 1.69957876e-02,\n","       3.99708748e-03, 3.57627869e-07, 4.68254089e-03, 0.00000000e+00,\n","       6.66379929e-04, 4.37676907e-03, 4.76837158e-07, 0.00000000e+00,\n","       8.34465027e-07, 0.00000000e+00, 2.50339508e-06, 3.99684906e-03,\n","       0.00000000e+00, 0.00000000e+00, 3.99756432e-03, 2.38418579e-07,\n","       1.19209290e-07, 3.99816036e-03, 0.00000000e+00, 3.99684906e-03,\n","       3.99732590e-03, 3.99732590e-03, 0.00000000e+00, 3.99732590e-03,\n","       1.19209290e-07, 5.09500504e-04, 7.15255737e-07, 3.99768353e-03,\n","       0.00000000e+00, 3.99732590e-03, 3.99696827e-03, 3.99756432e-03,\n","       1.53279305e-03, 3.99708748e-03, 3.99720669e-03, 3.99768353e-03,\n","       0.00000000e+00, 1.19209290e-07, 3.99720669e-03, 0.00000000e+00,\n","       7.99477100e-03, 3.99756432e-03, 3.99684906e-03, 7.99500942e-03,\n","       3.99661064e-03, 3.99696827e-03, 3.99720669e-03, 3.99684906e-03,\n","       5.95688820e-03, 3.99684906e-03, 8.34465027e-07, 1.52826309e-03,\n","       4.00030613e-03, 3.99744511e-03, 0.00000000e+00, 3.99732590e-03,\n","       3.99744511e-03, 0.00000000e+00, 3.99744511e-03, 3.99720669e-03,\n","       0.00000000e+00, 3.99780273e-03, 0.00000000e+00, 0.00000000e+00,\n","       3.99756432e-03, 3.99720669e-03, 0.00000000e+00, 3.99768353e-03,\n","       3.99732590e-03, 0.00000000e+00, 3.99804115e-03, 2.03752518e-03,\n","       2.04062462e-03, 3.99720669e-03, 7.99429417e-03, 0.00000000e+00,\n","       2.47371197e-03, 2.47418880e-03, 0.00000000e+00, 0.00000000e+00,\n","       3.99720669e-03, 3.99732590e-03, 0.00000000e+00, 0.00000000e+00,\n","       0.00000000e+00, 3.99649143e-03, 3.99744511e-03, 0.00000000e+00,\n","       3.99732590e-03, 0.00000000e+00, 0.00000000e+00, 1.19209290e-07,\n","       2.50339508e-06, 0.00000000e+00, 7.99512863e-03, 0.00000000e+00,\n","       0.00000000e+00, 2.38418579e-07, 2.38418579e-07, 0.00000000e+00,\n","       3.99732590e-03, 3.99744511e-03, 3.99720669e-03, 2.38418579e-07,\n","       0.00000000e+00, 3.99732590e-03, 3.57627869e-07, 3.99708748e-03,\n","       3.99708748e-03, 0.00000000e+00, 8.34465027e-07, 0.00000000e+00,\n","       3.57627869e-07, 1.19920969e-02, 0.00000000e+00, 1.19209290e-06,\n","       3.99744511e-03, 0.00000000e+00, 3.99780273e-03, 1.19665861e-02,\n","       9.53674316e-07, 3.99708748e-03, 4.72187996e-03, 3.64065170e-04,\n","       0.00000000e+00, 3.99732590e-03, 4.72187996e-03, 3.99684906e-03,\n","       3.99756432e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n","       3.99768353e-03, 5.37025928e-03, 3.99792194e-03, 1.66893005e-06,\n","       1.19209290e-07, 3.07965279e-03, 4.14252281e-04, 0.00000000e+00,\n","       0.00000000e+00, 7.15255737e-07, 1.52730942e-03, 1.92105770e-03,\n","       1.19209290e-07, 1.53088570e-03, 1.19209290e-07, 4.76837158e-07,\n","       7.99465179e-03, 0.00000000e+00, 0.00000000e+00, 3.99661064e-03,\n","       3.99756432e-03, 3.99780273e-03, 8.34465027e-07, 3.99744511e-03,\n","       4.76837158e-07, 0.00000000e+00, 3.99768353e-03, 3.99708748e-03,\n","       0.00000000e+00, 3.99708748e-03, 1.19209290e-07, 2.38418579e-07,\n","       3.99684906e-03, 3.99768353e-03, 1.43051147e-06, 0.00000000e+00,\n","       1.53257847e-02, 3.99768353e-03, 3.28898430e-04, 1.52575970e-03,\n","       4.79412079e-03, 3.99708748e-03, 3.99529934e-03, 0.00000000e+00,\n","       3.99744511e-03, 0.00000000e+00, 3.99732590e-03, 0.00000000e+00,\n","       3.99768353e-03, 2.38418579e-07, 0.00000000e+00, 0.00000000e+00,\n","       4.02176380e-03, 7.51137733e-04, 3.99744511e-03, 0.00000000e+00,\n","       9.53674316e-07, 2.03514099e-03, 0.00000000e+00, 0.00000000e+00,\n","       0.00000000e+00, 0.00000000e+00, 3.99601460e-03, 3.99708748e-03,\n","       2.74181366e-06, 4.51445580e-04, 0.00000000e+00, 0.00000000e+00,\n","       0.00000000e+00, 3.48830223e-03, 4.34470177e-03, 8.60512257e-03,\n","       4.31621075e-03, 4.04798985e-03, 0.00000000e+00, 3.99827957e-03,\n","       3.99816036e-03, 8.63087177e-03, 4.33266163e-03, 3.99720669e-03,\n","       3.77714634e-03, 0.00000000e+00, 3.35931778e-04, 5.87344170e-04,\n","       2.46405602e-03, 1.34122372e-03, 3.99732590e-03, 5.96046448e-07]), 'param_learning_rate': masked_array(data=[0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04],\n","             mask=[False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False],\n","       fill_value='?',\n","            dtype=object), 'param_max_depth': masked_array(data=[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","                   4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","                   4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6,\n","                   6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n","                   6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n","                   6, 6, 6, 6, 6, 6, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n","                   8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n","                   8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n","                   10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n","                   10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n","                   10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n","                   10, 10, 10, 10, 10, 10, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","                   4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","                   4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","                   4, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n","                   6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n","                   6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 8, 8, 8, 8,\n","                   8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n","                   8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n","                   8, 8, 8, 8, 8, 8, 8, 8, 10, 10, 10, 10, 10, 10, 10, 10,\n","                   10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n","                   10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n","                   10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 4, 4,\n","                   4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","                   4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","                   4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6, 6, 6,\n","                   6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n","                   6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n","                   6, 6, 6, 6, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n","                   8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n","                   8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 10, 10,\n","                   10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n","                   10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n","                   10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n","                   10, 10, 10, 10, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","                   4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","                   4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 6,\n","                   6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n","                   6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n","                   6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 8, 8, 8, 8, 8, 8, 8,\n","                   8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n","                   8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n","                   8, 8, 8, 8, 8, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n","                   10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n","                   10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n","                   10, 10, 10, 10, 10, 10, 10, 10, 10, 10],\n","             mask=[False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False],\n","       fill_value='?',\n","            dtype=object), 'param_max_features': masked_array(data=[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5,\n","                   5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 7, 7, 7, 7,\n","                   7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3,\n","                   3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5,\n","                   5, 5, 5, 5, 5, 5, 5, 5, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n","                   7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n","                   3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n","                   5, 5, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n","                   3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5,\n","                   5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 7, 7, 7, 7,\n","                   7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3,\n","                   3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5,\n","                   5, 5, 5, 5, 5, 5, 5, 5, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n","                   7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n","                   3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n","                   5, 5, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n","                   3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5,\n","                   5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 7, 7, 7, 7,\n","                   7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3,\n","                   3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5,\n","                   5, 5, 5, 5, 5, 5, 5, 5, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n","                   7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n","                   3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n","                   5, 5, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n","                   3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5,\n","                   5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 7, 7, 7, 7,\n","                   7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3,\n","                   3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5,\n","                   5, 5, 5, 5, 5, 5, 5, 5, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n","                   7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n","                   3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n","                   5, 5, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n","                   3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5,\n","                   5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 7, 7, 7, 7,\n","                   7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3,\n","                   3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5,\n","                   5, 5, 5, 5, 5, 5, 5, 5, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n","                   7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n","                   3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n","                   5, 5, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n","                   3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5,\n","                   5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 7, 7, 7, 7,\n","                   7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7],\n","             mask=[False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False],\n","       fill_value='?',\n","            dtype=object), 'param_n_estimators': masked_array(data=[20, 20, 20, 20, 50, 50, 50, 50, 70, 70, 70, 70, 100,\n","                   100, 100, 100, 20, 20, 20, 20, 50, 50, 50, 50, 70, 70,\n","                   70, 70, 100, 100, 100, 100, 20, 20, 20, 20, 50, 50, 50,\n","                   50, 70, 70, 70, 70, 100, 100, 100, 100, 20, 20, 20, 20,\n","                   50, 50, 50, 50, 70, 70, 70, 70, 100, 100, 100, 100, 20,\n","                   20, 20, 20, 50, 50, 50, 50, 70, 70, 70, 70, 100, 100,\n","                   100, 100, 20, 20, 20, 20, 50, 50, 50, 50, 70, 70, 70,\n","                   70, 100, 100, 100, 100, 20, 20, 20, 20, 50, 50, 50, 50,\n","                   70, 70, 70, 70, 100, 100, 100, 100, 20, 20, 20, 20, 50,\n","                   50, 50, 50, 70, 70, 70, 70, 100, 100, 100, 100, 20, 20,\n","                   20, 20, 50, 50, 50, 50, 70, 70, 70, 70, 100, 100, 100,\n","                   100, 20, 20, 20, 20, 50, 50, 50, 50, 70, 70, 70, 70,\n","                   100, 100, 100, 100, 20, 20, 20, 20, 50, 50, 50, 50, 70,\n","                   70, 70, 70, 100, 100, 100, 100, 20, 20, 20, 20, 50, 50,\n","                   50, 50, 70, 70, 70, 70, 100, 100, 100, 100, 20, 20, 20,\n","                   20, 50, 50, 50, 50, 70, 70, 70, 70, 100, 100, 100, 100,\n","                   20, 20, 20, 20, 50, 50, 50, 50, 70, 70, 70, 70, 100,\n","                   100, 100, 100, 20, 20, 20, 20, 50, 50, 50, 50, 70, 70,\n","                   70, 70, 100, 100, 100, 100, 20, 20, 20, 20, 50, 50, 50,\n","                   50, 70, 70, 70, 70, 100, 100, 100, 100, 20, 20, 20, 20,\n","                   50, 50, 50, 50, 70, 70, 70, 70, 100, 100, 100, 100, 20,\n","                   20, 20, 20, 50, 50, 50, 50, 70, 70, 70, 70, 100, 100,\n","                   100, 100, 20, 20, 20, 20, 50, 50, 50, 50, 70, 70, 70,\n","                   70, 100, 100, 100, 100, 20, 20, 20, 20, 50, 50, 50, 50,\n","                   70, 70, 70, 70, 100, 100, 100, 100, 20, 20, 20, 20, 50,\n","                   50, 50, 50, 70, 70, 70, 70, 100, 100, 100, 100, 20, 20,\n","                   20, 20, 50, 50, 50, 50, 70, 70, 70, 70, 100, 100, 100,\n","                   100, 20, 20, 20, 20, 50, 50, 50, 50, 70, 70, 70, 70,\n","                   100, 100, 100, 100, 20, 20, 20, 20, 50, 50, 50, 50, 70,\n","                   70, 70, 70, 100, 100, 100, 100, 20, 20, 20, 20, 50, 50,\n","                   50, 50, 70, 70, 70, 70, 100, 100, 100, 100, 20, 20, 20,\n","                   20, 50, 50, 50, 50, 70, 70, 70, 70, 100, 100, 100, 100,\n","                   20, 20, 20, 20, 50, 50, 50, 50, 70, 70, 70, 70, 100,\n","                   100, 100, 100, 20, 20, 20, 20, 50, 50, 50, 50, 70, 70,\n","                   70, 70, 100, 100, 100, 100, 20, 20, 20, 20, 50, 50, 50,\n","                   50, 70, 70, 70, 70, 100, 100, 100, 100, 20, 20, 20, 20,\n","                   50, 50, 50, 50, 70, 70, 70, 70, 100, 100, 100, 100, 20,\n","                   20, 20, 20, 50, 50, 50, 50, 70, 70, 70, 70, 100, 100,\n","                   100, 100, 20, 20, 20, 20, 50, 50, 50, 50, 70, 70, 70,\n","                   70, 100, 100, 100, 100, 20, 20, 20, 20, 50, 50, 50, 50,\n","                   70, 70, 70, 70, 100, 100, 100, 100, 20, 20, 20, 20, 50,\n","                   50, 50, 50, 70, 70, 70, 70, 100, 100, 100, 100, 20, 20,\n","                   20, 20, 50, 50, 50, 50, 70, 70, 70, 70, 100, 100, 100,\n","                   100, 20, 20, 20, 20, 50, 50, 50, 50, 70, 70, 70, 70,\n","                   100, 100, 100, 100, 20, 20, 20, 20, 50, 50, 50, 50, 70,\n","                   70, 70, 70, 100, 100, 100, 100, 20, 20, 20, 20, 50, 50,\n","                   50, 50, 70, 70, 70, 70, 100, 100, 100, 100, 20, 20, 20,\n","                   20, 50, 50, 50, 50, 70, 70, 70, 70, 100, 100, 100, 100,\n","                   20, 20, 20, 20, 50, 50, 50, 50, 70, 70, 70, 70, 100,\n","                   100, 100, 100, 20, 20, 20, 20, 50, 50, 50, 50, 70, 70,\n","                   70, 70, 100, 100, 100, 100, 20, 20, 20, 20, 50, 50, 50,\n","                   50, 70, 70, 70, 70, 100, 100, 100, 100, 20, 20, 20, 20,\n","                   50, 50, 50, 50, 70, 70, 70, 70, 100, 100, 100, 100, 20,\n","                   20, 20, 20, 50, 50, 50, 50, 70, 70, 70, 70, 100, 100,\n","                   100, 100, 20, 20, 20, 20, 50, 50, 50, 50, 70, 70, 70,\n","                   70, 100, 100, 100, 100, 20, 20, 20, 20, 50, 50, 50, 50,\n","                   70, 70, 70, 70, 100, 100, 100, 100, 20, 20, 20, 20, 50,\n","                   50, 50, 50, 70, 70, 70, 70, 100, 100, 100, 100, 20, 20,\n","                   20, 20, 50, 50, 50, 50, 70, 70, 70, 70, 100, 100, 100,\n","                   100],\n","             mask=[False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False],\n","       fill_value='?',\n","            dtype=object), 'param_subsample': masked_array(data=[0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5,\n","                   0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7,\n","                   0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9,\n","                   0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2,\n","                   0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5,\n","                   0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7,\n","                   0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9,\n","                   0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2,\n","                   0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5,\n","                   0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7,\n","                   0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9,\n","                   0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2,\n","                   0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5,\n","                   0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7,\n","                   0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9,\n","                   0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2,\n","                   0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5,\n","                   0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7,\n","                   0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9,\n","                   0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2,\n","                   0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5,\n","                   0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7,\n","                   0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9,\n","                   0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2,\n","                   0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5,\n","                   0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7,\n","                   0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9,\n","                   0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2,\n","                   0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5,\n","                   0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7,\n","                   0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9,\n","                   0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2,\n","                   0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5,\n","                   0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7,\n","                   0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9,\n","                   0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2,\n","                   0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5,\n","                   0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7,\n","                   0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9,\n","                   0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2,\n","                   0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5,\n","                   0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7,\n","                   0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9,\n","                   0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2,\n","                   0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5,\n","                   0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7,\n","                   0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9,\n","                   0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2,\n","                   0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5,\n","                   0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7,\n","                   0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9,\n","                   0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2,\n","                   0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5,\n","                   0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7,\n","                   0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9,\n","                   0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2,\n","                   0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5,\n","                   0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7,\n","                   0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9,\n","                   0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2,\n","                   0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5,\n","                   0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7,\n","                   0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9,\n","                   0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2,\n","                   0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5,\n","                   0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7,\n","                   0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9,\n","                   0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2,\n","                   0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5,\n","                   0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2],\n","             mask=[False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False],\n","       fill_value='?',\n","            dtype=object), 'params': [{'learning_rate': 0.01, 'max_depth': 4, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.2}], 'split0_test_score': array([0.66666667, 0.66666667, 0.66666667, 0.66666667, 0.88888889,\n","       0.88888889, 0.87407407, 0.83703704, 0.91111111, 0.9037037 ,\n","       0.9037037 , 0.86666667, 0.93333333, 0.91851852, 0.9037037 ,\n","       0.88148148, 0.66666667, 0.66666667, 0.66666667, 0.66666667,\n","       0.92592593, 0.91851852, 0.8962963 , 0.87407407, 0.91851852,\n","       0.93333333, 0.93333333, 0.86666667, 0.93333333, 0.94814815,\n","       0.94074074, 0.91851852, 0.66666667, 0.66666667, 0.66666667,\n","       0.66666667, 0.91111111, 0.93333333, 0.91111111, 0.82222222,\n","       0.94074074, 0.93333333, 0.9037037 , 0.88888889, 0.94074074,\n","       0.92592593, 0.92592593, 0.9037037 , 0.66666667, 0.66666667,\n","       0.66666667, 0.66666667, 0.88888889, 0.88148148, 0.88148148,\n","       0.87407407, 0.91851852, 0.9037037 , 0.8962963 , 0.85185185,\n","       0.91851852, 0.92592593, 0.91851852, 0.8962963 , 0.66666667,\n","       0.66666667, 0.66666667, 0.66666667, 0.91111111, 0.88888889,\n","       0.88148148, 0.82222222, 0.93333333, 0.93333333, 0.92592593,\n","       0.88148148, 0.94814815, 0.93333333, 0.94814815, 0.88888889,\n","       0.66666667, 0.66666667, 0.66666667, 0.66666667, 0.94074074,\n","       0.9037037 , 0.9037037 , 0.85925926, 0.93333333, 0.92592593,\n","       0.9037037 , 0.87407407, 0.95555556, 0.94814815, 0.94814815,\n","       0.88148148, 0.66666667, 0.66666667, 0.66666667, 0.66666667,\n","       0.8962963 , 0.88148148, 0.87407407, 0.87407407, 0.91851852,\n","       0.8962963 , 0.88888889, 0.86666667, 0.91851852, 0.91851852,\n","       0.93333333, 0.8962963 , 0.66666667, 0.66666667, 0.66666667,\n","       0.66666667, 0.93333333, 0.91111111, 0.88888889, 0.86666667,\n","       0.92592593, 0.92592593, 0.91851852, 0.88148148, 0.93333333,\n","       0.93333333, 0.96296296, 0.88888889, 0.66666667, 0.66666667,\n","       0.66666667, 0.66666667, 0.94814815, 0.92592593, 0.9037037 ,\n","       0.84444444, 0.94814815, 0.91111111, 0.93333333, 0.88148148,\n","       0.93333333, 0.93333333, 0.91851852, 0.88888889, 0.66666667,\n","       0.66666667, 0.66666667, 0.66666667, 0.88148148, 0.88148148,\n","       0.88888889, 0.82222222, 0.91111111, 0.9037037 , 0.9037037 ,\n","       0.88148148, 0.91851852, 0.91111111, 0.91851852, 0.88888889,\n","       0.66666667, 0.66666667, 0.66666667, 0.66666667, 0.91851852,\n","       0.91851852, 0.88888889, 0.85925926, 0.91851852, 0.93333333,\n","       0.91851852, 0.88888889, 0.93333333, 0.91851852, 0.93333333,\n","       0.8962963 , 0.66666667, 0.66666667, 0.66666667, 0.66666667,\n","       0.92592593, 0.91111111, 0.91111111, 0.84444444, 0.93333333,\n","       0.92592593, 0.91851852, 0.86666667, 0.95555556, 0.94074074,\n","       0.91111111, 0.8962963 , 0.88888889, 0.85185185, 0.85185185,\n","       0.8       , 0.94074074, 0.93333333, 0.9037037 , 0.8962963 ,\n","       0.93333333, 0.92592593, 0.9037037 , 0.88888889, 0.94814815,\n","       0.92592593, 0.91851852, 0.88888889, 0.8962963 , 0.91111111,\n","       0.88148148, 0.83703704, 0.95555556, 0.94074074, 0.92592593,\n","       0.9037037 , 0.94074074, 0.94814815, 0.93333333, 0.9037037 ,\n","       0.96296296, 0.97037037, 0.97037037, 0.9037037 , 0.94814815,\n","       0.91851852, 0.82962963, 0.80740741, 0.94074074, 0.93333333,\n","       0.93333333, 0.91851852, 0.92592593, 0.95555556, 0.95555556,\n","       0.94074074, 0.94074074, 0.96296296, 0.93333333, 0.94074074,\n","       0.86666667, 0.86666667, 0.87407407, 0.77777778, 0.91111111,\n","       0.91851852, 0.91851852, 0.88148148, 0.92592593, 0.92592593,\n","       0.91851852, 0.9037037 , 0.92592593, 0.94074074, 0.93333333,\n","       0.8962963 , 0.9037037 , 0.88888889, 0.87407407, 0.8       ,\n","       0.94814815, 0.94074074, 0.88888889, 0.88148148, 0.92592593,\n","       0.96296296, 0.94074074, 0.91111111, 0.94074074, 0.95555556,\n","       0.95555556, 0.91111111, 0.91851852, 0.91111111, 0.9037037 ,\n","       0.81481481, 0.92592593, 0.93333333, 0.94814815, 0.87407407,\n","       0.93333333, 0.94074074, 0.94074074, 0.8962963 , 0.93333333,\n","       0.95555556, 0.96296296, 0.93333333, 0.87407407, 0.88888889,\n","       0.82222222, 0.82962963, 0.92592593, 0.92592593, 0.9037037 ,\n","       0.9037037 , 0.91851852, 0.93333333, 0.94814815, 0.8962963 ,\n","       0.92592593, 0.92592593, 0.92592593, 0.92592593, 0.91111111,\n","       0.88148148, 0.86666667, 0.85925926, 0.92592593, 0.93333333,\n","       0.92592593, 0.8962963 , 0.95555556, 0.95555556, 0.94814815,\n","       0.87407407, 0.94074074, 0.95555556, 0.94814815, 0.9037037 ,\n","       0.91111111, 0.9037037 , 0.91111111, 0.8       , 0.94814815,\n","       0.91851852, 0.95555556, 0.88888889, 0.93333333, 0.94814815,\n","       0.94074074, 0.93333333, 0.94074074, 0.95555556, 0.97037037,\n","       0.94814815, 0.88148148, 0.87407407, 0.88148148, 0.75555556,\n","       0.91851852, 0.8962963 , 0.92592593, 0.88148148, 0.92592593,\n","       0.93333333, 0.92592593, 0.88888889, 0.94074074, 0.93333333,\n","       0.92592593, 0.93333333, 0.8962963 , 0.87407407, 0.86666667,\n","       0.82962963, 0.93333333, 0.93333333, 0.94814815, 0.86666667,\n","       0.94814815, 0.93333333, 0.94814815, 0.91111111, 0.94074074,\n","       0.95555556, 0.93333333, 0.92592593, 0.93333333, 0.9037037 ,\n","       0.8962963 , 0.84444444, 0.93333333, 0.92592593, 0.93333333,\n","       0.9037037 , 0.94074074, 0.92592593, 0.96296296, 0.92592593,\n","       0.94074074, 0.94814815, 0.95555556, 0.94074074, 0.9037037 ,\n","       0.88888889, 0.9037037 , 0.84444444, 0.9037037 , 0.93333333,\n","       0.93333333, 0.88888889, 0.94814815, 0.93333333, 0.91851852,\n","       0.91111111, 0.94074074, 0.94074074, 0.91851852, 0.91851852,\n","       0.93333333, 0.91851852, 0.8962963 , 0.82962963, 0.93333333,\n","       0.95555556, 0.95555556, 0.91111111, 0.96296296, 0.94814815,\n","       0.95555556, 0.9037037 , 0.96296296, 0.96296296, 0.96296296,\n","       0.91111111, 0.91851852, 0.91851852, 0.91111111, 0.82962963,\n","       0.94074074, 0.93333333, 0.94074074, 0.94814815, 0.93333333,\n","       0.96296296, 0.95555556, 0.93333333, 0.96296296, 0.96296296,\n","       0.97037037, 0.93333333, 0.91111111, 0.91111111, 0.8962963 ,\n","       0.85925926, 0.92592593, 0.94814815, 0.94814815, 0.88888889,\n","       0.93333333, 0.92592593, 0.92592593, 0.91111111, 0.94074074,\n","       0.93333333, 0.92592593, 0.91851852, 0.91851852, 0.91851852,\n","       0.8962963 , 0.87407407, 0.94814815, 0.94814815, 0.96296296,\n","       0.9037037 , 0.94074074, 0.97037037, 0.95555556, 0.91111111,\n","       0.94074074, 0.95555556, 0.96296296, 0.91111111, 0.95555556,\n","       0.91851852, 0.9037037 , 0.84444444, 0.93333333, 0.91851852,\n","       0.95555556, 0.92592593, 0.93333333, 0.96296296, 0.95555556,\n","       0.9037037 , 0.93333333, 0.95555556, 0.97037037, 0.91111111,\n","       0.91111111, 0.8962963 , 0.88148148, 0.85185185, 0.94074074,\n","       0.92592593, 0.94074074, 0.91111111, 0.93333333, 0.94074074,\n","       0.93333333, 0.8962963 , 0.92592593, 0.94074074, 0.92592593,\n","       0.92592593, 0.92592593, 0.9037037 , 0.91111111, 0.88888889,\n","       0.92592593, 0.94814815, 0.93333333, 0.88148148, 0.9037037 ,\n","       0.95555556, 0.95555556, 0.94074074, 0.94814815, 0.96296296,\n","       0.96296296, 0.91111111, 0.94074074, 0.92592593, 0.91111111,\n","       0.9037037 , 0.94074074, 0.96296296, 0.94814815, 0.94074074,\n","       0.94074074, 0.94074074, 0.94814815, 0.91111111, 0.93333333,\n","       0.96296296, 0.96296296, 0.93333333, 0.9037037 , 0.8962963 ,\n","       0.91111111, 0.85185185, 0.91851852, 0.94074074, 0.9037037 ,\n","       0.8962963 , 0.92592593, 0.92592593, 0.91851852, 0.8962963 ,\n","       0.92592593, 0.94814815, 0.94814815, 0.88888889, 0.93333333,\n","       0.95555556, 0.8962963 , 0.88148148, 0.94074074, 0.95555556,\n","       0.94814815, 0.91111111, 0.95555556, 0.95555556, 0.95555556,\n","       0.92592593, 0.92592593, 0.96296296, 0.95555556, 0.91851852,\n","       0.9037037 , 0.91111111, 0.9037037 , 0.82962963, 0.94074074,\n","       0.92592593, 0.97037037, 0.9037037 , 0.95555556, 0.95555556,\n","       0.96296296, 0.93333333, 0.94074074, 0.95555556, 0.96296296,\n","       0.93333333, 0.94814815, 0.91111111, 0.8962963 , 0.8962963 ,\n","       0.92592593, 0.92592593, 0.93333333, 0.91111111, 0.97037037,\n","       0.94814815, 0.95555556, 0.91851852, 0.93333333, 0.93333333,\n","       0.93333333, 0.9037037 , 0.92592593, 0.8962963 , 0.9037037 ,\n","       0.91111111, 0.96296296, 0.97037037, 0.96296296, 0.94074074,\n","       0.97037037, 0.97037037, 0.93333333, 0.92592593, 0.96296296,\n","       0.97037037, 0.96296296, 0.91851852, 0.94074074, 0.92592593,\n","       0.91851852, 0.84444444, 0.94074074, 0.96296296, 0.97037037,\n","       0.88888889, 0.96296296, 0.94814815, 0.94814815, 0.93333333,\n","       0.96296296, 0.96296296, 0.96296296, 0.94814815, 0.92592593,\n","       0.91851852, 0.8962963 , 0.8962963 , 0.92592593, 0.94074074,\n","       0.94074074, 0.93333333, 0.92592593, 0.92592593, 0.94814815,\n","       0.9037037 , 0.94814815, 0.94074074, 0.94814815, 0.91111111,\n","       0.91851852, 0.92592593, 0.91111111, 0.9037037 , 0.93333333,\n","       0.93333333, 0.96296296, 0.92592593, 0.93333333, 0.94814815,\n","       0.94814815, 0.94074074, 0.95555556, 0.97037037, 0.94814815,\n","       0.92592593, 0.94074074, 0.91851852, 0.91851852, 0.88148148,\n","       0.94074074, 0.95555556, 0.94814815, 0.93333333, 0.93333333,\n","       0.97037037, 0.97037037, 0.94814815, 0.92592593, 0.95555556,\n","       0.97037037, 0.93333333, 0.88888889, 0.8962963 , 0.91111111,\n","       0.87407407, 0.9037037 , 0.91851852, 0.91851852, 0.8962963 ,\n","       0.91111111, 0.93333333, 0.95555556, 0.91851852, 0.94074074,\n","       0.94814815, 0.92592593, 0.91111111, 0.91111111, 0.92592593,\n","       0.91111111, 0.91851852, 0.94074074, 0.95555556, 0.94814815,\n","       0.91111111, 0.93333333, 0.94814815, 0.94074074, 0.91851852,\n","       0.93333333, 0.96296296, 0.97037037, 0.93333333, 0.95555556,\n","       0.91851852, 0.91851852, 0.85925926, 0.94074074, 0.95555556,\n","       0.95555556, 0.9037037 , 0.95555556, 0.94074074, 0.94814815,\n","       0.9037037 , 0.94074074, 0.96296296, 0.97037037, 0.91851852,\n","       0.91111111, 0.92592593, 0.9037037 , 0.88148148, 0.8962963 ,\n","       0.93333333, 0.94074074, 0.94074074, 0.94074074, 0.91111111,\n","       0.91851852, 0.94814815, 0.94074074, 0.94814815, 0.92592593,\n","       0.92592593, 0.91111111, 0.91851852, 0.91111111, 0.9037037 ,\n","       0.94074074, 0.95555556, 0.94814815, 0.91111111, 0.94074074,\n","       0.96296296, 0.97037037, 0.91851852, 0.94814815, 0.97037037,\n","       0.96296296, 0.93333333, 0.95555556, 0.9037037 , 0.93333333,\n","       0.87407407, 0.94814815, 0.96296296, 0.92592593, 0.93333333,\n","       0.93333333, 0.97037037, 0.96296296, 0.92592593, 0.94074074,\n","       0.95555556, 0.97037037, 0.93333333]), 'split1_test_score': array([0.66666667, 0.66666667, 0.66666667, 0.66666667, 0.94814815,\n","       0.93333333, 0.94814815, 0.92592593, 0.93333333, 0.93333333,\n","       0.93333333, 0.93333333, 0.93333333, 0.93333333, 0.93333333,\n","       0.94814815, 0.66666667, 0.66666667, 0.66666667, 0.66666667,\n","       0.93333333, 0.93333333, 0.91851852, 0.93333333, 0.93333333,\n","       0.93333333, 0.93333333, 0.91851852, 0.93333333, 0.93333333,\n","       0.93333333, 0.91851852, 0.66666667, 0.66666667, 0.66666667,\n","       0.66666667, 0.93333333, 0.92592593, 0.91851852, 0.91111111,\n","       0.93333333, 0.92592593, 0.93333333, 0.93333333, 0.91111111,\n","       0.92592593, 0.92592593, 0.93333333, 0.66666667, 0.66666667,\n","       0.66666667, 0.66666667, 0.94814815, 0.93333333, 0.93333333,\n","       0.95555556, 0.93333333, 0.93333333, 0.94814815, 0.94814815,\n","       0.93333333, 0.93333333, 0.93333333, 0.93333333, 0.66666667,\n","       0.66666667, 0.66666667, 0.66666667, 0.92592593, 0.93333333,\n","       0.91851852, 0.93333333, 0.91851852, 0.93333333, 0.93333333,\n","       0.88888889, 0.86666667, 0.93333333, 0.93333333, 0.93333333,\n","       0.66666667, 0.66666667, 0.66666667, 0.66666667, 0.87407407,\n","       0.92592593, 0.91851852, 0.92592593, 0.87407407, 0.91111111,\n","       0.93333333, 0.91851852, 0.85185185, 0.86666667, 0.93333333,\n","       0.93333333, 0.66666667, 0.66666667, 0.66666667, 0.66666667,\n","       0.94814815, 0.94814815, 0.93333333, 0.8962963 , 0.93333333,\n","       0.93333333, 0.94814815, 0.94814815, 0.93333333, 0.93333333,\n","       0.93333333, 0.93333333, 0.66666667, 0.66666667, 0.66666667,\n","       0.66666667, 0.93333333, 0.93333333, 0.91851852, 0.93333333,\n","       0.93333333, 0.93333333, 0.93333333, 0.94814815, 0.93333333,\n","       0.92592593, 0.93333333, 0.93333333, 0.66666667, 0.66666667,\n","       0.66666667, 0.66666667, 0.87407407, 0.9037037 , 0.91851852,\n","       0.92592593, 0.86666667, 0.86666667, 0.91851852, 0.91851852,\n","       0.85925926, 0.86666667, 0.92592593, 0.91851852, 0.66666667,\n","       0.66666667, 0.66666667, 0.66666667, 0.93333333, 0.93333333,\n","       0.94814815, 0.96296296, 0.93333333, 0.93333333, 0.93333333,\n","       0.92592593, 0.93333333, 0.93333333, 0.93333333, 0.93333333,\n","       0.66666667, 0.66666667, 0.66666667, 0.66666667, 0.93333333,\n","       0.93333333, 0.91851852, 0.92592593, 0.92592593, 0.93333333,\n","       0.93333333, 0.91851852, 0.92592593, 0.93333333, 0.93333333,\n","       0.93333333, 0.66666667, 0.66666667, 0.66666667, 0.66666667,\n","       0.87407407, 0.93333333, 0.91851852, 0.95555556, 0.86666667,\n","       0.93333333, 0.91851852, 0.93333333, 0.85185185, 0.88148148,\n","       0.92592593, 0.91851852, 0.94074074, 0.95555556, 0.93333333,\n","       0.85925926, 0.93333333, 0.93333333, 0.93333333, 0.94814815,\n","       0.93333333, 0.93333333, 0.93333333, 0.93333333, 0.93333333,\n","       0.94074074, 0.93333333, 0.93333333, 0.93333333, 0.91851852,\n","       0.91111111, 0.91851852, 0.93333333, 0.92592593, 0.93333333,\n","       0.93333333, 0.92592593, 0.93333333, 0.93333333, 0.93333333,\n","       0.93333333, 0.93333333, 0.92592593, 0.93333333, 0.93333333,\n","       0.91851852, 0.91851852, 0.93333333, 0.92592593, 0.92592593,\n","       0.93333333, 0.91851852, 0.91111111, 0.93333333, 0.92592593,\n","       0.93333333, 0.86666667, 0.86666667, 0.93333333, 0.93333333,\n","       0.94814815, 0.97037037, 0.91851852, 0.88148148, 0.93333333,\n","       0.93333333, 0.93333333, 0.94814815, 0.93333333, 0.93333333,\n","       0.93333333, 0.93333333, 0.93333333, 0.93333333, 0.93333333,\n","       0.93333333, 0.91851852, 0.91851852, 0.93333333, 0.88888889,\n","       0.91851852, 0.93333333, 0.93333333, 0.94814815, 0.87407407,\n","       0.91851852, 0.93333333, 0.93333333, 0.86666667, 0.93333333,\n","       0.94074074, 0.93333333, 0.87407407, 0.91851852, 0.94814815,\n","       0.93333333, 0.86666667, 0.86666667, 0.92592593, 0.93333333,\n","       0.85185185, 0.91851852, 0.92592593, 0.93333333, 0.85185185,\n","       0.86666667, 0.92592593, 0.93333333, 0.94814815, 0.94074074,\n","       0.93333333, 0.85925926, 0.93333333, 0.93333333, 0.93333333,\n","       0.93333333, 0.93333333, 0.93333333, 0.93333333, 0.94814815,\n","       0.92592593, 0.93333333, 0.93333333, 0.93333333, 0.94074074,\n","       0.93333333, 0.93333333, 0.87407407, 0.91851852, 0.93333333,\n","       0.93333333, 0.93333333, 0.93333333, 0.92592593, 0.93333333,\n","       0.93333333, 0.92592593, 0.93333333, 0.93333333, 0.93333333,\n","       0.87407407, 0.91851852, 0.91851852, 0.94814815, 0.85185185,\n","       0.88148148, 0.93333333, 0.93333333, 0.86666667, 0.86666667,\n","       0.93333333, 0.93333333, 0.85925926, 0.86666667, 0.92592593,\n","       0.93333333, 0.93333333, 0.94814815, 0.93333333, 0.9037037 ,\n","       0.93333333, 0.93333333, 0.93333333, 0.94814815, 0.93333333,\n","       0.93333333, 0.93333333, 0.93333333, 0.93333333, 0.93333333,\n","       0.93333333, 0.93333333, 0.92592593, 0.93333333, 0.91851852,\n","       0.87407407, 0.9037037 , 0.92592593, 0.93333333, 0.91851852,\n","       0.93333333, 0.93333333, 0.93333333, 0.93333333, 0.92592593,\n","       0.92592593, 0.93333333, 0.93333333, 0.87407407, 0.92592593,\n","       0.91851852, 0.85185185, 0.85185185, 0.86666667, 0.93333333,\n","       0.93333333, 0.86666667, 0.91111111, 0.93333333, 0.93333333,\n","       0.85185185, 0.86666667, 0.91851852, 0.93333333, 0.93333333,\n","       0.94814815, 0.94814815, 0.92592593, 0.93333333, 0.94074074,\n","       0.93333333, 0.94814815, 0.94074074, 0.94074074, 0.93333333,\n","       0.93333333, 0.94074074, 0.94074074, 0.94074074, 0.93333333,\n","       0.92592593, 0.93333333, 0.93333333, 0.93333333, 0.92592593,\n","       0.93333333, 0.92592593, 0.93333333, 0.92592593, 0.94074074,\n","       0.93333333, 0.93333333, 0.91851852, 0.92592593, 0.93333333,\n","       0.93333333, 0.91111111, 0.92592593, 0.91851852, 0.91111111,\n","       0.91111111, 0.93333333, 0.92592593, 0.91851852, 0.91851852,\n","       0.91851852, 0.93333333, 0.93333333, 0.86666667, 0.92592593,\n","       0.91111111, 0.93333333, 0.93333333, 0.93333333, 0.92592593,\n","       0.88148148, 0.93333333, 0.93333333, 0.93333333, 0.93333333,\n","       0.93333333, 0.93333333, 0.93333333, 0.94814815, 0.93333333,\n","       0.93333333, 0.93333333, 0.93333333, 0.93333333, 0.93333333,\n","       0.91851852, 0.94814815, 0.86666667, 0.91111111, 0.92592593,\n","       0.93333333, 0.86666667, 0.92592593, 0.94074074, 0.93333333,\n","       0.86666667, 0.92592593, 0.93333333, 0.93333333, 0.87407407,\n","       0.93333333, 0.94814815, 0.91851852, 0.85185185, 0.86666667,\n","       0.92592593, 0.93333333, 0.85185185, 0.86666667, 0.8962963 ,\n","       0.93333333, 0.85925926, 0.86666667, 0.92592593, 0.93333333,\n","       0.94814815, 0.93333333, 0.93333333, 0.93333333, 0.93333333,\n","       0.93333333, 0.93333333, 0.93333333, 0.92592593, 0.93333333,\n","       0.93333333, 0.93333333, 0.93333333, 0.93333333, 0.93333333,\n","       0.93333333, 0.92592593, 0.93333333, 0.91851852, 0.93333333,\n","       0.92592593, 0.93333333, 0.93333333, 0.92592593, 0.8962963 ,\n","       0.92592593, 0.93333333, 0.93333333, 0.92592593, 0.94074074,\n","       0.93333333, 0.93333333, 0.86666667, 0.87407407, 0.91851852,\n","       0.94074074, 0.85185185, 0.91111111, 0.93333333, 0.93333333,\n","       0.86666667, 0.86666667, 0.93333333, 0.93333333, 0.85925926,\n","       0.86666667, 0.93333333, 0.93333333, 0.93333333, 0.94814815,\n","       0.93333333, 0.94074074, 0.93333333, 0.93333333, 0.93333333,\n","       0.93333333, 0.93333333, 0.93333333, 0.93333333, 0.93333333,\n","       0.93333333, 0.93333333, 0.93333333, 0.93333333, 0.93333333,\n","       0.93333333, 0.93333333, 0.93333333, 0.9037037 , 0.93333333,\n","       0.93333333, 0.93333333, 0.92592593, 0.93333333, 0.93333333,\n","       0.93333333, 0.91111111, 0.91851852, 0.93333333, 0.93333333,\n","       0.87407407, 0.93333333, 0.93333333, 0.91111111, 0.85925926,\n","       0.87407407, 0.94074074, 0.93333333, 0.85185185, 0.88148148,\n","       0.93333333, 0.93333333, 0.85925926, 0.86666667, 0.86666667,\n","       0.93333333, 0.94814815, 0.93333333, 0.93333333, 0.87407407,\n","       0.93333333, 0.93333333, 0.93333333, 0.93333333, 0.94074074,\n","       0.94074074, 0.93333333, 0.92592593, 0.93333333, 0.94074074,\n","       0.94074074, 0.93333333, 0.93333333, 0.93333333, 0.93333333,\n","       0.93333333, 0.92592593, 0.93333333, 0.92592593, 0.93333333,\n","       0.91851852, 0.93333333, 0.93333333, 0.93333333, 0.91111111,\n","       0.93333333, 0.93333333, 0.93333333, 0.92592593, 0.92592593,\n","       0.95555556, 0.91111111, 0.91851852, 0.93333333, 0.92592593,\n","       0.93333333, 0.85925926, 0.92592593, 0.93333333, 0.93333333,\n","       0.86666667, 0.88148148, 0.93333333, 0.93333333, 0.93333333,\n","       0.93333333, 0.94814815, 0.94814815, 0.94074074, 0.93333333,\n","       0.93333333, 0.93333333, 0.94074074, 0.93333333, 0.93333333,\n","       0.93333333, 0.94074074, 0.94074074, 0.93333333, 0.93333333,\n","       0.92592593, 0.91851852, 0.91851852, 0.93333333, 0.86666667,\n","       0.91851852, 0.92592593, 0.93333333, 0.86666667, 0.92592593,\n","       0.94074074, 0.93333333, 0.85925926, 0.93333333, 0.93333333,\n","       0.93333333, 0.86666667, 0.94074074, 0.93333333, 0.92592593,\n","       0.85185185, 0.86666667, 0.93333333, 0.93333333, 0.85185185,\n","       0.86666667, 0.92592593, 0.93333333, 0.85185185, 0.8962963 ,\n","       0.91111111, 0.93333333, 0.93333333, 0.93333333, 0.94814815,\n","       0.94814815, 0.93333333, 0.93333333, 0.93333333, 0.93333333,\n","       0.94074074, 0.93333333, 0.93333333, 0.92592593, 0.94074074,\n","       0.93333333, 0.93333333, 0.93333333, 0.91851852, 0.93333333,\n","       0.93333333, 0.93333333, 0.85925926, 0.8962963 , 0.93333333,\n","       0.93333333, 0.87407407, 0.93333333, 0.93333333, 0.93333333,\n","       0.86666667, 0.92592593, 0.92592593, 0.93333333, 0.86666667,\n","       0.85185185, 0.91111111, 0.92592593, 0.86666667, 0.86666667,\n","       0.93333333, 0.93333333, 0.85925926, 0.86666667, 0.88148148,\n","       0.93333333, 0.85925926, 0.91111111, 0.88148148, 0.93333333,\n","       0.93333333, 0.93333333, 0.93333333, 0.94814815, 0.93333333,\n","       0.93333333, 0.93333333, 0.93333333, 0.93333333, 0.93333333,\n","       0.93333333, 0.92592593, 0.93333333, 0.93333333, 0.93333333,\n","       0.92592593, 0.93333333, 0.93333333, 0.93333333, 0.92592593,\n","       0.86666667, 0.92592593, 0.93333333, 0.93333333, 0.9037037 ,\n","       0.93333333, 0.93333333, 0.93333333, 0.86666667, 0.93333333,\n","       0.94074074, 0.93333333, 0.87407407, 0.87407407, 0.94074074,\n","       0.91851852, 0.86666667, 0.86666667, 0.93333333, 0.93333333,\n","       0.85925926, 0.91851852, 0.88148148, 0.93333333, 0.85925926,\n","       0.86666667, 0.94074074, 0.91851852]), 'mean_test_score': array([0.66666667, 0.66666667, 0.66666667, 0.66666667, 0.91851852,\n","       0.91111111, 0.91111111, 0.88148148, 0.92222222, 0.91851852,\n","       0.91851852, 0.9       , 0.93333333, 0.92592593, 0.91851852,\n","       0.91481481, 0.66666667, 0.66666667, 0.66666667, 0.66666667,\n","       0.92962963, 0.92592593, 0.90740741, 0.9037037 , 0.92592593,\n","       0.93333333, 0.93333333, 0.89259259, 0.93333333, 0.94074074,\n","       0.93703704, 0.91851852, 0.66666667, 0.66666667, 0.66666667,\n","       0.66666667, 0.92222222, 0.92962963, 0.91481481, 0.86666667,\n","       0.93703704, 0.92962963, 0.91851852, 0.91111111, 0.92592593,\n","       0.92592593, 0.92592593, 0.91851852, 0.66666667, 0.66666667,\n","       0.66666667, 0.66666667, 0.91851852, 0.90740741, 0.90740741,\n","       0.91481481, 0.92592593, 0.91851852, 0.92222222, 0.9       ,\n","       0.92592593, 0.92962963, 0.92592593, 0.91481481, 0.66666667,\n","       0.66666667, 0.66666667, 0.66666667, 0.91851852, 0.91111111,\n","       0.9       , 0.87777778, 0.92592593, 0.93333333, 0.92962963,\n","       0.88518519, 0.90740741, 0.93333333, 0.94074074, 0.91111111,\n","       0.66666667, 0.66666667, 0.66666667, 0.66666667, 0.90740741,\n","       0.91481481, 0.91111111, 0.89259259, 0.9037037 , 0.91851852,\n","       0.91851852, 0.8962963 , 0.9037037 , 0.90740741, 0.94074074,\n","       0.90740741, 0.66666667, 0.66666667, 0.66666667, 0.66666667,\n","       0.92222222, 0.91481481, 0.9037037 , 0.88518519, 0.92592593,\n","       0.91481481, 0.91851852, 0.90740741, 0.92592593, 0.92592593,\n","       0.93333333, 0.91481481, 0.66666667, 0.66666667, 0.66666667,\n","       0.66666667, 0.93333333, 0.92222222, 0.9037037 , 0.9       ,\n","       0.92962963, 0.92962963, 0.92592593, 0.91481481, 0.93333333,\n","       0.92962963, 0.94814815, 0.91111111, 0.66666667, 0.66666667,\n","       0.66666667, 0.66666667, 0.91111111, 0.91481481, 0.91111111,\n","       0.88518519, 0.90740741, 0.88888889, 0.92592593, 0.9       ,\n","       0.8962963 , 0.9       , 0.92222222, 0.9037037 , 0.66666667,\n","       0.66666667, 0.66666667, 0.66666667, 0.90740741, 0.90740741,\n","       0.91851852, 0.89259259, 0.92222222, 0.91851852, 0.91851852,\n","       0.9037037 , 0.92592593, 0.92222222, 0.92592593, 0.91111111,\n","       0.66666667, 0.66666667, 0.66666667, 0.66666667, 0.92592593,\n","       0.92592593, 0.9037037 , 0.89259259, 0.92222222, 0.93333333,\n","       0.92592593, 0.9037037 , 0.92962963, 0.92592593, 0.93333333,\n","       0.91481481, 0.66666667, 0.66666667, 0.66666667, 0.66666667,\n","       0.9       , 0.92222222, 0.91481481, 0.9       , 0.9       ,\n","       0.92962963, 0.91851852, 0.9       , 0.9037037 , 0.91111111,\n","       0.91851852, 0.90740741, 0.91481481, 0.9037037 , 0.89259259,\n","       0.82962963, 0.93703704, 0.93333333, 0.91851852, 0.92222222,\n","       0.93333333, 0.92962963, 0.91851852, 0.91111111, 0.94074074,\n","       0.93333333, 0.92592593, 0.91111111, 0.91481481, 0.91481481,\n","       0.8962963 , 0.87777778, 0.94444444, 0.93333333, 0.92962963,\n","       0.91851852, 0.93333333, 0.94074074, 0.93333333, 0.91851852,\n","       0.94814815, 0.95185185, 0.94814815, 0.91851852, 0.94074074,\n","       0.91851852, 0.87407407, 0.87037037, 0.93333333, 0.92962963,\n","       0.93333333, 0.91851852, 0.91851852, 0.94444444, 0.94074074,\n","       0.93703704, 0.9037037 , 0.91481481, 0.93333333, 0.93703704,\n","       0.90740741, 0.91851852, 0.8962963 , 0.82962963, 0.92222222,\n","       0.92592593, 0.92592593, 0.91481481, 0.92962963, 0.92962963,\n","       0.92592593, 0.91851852, 0.92962963, 0.93703704, 0.93333333,\n","       0.91481481, 0.91111111, 0.9037037 , 0.9037037 , 0.84444444,\n","       0.93333333, 0.93703704, 0.91111111, 0.91481481, 0.9       ,\n","       0.94074074, 0.93703704, 0.92222222, 0.9037037 , 0.94444444,\n","       0.94814815, 0.92222222, 0.8962963 , 0.91481481, 0.92592593,\n","       0.87407407, 0.8962963 , 0.9       , 0.93703704, 0.9037037 ,\n","       0.89259259, 0.92962963, 0.93333333, 0.91481481, 0.89259259,\n","       0.91111111, 0.94444444, 0.93333333, 0.91111111, 0.91481481,\n","       0.87777778, 0.84444444, 0.92962963, 0.92962963, 0.91851852,\n","       0.91851852, 0.92592593, 0.93333333, 0.94074074, 0.92222222,\n","       0.92592593, 0.92962963, 0.92962963, 0.92962963, 0.92592593,\n","       0.90740741, 0.9       , 0.86666667, 0.92222222, 0.93333333,\n","       0.92962963, 0.91481481, 0.94444444, 0.94074074, 0.94074074,\n","       0.9037037 , 0.93333333, 0.94444444, 0.94074074, 0.91851852,\n","       0.89259259, 0.91111111, 0.91481481, 0.87407407, 0.9       ,\n","       0.9       , 0.94444444, 0.91111111, 0.9       , 0.90740741,\n","       0.93703704, 0.93333333, 0.9       , 0.91111111, 0.94814815,\n","       0.94074074, 0.90740741, 0.91111111, 0.90740741, 0.82962963,\n","       0.92592593, 0.91481481, 0.92962963, 0.91481481, 0.92962963,\n","       0.93333333, 0.92962963, 0.91111111, 0.93703704, 0.93333333,\n","       0.92962963, 0.93333333, 0.91111111, 0.9037037 , 0.89259259,\n","       0.85185185, 0.91851852, 0.92962963, 0.94074074, 0.89259259,\n","       0.94074074, 0.93333333, 0.94074074, 0.92222222, 0.93333333,\n","       0.94074074, 0.93333333, 0.92962963, 0.9037037 , 0.91481481,\n","       0.90740741, 0.84814815, 0.89259259, 0.8962963 , 0.93333333,\n","       0.91851852, 0.9037037 , 0.91851852, 0.94814815, 0.92962963,\n","       0.8962963 , 0.90740741, 0.93703704, 0.93703704, 0.91851852,\n","       0.91851852, 0.92592593, 0.88518519, 0.91851852, 0.93703704,\n","       0.93333333, 0.91851852, 0.94444444, 0.93703704, 0.92592593,\n","       0.92222222, 0.94074074, 0.94074074, 0.92962963, 0.92592593,\n","       0.92962963, 0.92592593, 0.91481481, 0.88148148, 0.92962963,\n","       0.94444444, 0.94074074, 0.92222222, 0.94444444, 0.94444444,\n","       0.94444444, 0.91851852, 0.94074074, 0.94444444, 0.94814815,\n","       0.92222222, 0.91481481, 0.92222222, 0.91481481, 0.87037037,\n","       0.92592593, 0.93333333, 0.93333333, 0.93333333, 0.92592593,\n","       0.94074074, 0.94444444, 0.93333333, 0.91481481, 0.94444444,\n","       0.94074074, 0.93333333, 0.92222222, 0.92222222, 0.91111111,\n","       0.87037037, 0.92962963, 0.94074074, 0.94074074, 0.91111111,\n","       0.93333333, 0.92962963, 0.92962963, 0.92962963, 0.93703704,\n","       0.93333333, 0.92962963, 0.92592593, 0.92592593, 0.92592593,\n","       0.90740741, 0.91111111, 0.90740741, 0.92962963, 0.94444444,\n","       0.91851852, 0.9037037 , 0.94814815, 0.94814815, 0.92222222,\n","       0.9037037 , 0.94074074, 0.94814815, 0.92222222, 0.91481481,\n","       0.92592593, 0.92592593, 0.88148148, 0.89259259, 0.89259259,\n","       0.94074074, 0.92962963, 0.89259259, 0.91481481, 0.92592593,\n","       0.91851852, 0.8962963 , 0.91111111, 0.94814815, 0.92222222,\n","       0.92962963, 0.91481481, 0.90740741, 0.89259259, 0.93703704,\n","       0.92962963, 0.93703704, 0.92222222, 0.92962963, 0.93703704,\n","       0.93333333, 0.91481481, 0.92962963, 0.93703704, 0.92962963,\n","       0.92962963, 0.92592593, 0.91851852, 0.91481481, 0.91111111,\n","       0.92592593, 0.94074074, 0.93333333, 0.9037037 , 0.9       ,\n","       0.94074074, 0.94444444, 0.93703704, 0.93703704, 0.95185185,\n","       0.94814815, 0.92222222, 0.9037037 , 0.9       , 0.91481481,\n","       0.92222222, 0.8962963 , 0.93703704, 0.94074074, 0.93703704,\n","       0.9037037 , 0.9037037 , 0.94074074, 0.92222222, 0.8962963 ,\n","       0.91481481, 0.94814815, 0.93333333, 0.91851852, 0.92222222,\n","       0.92222222, 0.8962963 , 0.92592593, 0.93703704, 0.91851852,\n","       0.91481481, 0.92962963, 0.92962963, 0.92592593, 0.91481481,\n","       0.92962963, 0.94074074, 0.94074074, 0.91111111, 0.93333333,\n","       0.94444444, 0.91481481, 0.90740741, 0.92222222, 0.94444444,\n","       0.94074074, 0.92222222, 0.94074074, 0.94444444, 0.94444444,\n","       0.92962963, 0.91851852, 0.94074074, 0.94444444, 0.92592593,\n","       0.88888889, 0.92222222, 0.91851852, 0.87037037, 0.9       ,\n","       0.9       , 0.95555556, 0.91851852, 0.9037037 , 0.91851852,\n","       0.94814815, 0.93333333, 0.9       , 0.91111111, 0.91481481,\n","       0.93333333, 0.94814815, 0.92222222, 0.91481481, 0.88518519,\n","       0.92962963, 0.92962963, 0.93333333, 0.92222222, 0.95555556,\n","       0.94444444, 0.94444444, 0.92222222, 0.93333333, 0.93703704,\n","       0.93703704, 0.91851852, 0.92962963, 0.91481481, 0.91851852,\n","       0.92222222, 0.94444444, 0.95185185, 0.94444444, 0.93703704,\n","       0.94444444, 0.95185185, 0.93333333, 0.92962963, 0.93703704,\n","       0.95185185, 0.94814815, 0.92592593, 0.93333333, 0.92592593,\n","       0.93703704, 0.87777778, 0.92962963, 0.94814815, 0.94814815,\n","       0.91111111, 0.91111111, 0.93703704, 0.94074074, 0.93333333,\n","       0.91481481, 0.92222222, 0.94814815, 0.94074074, 0.92962963,\n","       0.92592593, 0.92222222, 0.92222222, 0.93333333, 0.93703704,\n","       0.93703704, 0.93333333, 0.93333333, 0.92962963, 0.94074074,\n","       0.91851852, 0.94444444, 0.94074074, 0.94074074, 0.92222222,\n","       0.92222222, 0.92222222, 0.91481481, 0.91851852, 0.9       ,\n","       0.92592593, 0.94444444, 0.92962963, 0.9       , 0.93703704,\n","       0.94444444, 0.93703704, 0.90740741, 0.95185185, 0.94074074,\n","       0.92962963, 0.9037037 , 0.92962963, 0.92592593, 0.9037037 ,\n","       0.8962963 , 0.91111111, 0.94074074, 0.93333333, 0.89259259,\n","       0.91851852, 0.94814815, 0.94074074, 0.88888889, 0.92592593,\n","       0.94074074, 0.93333333, 0.91111111, 0.91481481, 0.92962963,\n","       0.91111111, 0.91851852, 0.92592593, 0.92592593, 0.91481481,\n","       0.92592593, 0.93333333, 0.94444444, 0.92222222, 0.94074074,\n","       0.94074074, 0.92962963, 0.92222222, 0.91481481, 0.92962963,\n","       0.92222222, 0.92592593, 0.9       , 0.92592593, 0.94074074,\n","       0.92222222, 0.9037037 , 0.94074074, 0.93703704, 0.92592593,\n","       0.9       , 0.94444444, 0.94814815, 0.93333333, 0.91111111,\n","       0.88518519, 0.91481481, 0.89259259, 0.9037037 , 0.91111111,\n","       0.94444444, 0.91851852, 0.90740741, 0.9037037 , 0.91481481,\n","       0.91851852, 0.9       , 0.93703704, 0.92592593, 0.92592593,\n","       0.92222222, 0.92962963, 0.91851852, 0.91481481, 0.91481481,\n","       0.93333333, 0.93703704, 0.93703704, 0.93703704, 0.92222222,\n","       0.92592593, 0.93703704, 0.93703704, 0.94074074, 0.92962963,\n","       0.92592593, 0.92222222, 0.92592593, 0.92222222, 0.91481481,\n","       0.9037037 , 0.94074074, 0.94074074, 0.92222222, 0.92222222,\n","       0.94814815, 0.95185185, 0.92592593, 0.90740741, 0.95185185,\n","       0.95185185, 0.93333333, 0.91481481, 0.88888889, 0.93703704,\n","       0.8962963 , 0.90740741, 0.91481481, 0.92962963, 0.93333333,\n","       0.8962963 , 0.94444444, 0.92222222, 0.92962963, 0.9       ,\n","       0.91111111, 0.95555556, 0.92592593]), 'std_test_score': array([0.        , 0.        , 0.        , 0.        , 0.02962963,\n","       0.02222222, 0.03703704, 0.04444444, 0.01111111, 0.01481481,\n","       0.01481481, 0.03333333, 0.        , 0.00740741, 0.01481481,\n","       0.03333333, 0.        , 0.        , 0.        , 0.        ,\n","       0.0037037 , 0.00740741, 0.01111111, 0.02962963, 0.00740741,\n","       0.        , 0.        , 0.02592593, 0.        , 0.00740741,\n","       0.0037037 , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.01111111, 0.0037037 , 0.0037037 , 0.04444444,\n","       0.0037037 , 0.0037037 , 0.01481481, 0.02222222, 0.01481481,\n","       0.        , 0.        , 0.01481481, 0.        , 0.        ,\n","       0.        , 0.        , 0.02962963, 0.02592593, 0.02592593,\n","       0.04074074, 0.00740741, 0.01481481, 0.02592593, 0.04814815,\n","       0.00740741, 0.0037037 , 0.00740741, 0.01851852, 0.        ,\n","       0.        , 0.        , 0.        , 0.00740741, 0.02222222,\n","       0.01851852, 0.05555556, 0.00740741, 0.        , 0.0037037 ,\n","       0.0037037 , 0.04074074, 0.        , 0.00740741, 0.02222222,\n","       0.        , 0.        , 0.        , 0.        , 0.03333333,\n","       0.01111111, 0.00740741, 0.03333333, 0.02962963, 0.00740741,\n","       0.01481481, 0.02222222, 0.05185185, 0.04074074, 0.00740741,\n","       0.02592593, 0.        , 0.        , 0.        , 0.        ,\n","       0.02592593, 0.03333333, 0.02962963, 0.01111111, 0.00740741,\n","       0.01851852, 0.02962963, 0.04074074, 0.00740741, 0.00740741,\n","       0.        , 0.01851852, 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.01111111, 0.01481481, 0.03333333,\n","       0.0037037 , 0.0037037 , 0.00740741, 0.03333333, 0.        ,\n","       0.0037037 , 0.01481481, 0.02222222, 0.        , 0.        ,\n","       0.        , 0.        , 0.03703704, 0.01111111, 0.00740741,\n","       0.04074074, 0.04074074, 0.02222222, 0.00740741, 0.01851852,\n","       0.03703704, 0.03333333, 0.0037037 , 0.01481481, 0.        ,\n","       0.        , 0.        , 0.        , 0.02592593, 0.02592593,\n","       0.02962963, 0.07037037, 0.01111111, 0.01481481, 0.01481481,\n","       0.02222222, 0.00740741, 0.01111111, 0.00740741, 0.02222222,\n","       0.        , 0.        , 0.        , 0.        , 0.00740741,\n","       0.00740741, 0.01481481, 0.03333333, 0.0037037 , 0.        ,\n","       0.00740741, 0.01481481, 0.0037037 , 0.00740741, 0.        ,\n","       0.01851852, 0.        , 0.        , 0.        , 0.        ,\n","       0.02592593, 0.01111111, 0.0037037 , 0.05555556, 0.03333333,\n","       0.0037037 , 0.        , 0.03333333, 0.05185185, 0.02962963,\n","       0.00740741, 0.01111111, 0.02592593, 0.05185185, 0.04074074,\n","       0.02962963, 0.0037037 , 0.        , 0.01481481, 0.02592593,\n","       0.        , 0.0037037 , 0.01481481, 0.02222222, 0.00740741,\n","       0.00740741, 0.00740741, 0.02222222, 0.01851852, 0.0037037 ,\n","       0.01481481, 0.04074074, 0.01111111, 0.00740741, 0.0037037 ,\n","       0.01481481, 0.00740741, 0.00740741, 0.        , 0.01481481,\n","       0.01481481, 0.01851852, 0.02222222, 0.01481481, 0.00740741,\n","       0.        , 0.04444444, 0.06296296, 0.00740741, 0.0037037 ,\n","       0.        , 0.        , 0.00740741, 0.01111111, 0.01481481,\n","       0.0037037 , 0.03703704, 0.04814815, 0.        , 0.0037037 ,\n","       0.04074074, 0.05185185, 0.02222222, 0.05185185, 0.01111111,\n","       0.00740741, 0.00740741, 0.03333333, 0.0037037 , 0.0037037 ,\n","       0.00740741, 0.01481481, 0.0037037 , 0.0037037 , 0.        ,\n","       0.01851852, 0.00740741, 0.01481481, 0.02962963, 0.04444444,\n","       0.01481481, 0.0037037 , 0.02222222, 0.03333333, 0.02592593,\n","       0.02222222, 0.0037037 , 0.01111111, 0.03703704, 0.01111111,\n","       0.00740741, 0.01111111, 0.02222222, 0.0037037 , 0.02222222,\n","       0.05925926, 0.02962963, 0.03333333, 0.01111111, 0.02962963,\n","       0.04074074, 0.01111111, 0.00740741, 0.01851852, 0.04074074,\n","       0.04444444, 0.01851852, 0.        , 0.03703704, 0.02592593,\n","       0.05555556, 0.01481481, 0.0037037 , 0.0037037 , 0.01481481,\n","       0.01481481, 0.00740741, 0.        , 0.00740741, 0.02592593,\n","       0.        , 0.0037037 , 0.0037037 , 0.0037037 , 0.01481481,\n","       0.02592593, 0.03333333, 0.00740741, 0.0037037 , 0.        ,\n","       0.0037037 , 0.01851852, 0.01111111, 0.01481481, 0.00740741,\n","       0.02962963, 0.00740741, 0.01111111, 0.00740741, 0.01481481,\n","       0.01851852, 0.00740741, 0.0037037 , 0.07407407, 0.04814815,\n","       0.01851852, 0.01111111, 0.02222222, 0.03333333, 0.04074074,\n","       0.0037037 , 0.        , 0.04074074, 0.04444444, 0.02222222,\n","       0.00740741, 0.02592593, 0.03703704, 0.02592593, 0.07407407,\n","       0.00740741, 0.01851852, 0.0037037 , 0.03333333, 0.0037037 ,\n","       0.        , 0.0037037 , 0.02222222, 0.0037037 , 0.        ,\n","       0.0037037 , 0.        , 0.01481481, 0.02962963, 0.02592593,\n","       0.02222222, 0.01481481, 0.0037037 , 0.00740741, 0.02592593,\n","       0.00740741, 0.        , 0.00740741, 0.01111111, 0.00740741,\n","       0.01481481, 0.        , 0.0037037 , 0.02962963, 0.01111111,\n","       0.01111111, 0.0037037 , 0.04074074, 0.02962963, 0.        ,\n","       0.01481481, 0.03703704, 0.00740741, 0.01481481, 0.0037037 ,\n","       0.04444444, 0.04074074, 0.01851852, 0.0037037 , 0.01481481,\n","       0.02962963, 0.02222222, 0.04074074, 0.01481481, 0.0037037 ,\n","       0.        , 0.02962963, 0.0037037 , 0.0037037 , 0.00740741,\n","       0.01111111, 0.        , 0.        , 0.01111111, 0.00740741,\n","       0.0037037 , 0.00740741, 0.01851852, 0.05185185, 0.0037037 ,\n","       0.01111111, 0.01481481, 0.01111111, 0.01851852, 0.0037037 ,\n","       0.01111111, 0.01481481, 0.02222222, 0.01851852, 0.01481481,\n","       0.01111111, 0.0037037 , 0.0037037 , 0.0037037 , 0.04074074,\n","       0.01481481, 0.        , 0.00740741, 0.01481481, 0.00740741,\n","       0.02222222, 0.01111111, 0.        , 0.04814815, 0.01851852,\n","       0.02962963, 0.        , 0.01111111, 0.01111111, 0.01481481,\n","       0.01111111, 0.0037037 , 0.00740741, 0.00740741, 0.02222222,\n","       0.        , 0.0037037 , 0.0037037 , 0.01851852, 0.0037037 ,\n","       0.        , 0.0037037 , 0.00740741, 0.00740741, 0.00740741,\n","       0.01111111, 0.03703704, 0.04074074, 0.01851852, 0.01851852,\n","       0.01481481, 0.03703704, 0.02222222, 0.00740741, 0.01111111,\n","       0.03703704, 0.01481481, 0.01481481, 0.01111111, 0.04074074,\n","       0.00740741, 0.02222222, 0.03703704, 0.04074074, 0.02592593,\n","       0.01481481, 0.0037037 , 0.04074074, 0.04814815, 0.02962963,\n","       0.01481481, 0.03703704, 0.04444444, 0.02222222, 0.01111111,\n","       0.01851852, 0.01851852, 0.02592593, 0.04074074, 0.0037037 ,\n","       0.0037037 , 0.0037037 , 0.01111111, 0.0037037 , 0.0037037 ,\n","       0.        , 0.01851852, 0.0037037 , 0.0037037 , 0.0037037 ,\n","       0.0037037 , 0.        , 0.01481481, 0.0037037 , 0.02222222,\n","       0.        , 0.00740741, 0.        , 0.02222222, 0.0037037 ,\n","       0.01481481, 0.01111111, 0.0037037 , 0.01111111, 0.01111111,\n","       0.01481481, 0.01111111, 0.03703704, 0.02592593, 0.0037037 ,\n","       0.01851852, 0.04444444, 0.02592593, 0.00740741, 0.0037037 ,\n","       0.03703704, 0.03703704, 0.00740741, 0.01111111, 0.03703704,\n","       0.04814815, 0.01481481, 0.        , 0.01481481, 0.02592593,\n","       0.01111111, 0.04444444, 0.00740741, 0.0037037 , 0.01481481,\n","       0.01851852, 0.0037037 , 0.0037037 , 0.00740741, 0.01851852,\n","       0.0037037 , 0.00740741, 0.00740741, 0.02222222, 0.        ,\n","       0.01111111, 0.01851852, 0.02592593, 0.01851852, 0.01111111,\n","       0.00740741, 0.01111111, 0.01481481, 0.01111111, 0.01111111,\n","       0.0037037 , 0.00740741, 0.02222222, 0.01111111, 0.00740741,\n","       0.01481481, 0.01111111, 0.01481481, 0.04074074, 0.04074074,\n","       0.02592593, 0.01481481, 0.01481481, 0.05185185, 0.03703704,\n","       0.01481481, 0.        , 0.04074074, 0.04444444, 0.04814815,\n","       0.        , 0.        , 0.01111111, 0.01851852, 0.01111111,\n","       0.0037037 , 0.0037037 , 0.        , 0.01111111, 0.01481481,\n","       0.0037037 , 0.01111111, 0.0037037 , 0.        , 0.0037037 ,\n","       0.0037037 , 0.01481481, 0.0037037 , 0.01851852, 0.01481481,\n","       0.01111111, 0.01851852, 0.01851852, 0.01851852, 0.0037037 ,\n","       0.02592593, 0.01851852, 0.        , 0.0037037 , 0.02592593,\n","       0.01851852, 0.01481481, 0.00740741, 0.00740741, 0.        ,\n","       0.01851852, 0.03333333, 0.01111111, 0.01481481, 0.02222222,\n","       0.02222222, 0.05185185, 0.01111111, 0.00740741, 0.        ,\n","       0.04814815, 0.04074074, 0.01481481, 0.00740741, 0.0037037 ,\n","       0.00740741, 0.02592593, 0.02592593, 0.00740741, 0.0037037 ,\n","       0.0037037 , 0.        , 0.00740741, 0.0037037 , 0.00740741,\n","       0.01481481, 0.0037037 , 0.        , 0.00740741, 0.01111111,\n","       0.0037037 , 0.0037037 , 0.0037037 , 0.01481481, 0.03333333,\n","       0.00740741, 0.01851852, 0.0037037 , 0.03333333, 0.01111111,\n","       0.0037037 , 0.0037037 , 0.04814815, 0.01851852, 0.00740741,\n","       0.0037037 , 0.03703704, 0.01111111, 0.00740741, 0.02222222,\n","       0.04444444, 0.04444444, 0.00740741, 0.        , 0.04074074,\n","       0.05185185, 0.02222222, 0.00740741, 0.03703704, 0.02962963,\n","       0.02962963, 0.        , 0.02222222, 0.01851852, 0.01851852,\n","       0.03703704, 0.01481481, 0.00740741, 0.00740741, 0.01851852,\n","       0.01481481, 0.        , 0.01111111, 0.0037037 , 0.        ,\n","       0.00740741, 0.0037037 , 0.01111111, 0.0037037 , 0.0037037 ,\n","       0.01111111, 0.00740741, 0.04074074, 0.02962963, 0.00740741,\n","       0.01111111, 0.02962963, 0.00740741, 0.0037037 , 0.00740741,\n","       0.03333333, 0.01851852, 0.02222222, 0.        , 0.04444444,\n","       0.03333333, 0.0037037 , 0.03333333, 0.03703704, 0.04444444,\n","       0.01111111, 0.01481481, 0.04814815, 0.03703704, 0.03333333,\n","       0.01481481, 0.04074074, 0.02592593, 0.04444444, 0.00740741,\n","       0.01111111, 0.0037037 , 0.01481481, 0.03333333, 0.01851852,\n","       0.        , 0.0037037 , 0.0037037 , 0.0037037 , 0.01111111,\n","       0.00740741, 0.01111111, 0.0037037 , 0.00740741, 0.0037037 ,\n","       0.        , 0.01111111, 0.00740741, 0.01111111, 0.01111111,\n","       0.03703704, 0.01481481, 0.00740741, 0.01111111, 0.01851852,\n","       0.01481481, 0.01851852, 0.00740741, 0.04074074, 0.01851852,\n","       0.01111111, 0.        , 0.04074074, 0.01481481, 0.0037037 ,\n","       0.02222222, 0.04074074, 0.04814815, 0.0037037 , 0.        ,\n","       0.03703704, 0.02592593, 0.04074074, 0.0037037 , 0.04074074,\n","       0.04444444, 0.01481481, 0.00740741]), 'rank_test_score': array([721, 721, 721, 721, 417, 545, 530, 698, 361, 417, 417, 628, 164,\n","       295, 417, 474, 721, 721, 721, 721, 228, 295, 568, 595, 295, 164,\n","       164, 671, 164,  69, 121, 417, 721, 721, 721, 721, 361, 228, 485,\n","       712, 121, 228, 417, 545, 295, 295, 295, 417, 721, 721, 721, 721,\n","       417, 568, 568, 474, 295, 417, 361, 628, 295, 228, 295, 485, 721,\n","       721, 721, 721, 417, 545, 628, 701, 295, 164, 228, 692, 568, 164,\n","        69, 545, 721, 721, 721, 721, 568, 485, 545, 671, 595, 417, 417,\n","       656, 595, 568,  69, 568, 721, 721, 721, 721, 361, 474, 595, 692,\n","       295, 485, 417, 568, 295, 295, 164, 485, 721, 721, 721, 721, 164,\n","       361, 595, 628, 228, 228, 295, 474, 164, 228,  13, 545, 721, 721,\n","       721, 721, 530, 485, 545, 692, 568, 688, 295, 628, 656, 628, 361,\n","       595, 721, 721, 721, 721, 568, 568, 417, 671, 361, 417, 417, 595,\n","       295, 361, 295, 545, 721, 721, 721, 721, 295, 295, 595, 671, 361,\n","       164, 295, 595, 228, 295, 164, 485, 721, 721, 721, 721, 628, 361,\n","       485, 628, 628, 228, 417, 628, 595, 530, 417, 568, 485, 595, 671,\n","       718, 121, 164, 417, 361, 164, 228, 417, 545,  69, 164, 295, 545,\n","       485, 485, 656, 701,  35, 164, 228, 417, 164,  69, 164, 417,  13,\n","         4,  13, 417,  69, 417, 706, 708, 164, 228, 164, 417, 417,  35,\n","        69, 121, 595, 485, 164, 121, 568, 417, 656, 718, 361, 295, 295,\n","       474, 228, 228, 295, 417, 228, 121, 164, 485, 545, 595, 595, 716,\n","       164, 121, 545, 474, 628, 115, 121, 361, 595,  35,  13, 361, 656,\n","       485, 295, 706, 656, 628, 121, 595, 671, 228, 164, 485, 671, 530,\n","        35, 164, 530, 485, 701, 716, 228, 228, 417, 417, 295, 164,  69,\n","       361, 295, 228, 228, 228, 295, 568, 628, 712, 361, 164, 228, 485,\n","        35,  69,  69, 595, 164,  35,  69, 417, 671, 545, 485, 705, 628,\n","       628,  35, 545, 628, 568, 121, 164, 628, 530,  13,  69, 568, 530,\n","       568, 720, 295, 485, 228, 474, 228, 164, 228, 545, 121, 164, 228,\n","       164, 545, 595, 671, 714, 417, 228,  69, 671,  69, 164,  69, 361,\n","       164,  69, 164, 228, 595, 485, 568, 715, 671, 656, 164, 417, 595,\n","       417,  13, 228, 656, 568, 121, 121, 417, 417, 295, 692, 417, 121,\n","       164, 417,  35, 121, 295, 361,  69,  69, 228, 295, 228, 295, 485,\n","       698, 228,  35,  69, 361,  35,  35,  35, 417, 115,  35,  13, 361,\n","       485, 361, 485, 710, 295, 164, 164, 164, 295, 115,  35, 164, 485,\n","        35, 115, 164, 361, 361, 545, 708, 228,  69,  69, 545, 164, 228,\n","       228, 228, 121, 164, 228, 295, 295, 295, 568, 530, 568, 228,  35,\n","       417, 595,  13,  13, 361, 595,  69,  13, 361, 474, 295, 295, 698,\n","       671, 671,  69, 228, 671, 485, 295, 417, 656, 530,  13, 361, 228,\n","       485, 568, 671, 121, 228, 121, 361, 228, 121, 164, 485, 228, 121,\n","       228, 228, 295, 417, 485, 545, 295,  69, 164, 595, 655,  69,  35,\n","       121, 121,   4,  13, 361, 595, 628, 485, 361, 656, 161,  69, 121,\n","       595, 595,  69, 361, 656, 485,  13, 164, 417, 361, 361, 656, 295,\n","       121, 417, 485, 228, 228, 295, 485, 228,  69,  69, 545, 164,  35,\n","       485, 568, 361,  35,  69, 361,  69,  35,  35, 228, 417, 115,  35,\n","       295, 688, 361, 417, 710, 628, 628,   1, 417, 595, 417,  13, 164,\n","       628, 530, 485, 164,  13, 361, 485, 692, 228, 228, 164, 361,   1,\n","        35,  35, 361, 164, 121, 121, 417, 228, 485, 417, 361,  35,   4,\n","        35, 121,  35,   4, 164, 228, 161,   4,  13, 295, 164, 295, 121,\n","       701, 228,  13,  13, 545, 545, 121,  69, 164, 485, 361,  13,  69,\n","       228, 295, 361, 361, 164, 121, 121, 164, 164, 228,  69, 417,  35,\n","        69,  69, 361, 361, 361, 485, 417, 628, 295,  35, 228, 628, 121,\n","        35, 121, 568,   4,  69, 228, 595, 228, 295, 595, 656, 530,  69,\n","       164, 671, 417,  13,  69, 688, 295, 115, 164, 545, 485, 228, 530,\n","       417, 295, 295, 485, 295, 164,  35, 361,  69,  69, 228, 361, 485,\n","       228, 361, 295, 628, 295,  69, 361, 595,  69, 121, 295, 628,  35,\n","        13, 164, 530, 692, 485, 671, 595, 530,  35, 417, 568, 595, 474,\n","       417, 628, 161, 295, 295, 361, 228, 417, 474, 485, 164, 121, 121,\n","       121, 361, 295, 121, 121,  69, 228, 295, 361, 295, 361, 485, 595,\n","        69,  69, 361, 361,  13,   4, 295, 568,   4,   4, 164, 474, 688,\n","       121, 656, 568, 485, 228, 164, 656,  35, 361, 228, 628, 530,   1,\n","       295])}\n"]}],"source":["# print(grid.best_estimator_)\n","# print(grid.best_score_)\n","# print(grid.best_params_)\n","# print(grid.cv_results_)"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["before smote\n"]},{"ename":"KeyError","evalue":"0","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2263\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2273\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 0","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[0;32mIn[18], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m opt_gb_clf \u001b[39m=\u001b[39m GradientBoostingClassifier(learning_rate\u001b[39m=\u001b[39m\u001b[39m0.02\u001b[39m, \n\u001b[1;32m      2\u001b[0m                                         max_depth\u001b[39m=\u001b[39m \u001b[39m6\u001b[39m, \n\u001b[1;32m      3\u001b[0m                                         subsample\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m, \n\u001b[1;32m      4\u001b[0m                                         n_estimators \u001b[39m=\u001b[39m \u001b[39m70\u001b[39m, \n\u001b[1;32m      5\u001b[0m                                         max_features \u001b[39m=\u001b[39m \u001b[39m7\u001b[39m)\n\u001b[1;32m      8\u001b[0m opt_gb_clf\u001b[39m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m----> 9\u001b[0m splitSmote(opt_gb_clf)\n","Cell \u001b[0;32mIn[8], line 33\u001b[0m, in \u001b[0;36msplitSmote\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39m#Data presentation \u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mbefore smote\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m sns\u001b[39m.\u001b[39;49mcountplot(y_train_fold[\u001b[39m\"\u001b[39;49m\u001b[39mRate\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m     34\u001b[0m plt\u001b[39m.\u001b[39mxlabel(\u001b[39m\"\u001b[39m\u001b[39mRate of safety\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m plt\u001b[39m.\u001b[39mylabel(\u001b[39m\"\u001b[39m\u001b[39mCount of rates\u001b[39m\u001b[39m\"\u001b[39m)\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/seaborn/categorical.py:2943\u001b[0m, in \u001b[0;36mcountplot\u001b[0;34m(data, x, y, hue, order, hue_order, orient, color, palette, saturation, width, dodge, ax, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m \u001b[39melif\u001b[39;00m x \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2941\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot pass values for both `x` and `y`\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 2943\u001b[0m plotter \u001b[39m=\u001b[39m _CountPlotter(\n\u001b[1;32m   2944\u001b[0m     x, y, hue, data, order, hue_order,\n\u001b[1;32m   2945\u001b[0m     estimator, errorbar, n_boot, units, seed,\n\u001b[1;32m   2946\u001b[0m     orient, color, palette, saturation,\n\u001b[1;32m   2947\u001b[0m     width, errcolor, errwidth, capsize, dodge\n\u001b[1;32m   2948\u001b[0m )\n\u001b[1;32m   2950\u001b[0m plotter\u001b[39m.\u001b[39mvalue_label \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcount\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2952\u001b[0m \u001b[39mif\u001b[39;00m ax \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/seaborn/categorical.py:1530\u001b[0m, in \u001b[0;36m_BarPlotter.__init__\u001b[0;34m(self, x, y, hue, data, order, hue_order, estimator, errorbar, n_boot, units, seed, orient, color, palette, saturation, width, errcolor, errwidth, capsize, dodge)\u001b[0m\n\u001b[1;32m   1525\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, x, y, hue, data, order, hue_order,\n\u001b[1;32m   1526\u001b[0m              estimator, errorbar, n_boot, units, seed,\n\u001b[1;32m   1527\u001b[0m              orient, color, palette, saturation, width,\n\u001b[1;32m   1528\u001b[0m              errcolor, errwidth, capsize, dodge):\n\u001b[1;32m   1529\u001b[0m     \u001b[39m\"\"\"Initialize the plotter.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1530\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mestablish_variables(x, y, hue, data, orient,\n\u001b[1;32m   1531\u001b[0m                              order, hue_order, units)\n\u001b[1;32m   1532\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestablish_colors(color, palette, saturation)\n\u001b[1;32m   1533\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimate_statistic(estimator, errorbar, n_boot, seed)\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/seaborn/categorical.py:486\u001b[0m, in \u001b[0;36m_CategoricalPlotter.establish_variables\u001b[0;34m(self, x, y, hue, data, orient, order, hue_order, units)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(data, \u001b[39m\"\u001b[39m\u001b[39mshape\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    485\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 486\u001b[0m         \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39misscalar(data[\u001b[39m0\u001b[39;49m]):\n\u001b[1;32m    487\u001b[0m             plot_data \u001b[39m=\u001b[39m [data]\n\u001b[1;32m    488\u001b[0m         \u001b[39melse\u001b[39;00m:\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/series.py:981\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    978\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[key]\n\u001b[1;32m    980\u001b[0m \u001b[39melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m--> 981\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_value(key)\n\u001b[1;32m    983\u001b[0m \u001b[39mif\u001b[39;00m is_hashable(key):\n\u001b[1;32m    984\u001b[0m     \u001b[39m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[1;32m    985\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    986\u001b[0m         \u001b[39m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/series.py:1089\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1086\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[label]\n\u001b[1;32m   1088\u001b[0m \u001b[39m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1089\u001b[0m loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mget_loc(label)\n\u001b[1;32m   1090\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39m_get_values_for_loc(\u001b[39mself\u001b[39m, loc, label)\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n","\u001b[0;31mKeyError\u001b[0m: 0"]}],"source":["opt_gb_clf = GradientBoostingClassifier(learning_rate=0.02, \n","                                        max_depth= 6, \n","                                        subsample=0.5, \n","                                        n_estimators = 70, \n","                                        max_features = 7)\n","\n","\n","opt_gb_clf.fit(X_train, y_train)\n","splitSmote(opt_gb_clf)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["the best until now\n","\n","learning_rate=0.03, \n","                                        max_depth= 6, \n","                                        subsample=0.5, \n","                                        n_estimators = 50, \n","                                        max_features = 7)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 4.4 LogisticRegression"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Before optimization"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["LogisticRegression\n","before smote\n"]},{"ename":"KeyError","evalue":"0","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2263\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2273\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 0","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[0;32mIn[23], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m logreg\u001b[39m=\u001b[39mLogisticRegression()\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLogisticRegression\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m splitSmote (logreg)\n","Cell \u001b[0;32mIn[22], line 33\u001b[0m, in \u001b[0;36msplitSmote\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39m#Data presentation \u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mbefore smote\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m sns\u001b[39m.\u001b[39;49mcountplot(y_train_fold[\u001b[39m\"\u001b[39;49m\u001b[39mRate\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m     34\u001b[0m plt\u001b[39m.\u001b[39mxlabel(\u001b[39m\"\u001b[39m\u001b[39mRate of safety\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m plt\u001b[39m.\u001b[39mylabel(\u001b[39m\"\u001b[39m\u001b[39mCount of rates\u001b[39m\u001b[39m\"\u001b[39m)\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/seaborn/categorical.py:2943\u001b[0m, in \u001b[0;36mcountplot\u001b[0;34m(data, x, y, hue, order, hue_order, orient, color, palette, saturation, width, dodge, ax, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m \u001b[39melif\u001b[39;00m x \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2941\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot pass values for both `x` and `y`\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 2943\u001b[0m plotter \u001b[39m=\u001b[39m _CountPlotter(\n\u001b[1;32m   2944\u001b[0m     x, y, hue, data, order, hue_order,\n\u001b[1;32m   2945\u001b[0m     estimator, errorbar, n_boot, units, seed,\n\u001b[1;32m   2946\u001b[0m     orient, color, palette, saturation,\n\u001b[1;32m   2947\u001b[0m     width, errcolor, errwidth, capsize, dodge\n\u001b[1;32m   2948\u001b[0m )\n\u001b[1;32m   2950\u001b[0m plotter\u001b[39m.\u001b[39mvalue_label \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcount\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2952\u001b[0m \u001b[39mif\u001b[39;00m ax \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/seaborn/categorical.py:1530\u001b[0m, in \u001b[0;36m_BarPlotter.__init__\u001b[0;34m(self, x, y, hue, data, order, hue_order, estimator, errorbar, n_boot, units, seed, orient, color, palette, saturation, width, errcolor, errwidth, capsize, dodge)\u001b[0m\n\u001b[1;32m   1525\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, x, y, hue, data, order, hue_order,\n\u001b[1;32m   1526\u001b[0m              estimator, errorbar, n_boot, units, seed,\n\u001b[1;32m   1527\u001b[0m              orient, color, palette, saturation, width,\n\u001b[1;32m   1528\u001b[0m              errcolor, errwidth, capsize, dodge):\n\u001b[1;32m   1529\u001b[0m     \u001b[39m\"\"\"Initialize the plotter.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1530\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mestablish_variables(x, y, hue, data, orient,\n\u001b[1;32m   1531\u001b[0m                              order, hue_order, units)\n\u001b[1;32m   1532\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestablish_colors(color, palette, saturation)\n\u001b[1;32m   1533\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimate_statistic(estimator, errorbar, n_boot, seed)\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/seaborn/categorical.py:486\u001b[0m, in \u001b[0;36m_CategoricalPlotter.establish_variables\u001b[0;34m(self, x, y, hue, data, orient, order, hue_order, units)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(data, \u001b[39m\"\u001b[39m\u001b[39mshape\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    485\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 486\u001b[0m         \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39misscalar(data[\u001b[39m0\u001b[39;49m]):\n\u001b[1;32m    487\u001b[0m             plot_data \u001b[39m=\u001b[39m [data]\n\u001b[1;32m    488\u001b[0m         \u001b[39melse\u001b[39;00m:\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/series.py:981\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    978\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[key]\n\u001b[1;32m    980\u001b[0m \u001b[39melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m--> 981\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_value(key)\n\u001b[1;32m    983\u001b[0m \u001b[39mif\u001b[39;00m is_hashable(key):\n\u001b[1;32m    984\u001b[0m     \u001b[39m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[1;32m    985\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    986\u001b[0m         \u001b[39m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/series.py:1089\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1086\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[label]\n\u001b[1;32m   1088\u001b[0m \u001b[39m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1089\u001b[0m loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mget_loc(label)\n\u001b[1;32m   1090\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39m_get_values_for_loc(\u001b[39mself\u001b[39m, loc, label)\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n","\u001b[0;31mKeyError\u001b[0m: 0"]}],"source":["#TODO: write LogisticRegression optimization code here\n","\n","logreg=LogisticRegression()\n","splitSmote (logreg)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["After optimization"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["#LogisticRegression\n","\n","#Grid search cross validation\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.linear_model import LogisticRegression\n","grid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\",\"elasticnet\"], 'solver'  : ['newton-cg', 'lbfgs', 'liblinear']}\n","logreg=LogisticRegression()\n","lr=GridSearchCV(logreg,grid,cv=10)\n","# lr.fit(X_train,y_train)\n","\n","# a=lr.best_params_\n","# b=lr.best_score_\n","# print(\"tuned hpyerparameters :(best parameters) \",lr.best_params_)\n","# print(\"accuracy :\",lr.best_score_)\n","# print(\"Best estimator: \",lr.best_estimator_ )\n","\n","#y_pred.append(pd.Series(lr.predict(X_test), name='LogisticRegression'))\n","\n","# precision, recall, fscore, _ = score(y_test, lr.predict(X_test), average='weighted')\n","# accuracy = accuracy_score(y_test, lr.predict(X_test))\n","# metrics.append(pd.Series({'precision':precision, 'recall':recall,'fscore':fscore, 'accuracy':accuracy}))\n","\n","#LogisticRegression\n","#from sklearn.linear_model import LogisticRegressionCV\n","#lr= LogisticRegressionCV(Cs=a['C'], penalty=a['penalty'], solver=a['solver']).fit(X_train, y_train)\n","\n","splitSmote (lr)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 4.5 Random Forest"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Before optimization"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy scores for each training fold:  [1.0, 1.0, 1.0, 1.0, 1.0]\n","Accuracy scores for each testing fold:  [0.9523809523809523, 1.0, 0.7903225806451613, 0.967741935483871, 0.7741935483870968]\n","Traning Mean accuracy score:  1.0\n","Testing Mean accuracy score:  0.8969278033794164\n"]}],"source":["#RandomForestClassifier\n","RF= RandomForestClassifier(random_state= 0)\n","# RF.fit(X_train,y_train)\n","\n","# #y_pred.append(pd.Series(RF.predict(X_test), name='RandomForestClassifier'))\n","\n","# precision, recall, fscore, _ = score(y_test, RF.predict(X_test), average='weighted')\n","# accuracy = accuracy_score(y_test, RF.predict(X_test))\n","# metrics.append(pd.Series({'precision':precision, 'recall':recall,'fscore':fscore, 'accuracy':accuracy}))\n","\n","\n","splitSmote (RF)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["After optimization"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["random_search\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32mc:\\Users\\Spark\\ML\\Cars-Ratings-ML\\Models.ipynb Cell 43\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Spark/ML/Cars-Ratings-ML/Models.ipynb#X60sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mrandom_search\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Spark/ML/Cars-Ratings-ML/Models.ipynb#X60sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m random_search\u001b[39m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Spark/ML/Cars-Ratings-ML/Models.ipynb#X60sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m splitSmote(random_search)\n","\u001b[1;32mc:\\Users\\Spark\\ML\\Cars-Ratings-ML\\Models.ipynb Cell 43\u001b[0m in \u001b[0;36msplitSmote\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Spark/ML/Cars-Ratings-ML/Models.ipynb#X60sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m x_sm, y_sm \u001b[39m=\u001b[39m smote\u001b[39m.\u001b[39mfit_resample(x_train_fold, y_train_fold)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Spark/ML/Cars-Ratings-ML/Models.ipynb#X60sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m#Fit the model to the training data\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Spark/ML/Cars-Ratings-ML/Models.ipynb#X60sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(x_sm, np\u001b[39m.\u001b[39;49mravel(y_sm))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Spark/ML/Cars-Ratings-ML/Models.ipynb#X60sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# Make predictions on the test data\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Spark/ML/Cars-Ratings-ML/Models.ipynb#X60sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m yTrain_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(x_train_fold)\n","File \u001b[1;32mc:\\Users\\Spark\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:891\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    885\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    886\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    887\u001b[0m     )\n\u001b[0;32m    889\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 891\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    893\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    894\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    895\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n","File \u001b[1;32mc:\\Users\\Spark\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1392\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1390\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1391\u001b[0m     \u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1392\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n","File \u001b[1;32mc:\\Users\\Spark\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:838\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    830\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    831\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    832\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    833\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    834\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[0;32m    835\u001b[0m         )\n\u001b[0;32m    836\u001b[0m     )\n\u001b[1;32m--> 838\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    839\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    840\u001b[0m         clone(base_estimator),\n\u001b[0;32m    841\u001b[0m         X,\n\u001b[0;32m    842\u001b[0m         y,\n\u001b[0;32m    843\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m    844\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m    845\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    846\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[0;32m    847\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[0;32m    848\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[0;32m    849\u001b[0m     )\n\u001b[0;32m    850\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[0;32m    851\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[0;32m    852\u001b[0m     )\n\u001b[0;32m    853\u001b[0m )\n\u001b[0;32m    855\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    856\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    857\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    858\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    859\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    860\u001b[0m     )\n","File \u001b[1;32mc:\\Users\\Spark\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1089\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\Spark\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\Spark\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n","File \u001b[1;32mc:\\Users\\Spark\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n","File \u001b[1;32mc:\\Users\\Spark\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n","File \u001b[1;32mc:\\Users\\Spark\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n","File \u001b[1;32mc:\\Users\\Spark\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n","File \u001b[1;32mc:\\Users\\Spark\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py:216\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    215\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig):\n\u001b[1;32m--> 216\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n","File \u001b[1;32mc:\\Users\\Spark\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    678\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    679\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 680\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, y_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    682\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    683\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    684\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n","File \u001b[1;32mc:\\Users\\Spark\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:439\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwarm_start \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    435\u001b[0m     \u001b[39m# We draw from the random state to get the random state we\u001b[39;00m\n\u001b[0;32m    436\u001b[0m     \u001b[39m# would have got if we hadn't used a warm_start.\u001b[39;00m\n\u001b[0;32m    437\u001b[0m     random_state\u001b[39m.\u001b[39mrandint(MAX_INT, size\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_))\n\u001b[1;32m--> 439\u001b[0m trees \u001b[39m=\u001b[39m [\n\u001b[0;32m    440\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_estimator(append\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[0;32m    441\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    442\u001b[0m ]\n\u001b[0;32m    444\u001b[0m \u001b[39m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    445\u001b[0m \u001b[39m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[39m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    447\u001b[0m \u001b[39m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    448\u001b[0m \u001b[39m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    449\u001b[0m \u001b[39m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m trees \u001b[39m=\u001b[39m Parallel(\n\u001b[0;32m    451\u001b[0m     n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs,\n\u001b[0;32m    452\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    467\u001b[0m     \u001b[39mfor\u001b[39;00m i, t \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(trees)\n\u001b[0;32m    468\u001b[0m )\n","File \u001b[1;32mc:\\Users\\Spark\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:440\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwarm_start \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    435\u001b[0m     \u001b[39m# We draw from the random state to get the random state we\u001b[39;00m\n\u001b[0;32m    436\u001b[0m     \u001b[39m# would have got if we hadn't used a warm_start.\u001b[39;00m\n\u001b[0;32m    437\u001b[0m     random_state\u001b[39m.\u001b[39mrandint(MAX_INT, size\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_))\n\u001b[0;32m    439\u001b[0m trees \u001b[39m=\u001b[39m [\n\u001b[1;32m--> 440\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_estimator(append\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, random_state\u001b[39m=\u001b[39;49mrandom_state)\n\u001b[0;32m    441\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    442\u001b[0m ]\n\u001b[0;32m    444\u001b[0m \u001b[39m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    445\u001b[0m \u001b[39m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[39m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    447\u001b[0m \u001b[39m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    448\u001b[0m \u001b[39m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    449\u001b[0m \u001b[39m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m trees \u001b[39m=\u001b[39m Parallel(\n\u001b[0;32m    451\u001b[0m     n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs,\n\u001b[0;32m    452\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    467\u001b[0m     \u001b[39mfor\u001b[39;00m i, t \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(trees)\n\u001b[0;32m    468\u001b[0m )\n","File \u001b[1;32mc:\\Users\\Spark\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:158\u001b[0m, in \u001b[0;36mBaseEnsemble._make_estimator\u001b[1;34m(self, append, random_state)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_make_estimator\u001b[39m(\u001b[39mself\u001b[39m, append\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, random_state\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    153\u001b[0m     \u001b[39m\"\"\"Make and configure a copy of the `base_estimator_` attribute.\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \n\u001b[0;32m    155\u001b[0m \u001b[39m    Warning: This method should be used to properly instantiate new\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[39m    sub-estimators.\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 158\u001b[0m     estimator \u001b[39m=\u001b[39m clone(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_estimator_)\n\u001b[0;32m    159\u001b[0m     estimator\u001b[39m.\u001b[39mset_params(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{p: \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, p) \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimator_params})\n\u001b[0;32m    161\u001b[0m     \u001b[39m# TODO: Remove in v1.2\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     \u001b[39m# criterion \"mse\" and \"mae\" would cause warnings in every call to\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[39m# DecisionTreeRegressor.fit(..)\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\Spark\\anaconda3\\lib\\site-packages\\sklearn\\base.py:88\u001b[0m, in \u001b[0;36mclone\u001b[1;34m(estimator, safe)\u001b[0m\n\u001b[0;32m     86\u001b[0m     new_object_params[name] \u001b[39m=\u001b[39m clone(param, safe\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     87\u001b[0m new_object \u001b[39m=\u001b[39m klass(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mnew_object_params)\n\u001b[1;32m---> 88\u001b[0m params_set \u001b[39m=\u001b[39m new_object\u001b[39m.\u001b[39;49mget_params(deep\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     90\u001b[0m \u001b[39m# quick sanity check of the parameters of the clone\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m new_object_params:\n","File \u001b[1;32mc:\\Users\\Spark\\anaconda3\\lib\\site-packages\\sklearn\\base.py:209\u001b[0m, in \u001b[0;36mBaseEstimator.get_params\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39mGet parameters for this estimator.\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39m    Parameter names mapped to their values.\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    208\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m()\n\u001b[1;32m--> 209\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_param_names():\n\u001b[0;32m    210\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, key)\n\u001b[0;32m    211\u001b[0m     \u001b[39mif\u001b[39;00m deep \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(value, \u001b[39m\"\u001b[39m\u001b[39mget_params\u001b[39m\u001b[39m\"\u001b[39m):\n","File \u001b[1;32mc:\\Users\\Spark\\anaconda3\\lib\\site-packages\\sklearn\\base.py:174\u001b[0m, in \u001b[0;36mBaseEstimator._get_param_names\u001b[1;34m(cls)\u001b[0m\n\u001b[0;32m    170\u001b[0m     \u001b[39mreturn\u001b[39;00m []\n\u001b[0;32m    172\u001b[0m \u001b[39m# introspect the constructor arguments to find the model parameters\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[39m# to represent\u001b[39;00m\n\u001b[1;32m--> 174\u001b[0m init_signature \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39;49msignature(init)\n\u001b[0;32m    175\u001b[0m \u001b[39m# Consider the constructor parameters excluding 'self'\u001b[39;00m\n\u001b[0;32m    176\u001b[0m parameters \u001b[39m=\u001b[39m [\n\u001b[0;32m    177\u001b[0m     p\n\u001b[0;32m    178\u001b[0m     \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m init_signature\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mvalues()\n\u001b[0;32m    179\u001b[0m     \u001b[39mif\u001b[39;00m p\u001b[39m.\u001b[39mname \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m p\u001b[39m.\u001b[39mkind \u001b[39m!=\u001b[39m p\u001b[39m.\u001b[39mVAR_KEYWORD\n\u001b[0;32m    180\u001b[0m ]\n","File \u001b[1;32mc:\\Users\\Spark\\anaconda3\\lib\\inspect.py:3113\u001b[0m, in \u001b[0;36msignature\u001b[1;34m(obj, follow_wrapped)\u001b[0m\n\u001b[0;32m   3111\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msignature\u001b[39m(obj, \u001b[39m*\u001b[39m, follow_wrapped\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m   3112\u001b[0m     \u001b[39m\"\"\"Get a signature object for the passed callable.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 3113\u001b[0m     \u001b[39mreturn\u001b[39;00m Signature\u001b[39m.\u001b[39;49mfrom_callable(obj, follow_wrapped\u001b[39m=\u001b[39;49mfollow_wrapped)\n","File \u001b[1;32mc:\\Users\\Spark\\anaconda3\\lib\\inspect.py:2862\u001b[0m, in \u001b[0;36mSignature.from_callable\u001b[1;34m(cls, obj, follow_wrapped)\u001b[0m\n\u001b[0;32m   2859\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m   2860\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_callable\u001b[39m(\u001b[39mcls\u001b[39m, obj, \u001b[39m*\u001b[39m, follow_wrapped\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m   2861\u001b[0m     \u001b[39m\"\"\"Constructs Signature for the given callable object.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2862\u001b[0m     \u001b[39mreturn\u001b[39;00m _signature_from_callable(obj, sigcls\u001b[39m=\u001b[39;49m\u001b[39mcls\u001b[39;49m,\n\u001b[0;32m   2863\u001b[0m                                     follow_wrapper_chains\u001b[39m=\u001b[39;49mfollow_wrapped)\n","File \u001b[1;32mc:\\Users\\Spark\\anaconda3\\lib\\inspect.py:2325\u001b[0m, in \u001b[0;36m_signature_from_callable\u001b[1;34m(obj, follow_wrapper_chains, skip_bound_arg, sigcls)\u001b[0m\n\u001b[0;32m   2320\u001b[0m             \u001b[39mreturn\u001b[39;00m sig\u001b[39m.\u001b[39mreplace(parameters\u001b[39m=\u001b[39mnew_params)\n\u001b[0;32m   2322\u001b[0m \u001b[39mif\u001b[39;00m isfunction(obj) \u001b[39mor\u001b[39;00m _signature_is_functionlike(obj):\n\u001b[0;32m   2323\u001b[0m     \u001b[39m# If it's a pure Python function, or an object that is duck type\u001b[39;00m\n\u001b[0;32m   2324\u001b[0m     \u001b[39m# of a Python function (Cython functions, for instance), then:\u001b[39;00m\n\u001b[1;32m-> 2325\u001b[0m     \u001b[39mreturn\u001b[39;00m _signature_from_function(sigcls, obj,\n\u001b[0;32m   2326\u001b[0m                                     skip_bound_arg\u001b[39m=\u001b[39;49mskip_bound_arg)\n\u001b[0;32m   2328\u001b[0m \u001b[39mif\u001b[39;00m _signature_is_builtin(obj):\n\u001b[0;32m   2329\u001b[0m     \u001b[39mreturn\u001b[39;00m _signature_from_builtin(sigcls, obj,\n\u001b[0;32m   2330\u001b[0m                                    skip_bound_arg\u001b[39m=\u001b[39mskip_bound_arg)\n","File \u001b[1;32mc:\\Users\\Spark\\anaconda3\\lib\\inspect.py:2225\u001b[0m, in \u001b[0;36m_signature_from_function\u001b[1;34m(cls, func, skip_bound_arg)\u001b[0m\n\u001b[0;32m   2222\u001b[0m         default \u001b[39m=\u001b[39m kwdefaults\u001b[39m.\u001b[39mget(name, _empty)\n\u001b[0;32m   2224\u001b[0m     annotation \u001b[39m=\u001b[39m annotations\u001b[39m.\u001b[39mget(name, _empty)\n\u001b[1;32m-> 2225\u001b[0m     parameters\u001b[39m.\u001b[39mappend(Parameter(name, annotation\u001b[39m=\u001b[39;49mannotation,\n\u001b[0;32m   2226\u001b[0m                                 kind\u001b[39m=\u001b[39;49m_KEYWORD_ONLY,\n\u001b[0;32m   2227\u001b[0m                                 default\u001b[39m=\u001b[39;49mdefault))\n\u001b[0;32m   2228\u001b[0m \u001b[39m# **kwargs\u001b[39;00m\n\u001b[0;32m   2229\u001b[0m \u001b[39mif\u001b[39;00m func_code\u001b[39m.\u001b[39mco_flags \u001b[39m&\u001b[39m CO_VARKEYWORDS:\n","File \u001b[1;32mc:\\Users\\Spark\\anaconda3\\lib\\inspect.py:2500\u001b[0m, in \u001b[0;36mParameter.__init__\u001b[1;34m(self, name, kind, default, annotation)\u001b[0m\n\u001b[0;32m   2498\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, kind, \u001b[39m*\u001b[39m, default\u001b[39m=\u001b[39m_empty, annotation\u001b[39m=\u001b[39m_empty):\n\u001b[0;32m   2499\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 2500\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_kind \u001b[39m=\u001b[39m _ParameterKind(kind)\n\u001b[0;32m   2501\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[0;32m   2502\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mvalue \u001b[39m\u001b[39m{\u001b[39;00mkind\u001b[39m!r}\u001b[39;00m\u001b[39m is not a valid Parameter.kind\u001b[39m\u001b[39m'\u001b[39m)\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# #TODO: Write RandomForest optimization code here\n","\n","# #RandomForest optimization\n","# random_grid={\n","# \"max_depth\":[2,4,6],\n","# \"min_samples_split\":[2,5,10],\n","# \"min_samples_leaf\":[1,2,4]}\n","# random_search= GridSearchCV(estimator=RF,param_grid=random_grid,cv=5)\n","\n","\n","# print(\"random_search\")\n","# random_search.fit(X_train, y_train)\n","# splitSmote(random_search)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.9407407407407407\n","{'max_depth': 6, 'min_samples_leaf': 2, 'min_samples_split': 5}\n","{'mean_fit_time': array([0.10915833, 0.10873075, 0.10442495, 0.10606613, 0.10393457,\n","       0.1043633 , 0.10886307, 0.11078372, 0.11789246, 0.11568594,\n","       0.11351151, 0.10705094, 0.10543561, 0.11611819, 0.10590014,\n","       0.10392418, 0.10913391, 0.10713654, 0.10808372, 0.10603871,\n","       0.10830426, 0.10976267, 0.10553489, 0.1068552 , 0.10562015,\n","       0.10437803, 0.10872912]), 'std_fit_time': array([3.45143893e-03, 3.92021144e-03, 1.02617642e-03, 3.10341403e-03,\n","       1.48566726e-05, 8.66248013e-04, 6.30879043e-03, 9.06395404e-03,\n","       5.92463911e-03, 1.45218046e-02, 5.98650097e-03, 4.61625434e-03,\n","       4.36142252e-03, 1.61829348e-02, 3.09842999e-03, 1.34283643e-05,\n","       3.46748557e-03, 3.92682770e-03, 3.28357460e-03, 3.09782790e-03,\n","       5.66847922e-03, 5.94912407e-03, 3.20990102e-03, 3.60520352e-03,\n","       3.15721474e-03, 8.79943959e-04, 3.93145805e-03]), 'mean_score_time': array([0.00800824, 0.00959697, 0.00800362, 0.00959997, 0.01278086,\n","       0.0096015 , 0.0095933 , 0.0095942 , 0.00798049, 0.01280465,\n","       0.01178083, 0.0127912 , 0.01279249, 0.01279125, 0.0079947 ,\n","       0.00799408, 0.00800071, 0.00799274, 0.00978241, 0.00959201,\n","       0.00959373, 0.01278591, 0.01118035, 0.01011348, 0.0114224 ,\n","       0.0111927 , 0.00799351]), 'std_score_time': array([1.52311970e-05, 3.19831398e-03, 1.23652619e-05, 3.19290539e-03,\n","       3.92012673e-03, 3.19464244e-03, 3.19860038e-03, 3.19743179e-03,\n","       1.69586517e-05, 3.91365894e-03, 3.60910852e-03, 3.93022503e-03,\n","       3.91722584e-03, 3.91640795e-03, 7.44843452e-07, 1.90734863e-07,\n","       1.29482143e-05, 3.68431926e-06, 3.12611059e-03, 3.19864819e-03,\n","       3.19802765e-03, 3.92428364e-03, 3.91347565e-03, 3.10513115e-03,\n","       4.21411502e-03, 3.91631048e-03, 3.04728419e-06]), 'param_max_depth': masked_array(data=[2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","                   6, 6, 6, 6, 6, 6, 6, 6, 6],\n","             mask=[False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False],\n","       fill_value='?',\n","            dtype=object), 'param_min_samples_leaf': masked_array(data=[1, 1, 1, 2, 2, 2, 4, 4, 4, 1, 1, 1, 2, 2, 2, 4, 4, 4,\n","                   1, 1, 1, 2, 2, 2, 4, 4, 4],\n","             mask=[False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False],\n","       fill_value='?',\n","            dtype=object), 'param_min_samples_split': masked_array(data=[2, 5, 10, 2, 5, 10, 2, 5, 10, 2, 5, 10, 2, 5, 10, 2, 5,\n","                   10, 2, 5, 10, 2, 5, 10, 2, 5, 10],\n","             mask=[False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False],\n","       fill_value='?',\n","            dtype=object), 'params': [{'max_depth': 2, 'min_samples_leaf': 1, 'min_samples_split': 2}, {'max_depth': 2, 'min_samples_leaf': 1, 'min_samples_split': 5}, {'max_depth': 2, 'min_samples_leaf': 1, 'min_samples_split': 10}, {'max_depth': 2, 'min_samples_leaf': 2, 'min_samples_split': 2}, {'max_depth': 2, 'min_samples_leaf': 2, 'min_samples_split': 5}, {'max_depth': 2, 'min_samples_leaf': 2, 'min_samples_split': 10}, {'max_depth': 2, 'min_samples_leaf': 4, 'min_samples_split': 2}, {'max_depth': 2, 'min_samples_leaf': 4, 'min_samples_split': 5}, {'max_depth': 2, 'min_samples_leaf': 4, 'min_samples_split': 10}, {'max_depth': 4, 'min_samples_leaf': 1, 'min_samples_split': 2}, {'max_depth': 4, 'min_samples_leaf': 1, 'min_samples_split': 5}, {'max_depth': 4, 'min_samples_leaf': 1, 'min_samples_split': 10}, {'max_depth': 4, 'min_samples_leaf': 2, 'min_samples_split': 2}, {'max_depth': 4, 'min_samples_leaf': 2, 'min_samples_split': 5}, {'max_depth': 4, 'min_samples_leaf': 2, 'min_samples_split': 10}, {'max_depth': 4, 'min_samples_leaf': 4, 'min_samples_split': 2}, {'max_depth': 4, 'min_samples_leaf': 4, 'min_samples_split': 5}, {'max_depth': 4, 'min_samples_leaf': 4, 'min_samples_split': 10}, {'max_depth': 6, 'min_samples_leaf': 1, 'min_samples_split': 2}, {'max_depth': 6, 'min_samples_leaf': 1, 'min_samples_split': 5}, {'max_depth': 6, 'min_samples_leaf': 1, 'min_samples_split': 10}, {'max_depth': 6, 'min_samples_leaf': 2, 'min_samples_split': 2}, {'max_depth': 6, 'min_samples_leaf': 2, 'min_samples_split': 5}, {'max_depth': 6, 'min_samples_leaf': 2, 'min_samples_split': 10}, {'max_depth': 6, 'min_samples_leaf': 4, 'min_samples_split': 2}, {'max_depth': 6, 'min_samples_leaf': 4, 'min_samples_split': 5}, {'max_depth': 6, 'min_samples_leaf': 4, 'min_samples_split': 10}], 'split0_test_score': array([0.92592593, 0.92592593, 0.92592593, 0.92592593, 0.92592593,\n","       0.92592593, 0.92592593, 0.92592593, 0.92592593, 0.98148148,\n","       0.98148148, 0.98148148, 0.98148148, 0.98148148, 1.        ,\n","       1.        , 1.        , 0.98148148, 0.98148148, 0.98148148,\n","       0.98148148, 0.98148148, 1.        , 1.        , 1.        ,\n","       1.        , 1.        ]), 'split1_test_score': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 'split2_test_score': array([0.7962963 , 0.7962963 , 0.7962963 , 0.7962963 , 0.7962963 ,\n","       0.7962963 , 0.7962963 , 0.7962963 , 0.7962963 , 0.87037037,\n","       0.87037037, 0.88888889, 0.87037037, 0.87037037, 0.87037037,\n","       0.77777778, 0.77777778, 0.77777778, 0.88888889, 0.88888889,\n","       0.87037037, 0.87037037, 0.88888889, 0.87037037, 0.88888889,\n","       0.88888889, 0.88888889]), 'split3_test_score': array([0.96296296, 0.96296296, 0.96296296, 0.96296296, 0.96296296,\n","       0.96296296, 0.96296296, 0.96296296, 0.96296296, 0.98148148,\n","       0.98148148, 0.98148148, 0.98148148, 0.98148148, 0.98148148,\n","       0.98148148, 0.98148148, 0.98148148, 0.98148148, 0.98148148,\n","       0.98148148, 0.98148148, 0.98148148, 0.98148148, 0.98148148,\n","       0.98148148, 0.98148148]), 'split4_test_score': array([0.83333333, 0.83333333, 0.83333333, 0.83333333, 0.83333333,\n","       0.83333333, 0.83333333, 0.83333333, 0.83333333, 0.83333333,\n","       0.83333333, 0.83333333, 0.83333333, 0.83333333, 0.83333333,\n","       0.83333333, 0.83333333, 0.83333333, 0.81481481, 0.81481481,\n","       0.81481481, 0.83333333, 0.83333333, 0.83333333, 0.83333333,\n","       0.83333333, 0.83333333]), 'mean_test_score': array([0.9037037 , 0.9037037 , 0.9037037 , 0.9037037 , 0.9037037 ,\n","       0.9037037 , 0.9037037 , 0.9037037 , 0.9037037 , 0.93333333,\n","       0.93333333, 0.93703704, 0.93333333, 0.93333333, 0.93703704,\n","       0.91851852, 0.91851852, 0.91481481, 0.93333333, 0.93333333,\n","       0.92962963, 0.93333333, 0.94074074, 0.93703704, 0.94074074,\n","       0.94074074, 0.94074074]), 'std_test_score': array([0.07715802, 0.07715802, 0.07715802, 0.07715802, 0.07715802,\n","       0.07715802, 0.07715802, 0.07715802, 0.07715802, 0.06789001,\n","       0.06789001, 0.06478835, 0.06789001, 0.06789001, 0.07085602,\n","       0.0941353 , 0.0941353 , 0.09117432, 0.07085602, 0.07085602,\n","       0.07351642, 0.06789001, 0.06768766, 0.07085602, 0.06768766,\n","       0.06768766, 0.06768766]), 'rank_test_score': array([19, 19, 19, 19, 19, 19, 19, 19, 19,  8,  8,  5,  8,  8,  5, 16, 16,\n","       18,  8,  8, 15,  8,  1,  5,  1,  1,  1])}\n"]}],"source":["# #print(random_search.best_estimator_)\n","# print(random_search.best_score_)\n","# print(random_search.best_params_)\n","# print(random_search.cv_results_)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 5. Training"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 5.1 Before Optimazation "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["KNN\n","Accuracy scores for each fold:  [0.9365079365079365, 0.9193548387096774, 0.7903225806451613, 0.9516129032258065, 0.7741935483870968]\n","Mean accuracy score:  0.8743983614951356\n","GradientBoosting\n","Accuracy scores for each fold:  [0.9682539682539683, 1.0, 0.7903225806451613, 0.9838709677419355, 0.7419354838709677]\n","Mean accuracy score:  0.8968766001024064\n","LogisticRegression\n","Accuracy scores for each fold:  [0.9047619047619048, 1.0, 0.8225806451612904, 0.967741935483871, 0.6612903225806451]\n","Mean accuracy score:  0.8712749615975423\n","RandomForest\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\rayoo\\AppData\\Local\\Temp\\ipykernel_45220\\3765096017.py:21: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  model.fit(x_sm, y_sm)\n","C:\\Users\\rayoo\\AppData\\Local\\Temp\\ipykernel_45220\\3765096017.py:21: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  model.fit(x_sm, y_sm)\n","C:\\Users\\rayoo\\AppData\\Local\\Temp\\ipykernel_45220\\3765096017.py:21: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  model.fit(x_sm, y_sm)\n","C:\\Users\\rayoo\\AppData\\Local\\Temp\\ipykernel_45220\\3765096017.py:21: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  model.fit(x_sm, y_sm)\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy scores for each fold:  [0.9523809523809523, 1.0, 0.7903225806451613, 0.967741935483871, 0.7741935483870968]\n","Mean accuracy score:  0.8969278033794164\n","DecisionTree\n","Accuracy scores for each fold:  [0.9841269841269841, 0.967741935483871, 0.7903225806451613, 0.9516129032258065, 0.7096774193548387]\n","Mean accuracy score:  0.8806963645673322\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\rayoo\\AppData\\Local\\Temp\\ipykernel_45220\\3765096017.py:21: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  model.fit(x_sm, y_sm)\n"]}],"source":["# Base models \n","kCls=KNeighborsClassifier()\n","print(\"KNN\")\n","splitSmote (kCls)\n","\n","gb_clf2 = GradientBoostingClassifier()\n","print(\"GradientBoosting\")\n","splitSmote (gb_clf2)\n","\n","logreg=LogisticRegression()\n","print(\"LogisticRegression\")\n","splitSmote (logreg)\n","\n","RF= RandomForestClassifier()\n","print(\"RandomForest\")\n","splitSmote (RF)\n","\n","clf=DecisionTreeClassifier()\n","print(\"DecisionTree\")\n","splitSmote (clf)\n","\n","\n","\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### After Optimization"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#TODO: write the code here"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 5. Evaluation"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 5.1 Before Optimization"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","def report( y_test, pred ):\n","    #report \n","    print(classification_report(y_test,pred,target_names=['0','1','2','3','4','5']))\n","\n","\n","def confusionMatrix():\n","    #confusion_matrix\n","    #the result will show how mwny sucessful predition and wrong from each class\n","\n","    cm = confusion_matrix(y_test, pred)\n","    plt.figure(figsize=(10,7))\n","\n","    sns.heatmap(cm, annot=True, fmt='.2g', cmap='Blues')\n","\n","    # TN   FP\n","    # FN   TP"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"ename":"TypeError","evalue":"first argument must be an iterable of pandas objects, you passed an object of type \"DataFrame\"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m metrics \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mconcat(metrics, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,names\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mDecisionTree\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mDecisionTreeOpt\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mKNN\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mGradientBoosting\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mLogisticRegression\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mRandomForest\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m      2\u001b[0m metrics\u001b[39m.\u001b[39mcolumns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mDecisionTree\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mDecisionTreeOpt\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mKNN\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mGradientBoosting\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mLogisticRegression\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mRandomForest\u001b[39m\u001b[39m'\u001b[39m]\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/reshape/concat.py:368\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[39m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, allowed_args\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mobjs\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    147\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconcat\u001b[39m(\n\u001b[1;32m    148\u001b[0m     objs: Iterable[NDFrame] \u001b[39m|\u001b[39m Mapping[HashableT, NDFrame],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    157\u001b[0m     copy: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    158\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m    159\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[39m    Concatenate pandas objects along a particular axis.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[39m    1   3   4\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 368\u001b[0m     op \u001b[39m=\u001b[39m _Concatenator(\n\u001b[1;32m    369\u001b[0m         objs,\n\u001b[1;32m    370\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[1;32m    371\u001b[0m         ignore_index\u001b[39m=\u001b[39;49mignore_index,\n\u001b[1;32m    372\u001b[0m         join\u001b[39m=\u001b[39;49mjoin,\n\u001b[1;32m    373\u001b[0m         keys\u001b[39m=\u001b[39;49mkeys,\n\u001b[1;32m    374\u001b[0m         levels\u001b[39m=\u001b[39;49mlevels,\n\u001b[1;32m    375\u001b[0m         names\u001b[39m=\u001b[39;49mnames,\n\u001b[1;32m    376\u001b[0m         verify_integrity\u001b[39m=\u001b[39;49mverify_integrity,\n\u001b[1;32m    377\u001b[0m         copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m    378\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[1;32m    379\u001b[0m     )\n\u001b[1;32m    381\u001b[0m     \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39mget_result()\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/reshape/concat.py:403\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    390\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    391\u001b[0m     objs: Iterable[NDFrame] \u001b[39m|\u001b[39m Mapping[HashableT, NDFrame],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    400\u001b[0m     sort\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    401\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(objs, (ABCSeries, ABCDataFrame, \u001b[39mstr\u001b[39m)):\n\u001b[0;32m--> 403\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    404\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mfirst argument must be an iterable of pandas \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    405\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mobjects, you passed an object of type \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(objs)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    406\u001b[0m         )\n\u001b[1;32m    408\u001b[0m     \u001b[39mif\u001b[39;00m join \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mouter\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    409\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintersect \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n","\u001b[0;31mTypeError\u001b[0m: first argument must be an iterable of pandas objects, you passed an object of type \"DataFrame\""]}],"source":["metrics = pd.concat(metrics, axis=1,names=['DecisionTree','DecisionTreeOpt','KNN','GradientBoosting','LogisticRegression','RandomForest'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>DecisionTree</th>\n","      <th>DecisionTreeOpt</th>\n","      <th>KNN</th>\n","      <th>GradientBoosting</th>\n","      <th>LogisticRegression</th>\n","      <th>RandomForest</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>precision</th>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.733490</td>\n","      <td>0.963374</td>\n","      <td>0.961147</td>\n","      <td>0.944874</td>\n","    </tr>\n","    <tr>\n","      <th>recall</th>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.776596</td>\n","      <td>0.968085</td>\n","      <td>0.968085</td>\n","      <td>0.968085</td>\n","    </tr>\n","    <tr>\n","      <th>fscore</th>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.749230</td>\n","      <td>0.963933</td>\n","      <td>0.962621</td>\n","      <td>0.954647</td>\n","    </tr>\n","    <tr>\n","      <th>accuracy</th>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.776596</td>\n","      <td>0.968085</td>\n","      <td>0.968085</td>\n","      <td>0.968085</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["           DecisionTree  DecisionTreeOpt       KNN  GradientBoosting  \\\n","precision           1.0              1.0  0.733490          0.963374   \n","recall              1.0              1.0  0.776596          0.968085   \n","fscore              1.0              1.0  0.749230          0.963933   \n","accuracy            1.0              1.0  0.776596          0.968085   \n","\n","           LogisticRegression  RandomForest  \n","precision            0.961147      0.944874  \n","recall               0.968085      0.968085  \n","fscore               0.962621      0.954647  \n","accuracy             0.968085      0.968085  "]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["metrics.columns=['DecisionTree','DecisionTreeOpt','KNN','GradientBoosting','LogisticRegression','RandomForest']\n","metrics"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 5.2 After Optimization"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#TODO: write the code here"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
