{"cells":[{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","import seaborn as sns\n","import matplotlib.pylab as plt\n","from imblearn.over_sampling import SMOTE\n","#%matplotlib inline\n","\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn import preprocessing\n","from sklearn.preprocessing import OrdinalEncoder\n","from sklearn.impute import SimpleImputer\n","from sklearn.model_selection import StratifiedKFold\n","\n","from scipy.stats import norm\n","from scipy import stats\n","from sklearn.impute import KNNImputer\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import precision_recall_fscore_support as score\n","\n","#DS\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import GridSearchCV, cross_val_score\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import classification_report\n","\n","#KNN\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","#GradientBoostingClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","\n","#LogisticRegressionCV\n","from sklearn.linear_model import LogisticRegressionCV\n","\n","#RandomForestClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","\n","def warn(*args, **kwargs):\n","    pass\n","import warnings\n","warnings.warn = warn"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 1. Loading the dataset"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["#loading the data from CSV file \n","data=pd.read_csv('final_Binary.csv')\n","data.head()\n","\n","'''dataFeatures= ['Side Chest Airbag-Driver', 'Side Chest Airbag-Passenger',\n","       'AEB Vulnerable Road Users', 'Side Head Airbag-Driver',\n","       'Side Head Airbag-Passenger', 'Seatbelt Reminder-Passenger',\n","       'AEB Car-to-Car', 'Belt Loadlimiter-Rear', 'Belt Pretensioner-Rear',\n","       'Side Head Airbag-Rear', 'Lane Assist System', 'Seatbelt Reminder-Rear',\n","       'Safety Assist', 'Speed Assistance', 'Adult Occupant',\n","       'Centre Airbag-Driver', 'Child Occupant', 'Tested Model',\n","       'Isofix/i-Size-Passenger'] '''\n","\n","dataFeatures= ['Adult Occupant', 'Belt Pretensioner-Rear', 'Safety Assist',\n","       'Child Occupant', 'Class', 'Side Pelvis Airbag-Rear',\n","       'AEB Vulnerable Road Users']\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/plain":["Index(['Adult Occupant', 'Belt Pretensioner-Rear', 'Safety Assist',\n","       'Child Occupant', 'Class', 'Side Pelvis Airbag-Rear',\n","       'AEB Vulnerable Road Users', 'Rate'],\n","      dtype='object')"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# print the columns in the dataset\n","data.columns"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["The range of feature inputs are within 0.0 to 1.0\n"]}],"source":["'''# Create a MinMaxScaler object for numrical data\n","scaler = MinMaxScaler()\n","\n","# Scaling the raw input features \n","feature_cols=data.columns[:-1]\n","X= scaler.fit_transform(data[feature_cols])\n","\n","print(f\"The range of feature inputs are within {X.min()} to {X.max()}\")'''"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 2. Split the dataset "]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Training dataset shape, X_train: (217, 7), y_train: (217,)\n","Testing dataset shape, X_test: (94, 7), y_test: (94,)\n"]}],"source":["\n","from sklearn.model_selection import StratifiedShuffleSplit\n","\n","feature_cols=data.columns[:-1]\n","# Get the split indexes\n","strat_shuf_split = StratifiedShuffleSplit(n_splits=1, \n","                                          test_size=0.3, random_state=0)\n","\n","train_idx, test_idx = next(strat_shuf_split.split(data[feature_cols], data['Rate']))\n","\n","# Create the dataframes\n","\n","\n","X_train = data.loc[train_idx, dataFeatures]\n","y_train = data.loc[train_idx, 'Rate']\n","\n","X_test  = data.loc[test_idx, dataFeatures]\n","y_test  = data.loc[test_idx, 'Rate']\n","\n","print(f\"Training dataset shape, X_train: {X_train.shape}, y_train: {y_train.shape}\")\n","print(f\"Testing dataset shape, X_test: {X_test.shape}, y_test: {y_test.shape}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 3. Smoot "]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[],"source":["X= data.loc[:,dataFeatures]\n","y= data.loc[:,[\"Rate\"]]\n","\n","def splitSmote (model):\n","    # Initialize the Stratified K-fold Cross-validator with 5 splits\n","    sk=StratifiedKFold(n_splits=5)\n","\n","    # Initialize the array to store the accuracy scores\n","    tr_accuracy_scores = []\n","    tst_accuracy_scores = []\n","\n","    # Perform cross-validation\n","    for train_index, test_index in sk.split(X, y):\n","        # Split the data into training and test sets\n","        x_train_fold, x_test_fold = X.loc[train_index,:], X.loc[test_index,:]\n","        y_train_fold, y_test_fold = y.loc[train_index,:], y.loc[test_index,:]\n","\n","        #smote = SMOTE(sampling_strategy='minority')\n","        smote = SMOTE(sampling_strategy=0.5)\n","        x_sm, y_sm = smote.fit_resample(x_train_fold, y_train_fold)\n","        #Fit the model to the training data\n","        model.fit(x_sm, np.ravel(y_sm))\n","        # Make predictions on the test data\n","        yTrain_pred = model.predict(x_train_fold)\n","        yTest_pred = model.predict(x_test_fold)\n","\n","        # Calculate the accuracy score and append it to the list\n","        tr_accuracy_scores.append(accuracy_score(y_train_fold, yTrain_pred))\n","        tst_accuracy_scores.append(accuracy_score(y_test_fold, yTest_pred))\n","\n","\n","\n","    # Print the accuracy scores for each fold\n","    print(\"Accuracy scores for each training fold: \", tr_accuracy_scores)\n","    print(\"Accuracy scores for each testing fold: \", tst_accuracy_scores)\n","\n","    # Calculate the mean accuracy scores\n","    print(\"Traning Mean accuracy score: \", np.mean(tr_accuracy_scores))\n","    print(\"Testing Mean accuracy score: \", np.mean(tst_accuracy_scores))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 4. Data normalization "]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/plain":["1    0.718894\n","0    0.281106\n","Name: Rate, dtype: float64"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["y_train.value_counts(normalize=True)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":["1    0.723404\n","0    0.276596\n","Name: Rate, dtype: float64"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["y_test.value_counts(normalize=True)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 4. Models "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 4.1 Decision Tree "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["before optimization"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy scores for each fold:  [0.9523809523809523, 0.967741935483871, 0.7903225806451613, 0.9838709677419355, 0.7258064516129032]\n","Mean accuracy score:  0.8840245775729647\n"]}],"source":["# DecisionTreeClassifier\n","#importing the classfier\n","metrics=[]\n","\n","clf=DecisionTreeClassifier(random_state=0)\n","#clf2=clf.fit(X_train,y_train)\n","\n","\n","#y_pred.append(pd.Series(clf2.predict(X_test), name='DecisionTreeClassifier'))\n","# Preciision, recall, f-score from the multi-class support function\n","\n","# precision, recall, fscore, _ = score(y_test, clf2.predict(X_test), average='weighted')\n","# accuracy = accuracy_score(y_test, clf2.predict(X_test))\n","# metrics.append(pd.Series({'precision':precision, 'recall':recall,'fscore':fscore, 'accuracy':accuracy}))\n","\n","\n","splitSmote (clf)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["After optimization"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy scores for each fold:  [0.9365079365079365, 0.8870967741935484, 0.7903225806451613, 0.967741935483871, 0.7741935483870968]\n","Mean accuracy score:  0.8711725550435228\n"]}],"source":["# DecisionTree opt\n","\n","\n","#optimization\n","param_grid={\n","\"max_depth\":[2,4,6],\n","\"min_samples_split\":[2,5,10],\n","\"min_samples_leaf\":[1,2,4]}\n","grid_search= GridSearchCV(estimator=clf,param_grid=param_grid,cv=5)\n","\n","# grid_search.fit(X_train,y_train)\n","# print(\"Best hyper-param: \",grid_search.best_params_ )\n","# print(\"Best estimator: \",grid_search.best_estimator_ )\n","# print(\"Best score: \",grid_search.best_score_ )\n","\n","\n","# #precision, recall, fscore, _ = score(y_test, grid_search.predict(X_test), average='weighted')\n","# accuracy = accuracy_score(y_test, grid_search.predict(X_test))\n","# metrics.append(pd.Series({'precision':precision, 'recall':recall,'fscore':fscore, 'accuracy':accuracy}))\n","\n","\n","splitSmote (kCls)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["grid_search\n","Accuracy scores for each fold:  [0.9841269841269841, 0.9838709677419355, 0.7741935483870968, 0.9516129032258065, 0.6935483870967742]\n","Mean accuracy score:  0.8774705581157194\n"]}],"source":["#DT aftar opt\n","\n","print(\"grid_search\")\n","\n","splitSmote(grid_search)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 4.2 KNN"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Before optimization"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy scores for each fold:  [0.9206349206349206, 0.9193548387096774, 0.7903225806451613, 0.9838709677419355, 0.7580645161290323]\n","Mean accuracy score:  0.8744495647721454\n"]}],"source":["# KNN\n","#importing the classfier\n","\n","kCls=KNeighborsClassifier()\n","splitSmote (kCls)\n","#kCls.fit(X_train,y_train)\n","\n","#y_pred.append(pd.Series(kCls.predict(X_test), name='KNeighborsClassifier'))\n","\n","#precision, recall, fscore, _ = score(y_test, kCls.predict(X_test), average='weighted')\n","#accuracy = accuracy_score(y_test, kCls.predict(X_test))\n","#metrics.append(pd.Series({'precision':precision, 'recall':recall,'fscore':fscore, 'accuracy':accuracy}))\n","\n","#kCls=KNeighborsClassifier(n_neighbors=9)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["After optimization"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#TODO: Write the code here"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 4.3 GradientBoosting"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Before optimization"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy scores for each training fold:  [1.0, 0.9959839357429718, 1.0, 1.0, 1.0]\n","Accuracy scores for each testing fold:  [0.9682539682539683, 1.0, 0.7903225806451613, 0.9838709677419355, 0.7096774193548387]\n","Traning Mean accuracy score:  0.9991967871485944\n","Testing Mean accuracy score:  0.8904249871991807\n"]}],"source":["# GradientBoostingClassifier\n","#importing the classfier\n","\n","gb_clf = GradientBoostingClassifier()\n","splitSmote(gb_clf)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["After optimization"]},{"cell_type":"code","execution_count":87,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy scores for each training fold:  [0.9274193548387096, 0.9839357429718876, 0.9759036144578314, 0.9879518072289156, 0.9959839357429718]\n","Accuracy scores for each testing fold:  [0.8888888888888888, 0.967741935483871, 0.7903225806451613, 0.9516129032258065, 0.7580645161290323]\n","Traning Mean accuracy score:  0.9742388910480632\n","Testing Mean accuracy score:  0.8713261648745521\n"]}],"source":["# optimizing the GradientBoostingClassifier using RandomizedSearchCV\n","from sklearn.model_selection import GridSearchCV\n","parameters = {'n_estimators' : [20,50,70,100], \n","                'learning_rate' : [0.01, 0.02, 0.03, 0.04], \n","                'max_depth' : [4,6,8,10],\n","                'subsample' : [0.9, 0.7 ,0.5 , 0.2 ],\n","                'max_features' :[3,5,7]\n","                }\n","\n","grid = GridSearchCV(estimator= gb_clf, \n","                    param_grid= parameters, \n","                    cv= 2,\n","                    n_jobs= -1)\n","\n","\n","grid.fit(X_train, y_train)\n","splitSmote(grid)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":91,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["GradientBoostingClassifier(learning_rate=0.02, max_depth=6, max_features=7,\n","                           n_estimators=70, subsample=0.5)\n","0.9481481481481482\n","{'learning_rate': 0.02, 'max_depth': 6, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.5}\n","{'mean_fit_time': array([0.04080141, 0.04237103, 0.0576539 , 0.03782797, 0.119385  ,\n","       0.11440969, 0.10527384, 0.09539688, 0.17491007, 0.16693747,\n","       0.16909063, 0.15619743, 0.23915851, 0.23641419, 0.23705053,\n","       0.19027007, 0.04436958, 0.04681587, 0.04169714, 0.03716147,\n","       0.11075449, 0.13038552, 0.12795711, 0.10132539, 0.16424847,\n","       0.16849053, 0.15819299, 0.13795483, 0.24959648, 0.2531085 ,\n","       0.22965193, 0.20380056, 0.05256903, 0.05493033, 0.04705405,\n","       0.03778708, 0.12419891, 0.1306895 , 0.12908149, 0.11770391,\n","       0.13104582, 0.15109789, 0.14348495, 0.12707603, 0.21507251,\n","       0.22988653, 0.22000456, 0.20920503, 0.0677036 , 0.07489216,\n","       0.06903839, 0.04861796, 0.31010747, 0.3061595 , 0.31030154,\n","       0.24383748, 0.43744349, 0.41374838, 0.39307952, 0.31660604,\n","       0.62449086, 0.59428704, 0.55377078, 0.4524039 , 0.09723163,\n","       0.11774445, 0.10888708, 0.11070395, 0.27420044, 0.28389359,\n","       0.26497996, 0.22112346, 0.31485939, 0.30853355, 0.29212415,\n","       0.24437547, 0.33308804, 0.35107088, 0.33504701, 0.2797215 ,\n","       0.05244613, 0.05646765, 0.05226374, 0.0416435 , 0.11505198,\n","       0.12944412, 0.14696836, 0.12132847, 0.18255401, 0.19521809,\n","       0.19111109, 0.16731191, 0.24362803, 0.2730726 , 0.28384054,\n","       0.23049843, 0.06783962, 0.08219707, 0.07258463, 0.05016994,\n","       0.22443414, 0.22161567, 0.21909654, 0.13867807, 0.30450904,\n","       0.31877398, 0.293239  , 0.20502245, 0.47300541, 0.49460554,\n","       0.44392049, 0.30331314, 0.05085194, 0.05124855, 0.06165397,\n","       0.04830718, 0.13109088, 0.15798151, 0.1633004 , 0.12464404,\n","       0.22525811, 0.30679691, 0.33844948, 0.24789834, 0.48505962,\n","       0.45119107, 0.43430007, 0.32310212, 0.03997755, 0.04240358,\n","       0.04488504, 0.03856254, 0.12932909, 0.15914285, 0.13807452,\n","       0.12567508, 0.26190853, 0.2759949 , 0.30340362, 0.27260447,\n","       0.22293544, 0.25743103, 0.26007402, 0.22506046, 0.05403042,\n","       0.07287693, 0.09030437, 0.04657471, 0.19010794, 0.24382949,\n","       0.23618698, 0.12217104, 0.38129508, 0.43924415, 0.37486827,\n","       0.2798425 , 0.50665188, 0.62325716, 0.57551789, 0.299703  ,\n","       0.04690254, 0.09915698, 0.11440134, 0.10320854, 0.25436747,\n","       0.30672598, 0.29210293, 0.22685349, 0.21904898, 0.34528005,\n","       0.35828805, 0.24261904, 0.430323  , 0.51843715, 0.47304261,\n","       0.33527434, 0.07718849, 0.04723811, 0.03914702, 0.04161084,\n","       0.14104998, 0.1729815 , 0.17642462, 0.14704859, 0.16709733,\n","       0.21370602, 0.22021604, 0.17867053, 0.25794303, 0.56499767,\n","       0.52725351, 0.48357093, 0.25002718, 0.04404616, 0.056072  ,\n","       0.04355073, 0.12262416, 0.12428141, 0.11934948, 0.10316193,\n","       0.19606555, 0.1983211 , 0.19008338, 0.17990661, 0.25645828,\n","       0.25519502, 0.24669766, 0.22247052, 0.05867648, 0.06007397,\n","       0.06067288, 0.06129289, 0.10343993, 0.09591353, 0.10161448,\n","       0.08744442, 0.14744031, 0.14156044, 0.13973939, 0.11526811,\n","       0.21187258, 0.21741796, 0.21208942, 0.19261038, 0.03551209,\n","       0.03433645, 0.03460109, 0.03258395, 0.17523003, 0.16942203,\n","       0.16018999, 0.14831698, 0.32077789, 0.34854949, 0.33052146,\n","       0.30851233, 0.45496106, 0.47904611, 0.463781  , 0.43638647,\n","       0.1261524 , 0.15359378, 0.14251864, 0.05476642, 0.38320017,\n","       0.37148535, 0.37239075, 0.32445848, 0.4351095 , 0.42168951,\n","       0.40099394, 0.31940699, 0.63546252, 0.62263799, 0.55951703,\n","       0.45758355, 0.10505009, 0.11768258, 0.09929252, 0.08539939,\n","       0.22704649, 0.22441995, 0.22451162, 0.21334755, 0.19708145,\n","       0.17830193, 0.1656388 , 0.13838339, 0.27369392, 0.28390849,\n","       0.27732813, 0.21690094, 0.03393698, 0.05209696, 0.05202508,\n","       0.04712594, 0.11183548, 0.11867607, 0.12100792, 0.10971248,\n","       0.14665604, 0.17788494, 0.16712904, 0.14479041, 0.22841048,\n","       0.25526965, 0.25900412, 0.22439802, 0.05202997, 0.08251381,\n","       0.07001841, 0.05118144, 0.33600473, 0.327124  , 0.32138848,\n","       0.22727001, 0.38223398, 0.36061561, 0.3332454 , 0.24339902,\n","       0.45225072, 0.49620199, 0.45778   , 0.31882429, 0.05007935,\n","       0.07322705, 0.063676  , 0.05369794, 0.14641964, 0.16915989,\n","       0.16075003, 0.12231958, 0.19584858, 0.29042339, 0.27124453,\n","       0.19389176, 0.36842453, 0.42163491, 0.36803389, 0.27497017,\n","       0.04935253, 0.0537591 , 0.04599833, 0.05948913, 0.13129389,\n","       0.16216946, 0.15457046, 0.12002361, 0.21991217, 0.22701001,\n","       0.2175405 , 0.19999349, 0.26118743, 0.37798369, 0.33336246,\n","       0.28569543, 0.07601345, 0.08920419, 0.09352946, 0.07085741,\n","       0.23928666, 0.27634561, 0.27677929, 0.16185296, 0.33483601,\n","       0.39186847, 0.34548295, 0.20749438, 0.58676004, 0.60777915,\n","       0.56000054, 0.31099892, 0.0745101 , 0.07271898, 0.06145298,\n","       0.05730247, 0.17250013, 0.21917593, 0.23909938, 0.17038953,\n","       0.30791748, 0.32686806, 0.30571103, 0.21454906, 0.28468943,\n","       0.4169805 , 0.36914945, 0.2619344 , 0.04182446, 0.04742062,\n","       0.04990935, 0.04251254, 0.11202812, 0.17817032, 0.18903351,\n","       0.14156246, 0.23376715, 0.24659932, 0.2565639 , 0.20866466,\n","       0.27643788, 0.32785642, 0.34185863, 0.28116691, 0.04024887,\n","       0.05223119, 0.04461455, 0.03440714, 0.1154207 , 0.12425792,\n","       0.13289356, 0.14274383, 0.17605746, 0.16596961, 0.13796318,\n","       0.11956704, 0.19862151, 0.19312453, 0.18836558, 0.1618731 ,\n","       0.03347147, 0.03542101, 0.03687286, 0.03871679, 0.11891341,\n","       0.1188339 , 0.10932302, 0.10925031, 0.12254095, 0.13302946,\n","       0.12186563, 0.10462642, 0.21829665, 0.21740198, 0.19866109,\n","       0.181867  , 0.03237092, 0.03436351, 0.03470504, 0.03391564,\n","       0.08886456, 0.09017444, 0.09332454, 0.08295751, 0.11813712,\n","       0.11692989, 0.11447239, 0.10558641, 0.18652403, 0.18627584,\n","       0.18668056, 0.17001963, 0.05412698, 0.05667448, 0.05413854,\n","       0.04082906, 0.20197558, 0.19503951, 0.1792345 , 0.15094113,\n","       0.20316947, 0.20021033, 0.18273854, 0.14190292, 0.30312443,\n","       0.29739416, 0.26500845, 0.20977151, 0.04857051, 0.05069864,\n","       0.04998088, 0.04358852, 0.12672663, 0.13350582, 0.12414861,\n","       0.10569358, 0.16230297, 0.17922223, 0.1751256 , 0.13669252,\n","       0.26579797, 0.25687051, 0.2501744 , 0.19437337, 0.04055703,\n","       0.04616952, 0.04507542, 0.04208004, 0.10113013, 0.12982893,\n","       0.14892483, 0.11740696, 0.17572355, 0.17531145, 0.17571366,\n","       0.13228405, 0.20561981, 0.24067795, 0.24183702, 0.2024802 ,\n","       0.05852294, 0.06822503, 0.06029904, 0.04821765, 0.17734873,\n","       0.18227398, 0.17222607, 0.12152255, 0.25813401, 0.28919148,\n","       0.26847947, 0.17993641, 0.43607831, 0.45587099, 0.41760302,\n","       0.28160131, 0.06500804, 0.05876493, 0.06074905, 0.04322827,\n","       0.13559794, 0.14495051, 0.15535927, 0.1127274 , 0.24602604,\n","       0.26483953, 0.26241744, 0.19662035, 0.40660012, 0.39024556,\n","       0.39243114, 0.29490805, 0.04695714, 0.06433606, 0.05255938,\n","       0.04564643, 0.12947297, 0.14973152, 0.16375351, 0.13801956,\n","       0.19958365, 0.20905757, 0.20409155, 0.17251337, 0.22455406,\n","       0.29714704, 0.29772794, 0.25097346, 0.06189239, 0.06470037,\n","       0.0687362 , 0.04474056, 0.16347492, 0.22147548, 0.20630908,\n","       0.13369393, 0.29500806, 0.33937204, 0.30115545, 0.19561398,\n","       0.50731409, 0.52658701, 0.45764804, 0.27326608, 0.04427493,\n","       0.05930161, 0.05835712, 0.04939508, 0.152637  , 0.20174646,\n","       0.21255291, 0.14858687, 0.19438493, 0.28193963, 0.2552706 ,\n","       0.17993367, 0.33443713, 0.38548517, 0.35708201, 0.2489109 ,\n","       0.040223  , 0.04429352, 0.03975558, 0.04066956, 0.10962558,\n","       0.15056717, 0.15174949, 0.11981905, 0.17398691, 0.21223998,\n","       0.2281971 , 0.17797518, 0.24472058, 0.28659058, 0.33346748,\n","       0.25486791, 0.04018378, 0.04256797, 0.04023933, 0.03660548,\n","       0.09626555, 0.09679401, 0.10032392, 0.08639944, 0.12804604,\n","       0.13525152, 0.12883604, 0.11777353, 0.17565739, 0.16757071,\n","       0.16827106, 0.15690958, 0.03630352, 0.03539944, 0.03529143,\n","       0.03466094, 0.089849  , 0.08990526, 0.08432603, 0.08146954,\n","       0.12590504, 0.12932384, 0.11277223, 0.10322869, 0.18047965,\n","       0.1657871 , 0.15771425, 0.14890039, 0.03687751, 0.0411936 ,\n","       0.03212607, 0.02990997, 0.08577693, 0.08852243, 0.08629036,\n","       0.0825367 , 0.11645007, 0.12062633, 0.11538506, 0.10344744,\n","       0.16400504, 0.17273843, 0.17448866, 0.1397686 , 0.05743194,\n","       0.04817152, 0.04704988, 0.03902698, 0.13948905, 0.13249445,\n","       0.12375402, 0.10810506, 0.21287394, 0.18708503, 0.1764493 ,\n","       0.14131963, 0.29347217, 0.27524066, 0.26198447, 0.20034933,\n","       0.05090106, 0.04930294, 0.05074537, 0.04377699, 0.10904753,\n","       0.11456132, 0.11742544, 0.10165799, 0.17376459, 0.17029297,\n","       0.15647948, 0.13833642, 0.24816954, 0.27295446, 0.26954794,\n","       0.23050404, 0.03823912, 0.04081249, 0.04296196, 0.03733754,\n","       0.09620845, 0.11330938, 0.23133492, 0.10439253, 0.46272552,\n","       0.49879956, 0.52947855, 0.36531198, 0.33313906, 0.33907259,\n","       0.32416642, 0.27704012, 0.09334469, 0.09593606, 0.09488344,\n","       0.06976116, 0.25948751, 0.30187809, 0.26805103, 0.16877198,\n","       0.41849387, 0.42187643, 0.3702209 , 0.25234604, 0.50187457,\n","       0.53243101, 0.47573662, 0.32123232, 0.05757356, 0.05246055,\n","       0.06144142, 0.05801702, 0.15685856, 0.19321489, 0.18176508,\n","       0.13523638, 0.22303343, 0.26648939, 0.26640046, 0.18579304,\n","       0.33332145, 0.38087845, 0.36354291, 0.26608849, 0.0513339 ,\n","       0.04514551, 0.04561448, 0.05204153, 0.12274468, 0.15060294,\n","       0.14328158, 0.13059402, 0.19451094, 0.21787167, 0.21471441,\n","       0.18256187, 0.23503506, 0.27321041, 0.27570951, 0.205531  ,\n","       0.05519295, 0.06026793, 0.06998253, 0.04494202, 0.15734446,\n","       0.22863853, 0.22451162, 0.12672651, 0.37872922, 0.46218145,\n","       0.40173995, 0.27400005, 0.45215762, 0.48608971, 0.40717745,\n","       0.24927843, 0.04000199, 0.05547488, 0.04707861, 0.03856921,\n","       0.13475347, 0.16898608, 0.17380297, 0.12707937, 0.18626499,\n","       0.23540032, 0.24518085, 0.16811991, 0.2931205 , 0.37775505,\n","       0.37362254, 0.24621964, 0.04253304, 0.04282391, 0.04498494,\n","       0.04467249, 0.30908275, 0.37528372, 0.43104112, 0.36375988,\n","       0.57472503, 0.65456402, 0.68681741, 0.60258389, 0.71678758,\n","       0.89644051, 0.85651886, 0.73970747]), 'std_fit_time': array([1.41155720e-03, 3.06892395e-03, 3.05199623e-03, 2.78496742e-03,\n","       2.31719017e-03, 7.70449638e-03, 4.28080559e-04, 1.18100643e-03,\n","       1.89509392e-02, 1.47973299e-02, 1.35003328e-02, 2.13195086e-02,\n","       2.77475119e-02, 1.92210674e-02, 1.81365013e-02, 1.79994106e-03,\n","       2.94172764e-03, 3.46398354e-03, 1.05202198e-03, 8.70585442e-04,\n","       6.23345375e-03, 7.44259357e-03, 2.66499519e-02, 1.89734697e-02,\n","       3.31904888e-02, 1.52004957e-02, 4.16195393e-03, 1.62459612e-02,\n","       2.15386152e-02, 2.91295052e-02, 1.51700974e-02, 1.94004774e-02,\n","       1.62118673e-02, 2.68948078e-03, 1.49888992e-02, 3.82006168e-03,\n","       1.27279758e-02, 3.29395533e-02, 3.20434570e-04, 1.45058632e-02,\n","       9.01937485e-04, 4.07397747e-03, 2.23815441e-03, 2.44021416e-04,\n","       2.23575830e-02, 2.25996971e-02, 1.88724995e-02, 1.49170160e-02,\n","       1.06183290e-02, 6.78408146e-03, 2.84457207e-03, 5.51116467e-03,\n","       9.03484821e-02, 6.59096241e-02, 6.44774437e-02, 7.84193277e-02,\n","       1.85213089e-02, 4.16496992e-02, 2.54373550e-02, 1.84659958e-02,\n","       1.93389654e-02, 9.17589664e-03, 2.07757950e-03, 1.00290775e-03,\n","       1.30248070e-03, 1.19194984e-02, 2.13340521e-02, 2.37779617e-02,\n","       2.47814655e-02, 1.23453140e-03, 1.83700323e-02, 1.82464123e-02,\n","       1.01437569e-01, 9.03795958e-02, 3.90158892e-02, 4.91013527e-02,\n","       4.56770658e-02, 2.78961658e-02, 3.45160961e-02, 2.23696232e-02,\n","       9.26208496e-03, 2.45463848e-03, 1.55210495e-03, 9.20355320e-03,\n","       1.08830929e-02, 8.95905495e-03, 7.01236725e-03, 1.04495287e-02,\n","       2.67589092e-02, 8.35800171e-03, 4.39500809e-03, 6.08205795e-04,\n","       2.47581005e-02, 4.06044722e-02, 8.19337368e-03, 1.98733807e-03,\n","       7.96556473e-03, 1.33479834e-02, 1.68156624e-03, 6.82306290e-03,\n","       8.10599327e-03, 1.40115023e-02, 5.22029400e-03, 1.17397308e-03,\n","       1.59891844e-02, 2.78401375e-03, 5.53905964e-03, 4.39155102e-03,\n","       3.62714529e-02, 1.44555569e-02, 3.51130962e-03, 4.92203236e-03,\n","       3.62038612e-04, 1.09505653e-02, 7.76398182e-03, 1.21698380e-02,\n","       2.11260319e-02, 2.51055956e-02, 9.59050655e-03, 1.88040733e-02,\n","       3.41820717e-02, 2.46368647e-02, 5.98475933e-02, 2.09155083e-02,\n","       1.23747468e-01, 7.76079893e-02, 6.23022318e-02, 5.04791737e-03,\n","       3.47256660e-03, 5.79941273e-03, 2.55692005e-03, 6.42943382e-03,\n","       1.98260546e-02, 1.81609392e-02, 2.01865435e-02, 4.79209423e-03,\n","       1.56385899e-02, 3.15319300e-02, 5.75485229e-02, 9.39154625e-03,\n","       3.70926857e-02, 4.57241535e-02, 2.85481215e-02, 6.83045387e-03,\n","       9.65452194e-03, 2.22396851e-03, 1.76684856e-02, 4.81045246e-03,\n","       4.14260626e-02, 1.60424709e-02, 1.60689354e-02, 4.85789776e-03,\n","       2.61241198e-02, 2.67803669e-03, 8.17214251e-02, 3.08126211e-02,\n","       4.07290459e-03, 1.03720903e-01, 1.11721039e-01, 7.20894337e-03,\n","       8.08441639e-03, 3.18092108e-02, 1.66084766e-02, 2.20985413e-02,\n","       4.22106981e-02, 5.52597046e-02, 2.94080973e-02, 1.94875002e-02,\n","       3.59830856e-02, 1.77458525e-02, 5.02970219e-02, 2.15890408e-02,\n","       1.55516982e-01, 2.10089684e-02, 1.20135546e-02, 1.84694529e-02,\n","       3.12044621e-02, 6.17909431e-03, 5.28991222e-03, 1.98960304e-04,\n","       2.09161043e-02, 7.22885132e-02, 8.07305574e-02, 5.55363894e-02,\n","       1.99983120e-02, 4.41589355e-02, 1.28130913e-02, 1.36674643e-02,\n","       3.91980410e-02, 1.62626505e-01, 1.70940518e-01, 1.92829013e-01,\n","       1.84827089e-01, 9.09495354e-03, 3.93104553e-03, 7.75694847e-03,\n","       3.28207016e-03, 6.90054893e-03, 1.39174461e-02, 9.30321217e-03,\n","       7.45116472e-02, 6.84759617e-02, 6.84465170e-02, 6.66644573e-02,\n","       3.67240906e-02, 3.63169909e-02, 4.65784073e-02, 3.73992920e-02,\n","       9.67335701e-03, 5.55598736e-03, 3.69393826e-03, 2.34508514e-03,\n","       1.50791407e-02, 3.19540501e-03, 5.92851639e-03, 1.40825510e-02,\n","       3.16134691e-02, 3.06864977e-02, 1.51284933e-02, 6.64198399e-03,\n","       3.33344936e-02, 2.20229626e-02, 2.72043943e-02, 2.83933878e-02,\n","       3.20899487e-03, 1.20651722e-03, 1.04415417e-03, 5.34057617e-04,\n","       4.37188148e-03, 1.44912004e-02, 4.34792042e-03, 1.35501623e-02,\n","       1.62713051e-01, 1.86013579e-01, 2.01916575e-01, 1.75550103e-01,\n","       3.09851170e-02, 5.50858974e-02, 5.69959879e-02, 5.63515425e-02,\n","       7.74813890e-02, 7.92329311e-02, 6.10505342e-02, 2.30360031e-03,\n","       1.54738188e-01, 1.29523396e-01, 1.63404465e-01, 1.54652715e-01,\n","       3.10565233e-02, 6.59561157e-03, 1.94331408e-02, 2.36179829e-02,\n","       9.01405811e-02, 1.76975012e-01, 1.52097106e-01, 1.15548491e-01,\n","       1.70080662e-02, 3.18062305e-03, 8.06450844e-03, 2.20441818e-03,\n","       2.62727737e-02, 3.56551409e-02, 1.65874958e-02, 2.01364756e-02,\n","       3.09635401e-02, 3.20303440e-03, 6.23786449e-03, 7.56502151e-04,\n","       9.08792019e-03, 1.66738033e-03, 6.05690479e-03, 2.36388445e-02,\n","       1.77693367e-03, 5.87308407e-03, 4.37784195e-03, 7.22205639e-03,\n","       2.23603249e-02, 2.47949362e-02, 1.34439468e-02, 3.70943546e-03,\n","       5.77521324e-03, 1.54129267e-02, 6.04987144e-03, 3.64565849e-03,\n","       3.24225426e-02, 2.15584040e-02, 1.06940269e-02, 4.81009483e-04,\n","       1.10598803e-02, 1.02128983e-02, 1.15994215e-02, 1.91652775e-03,\n","       6.61864281e-02, 9.37987566e-02, 9.38296318e-02, 7.57070780e-02,\n","       6.23651743e-02, 4.17464972e-02, 5.02845049e-02, 4.13640738e-02,\n","       2.98695564e-02, 8.43715668e-03, 2.57960558e-02, 8.31365585e-03,\n","       4.96959686e-03, 5.59103489e-03, 1.83950663e-02, 7.21609592e-03,\n","       3.83745432e-02, 2.49750614e-02, 5.14090061e-03, 2.03144550e-03,\n","       8.41033459e-03, 2.75113583e-02, 3.27293873e-02, 1.58655643e-02,\n","       5.96014261e-02, 2.42888927e-02, 1.54950619e-02, 9.39011574e-04,\n","       4.14335728e-03, 8.32390785e-03, 7.93838501e-03, 3.56805325e-03,\n","       1.42871141e-02, 2.64174938e-02, 3.39964628e-02, 5.57053089e-03,\n","       3.62139940e-02, 4.98518944e-02, 4.49924469e-02, 4.09853458e-03,\n","       5.61815500e-02, 2.57183313e-02, 1.34050846e-03, 3.95262241e-03,\n","       4.86934185e-03, 3.93903255e-03, 1.08573437e-02, 9.32943821e-03,\n","       1.17726326e-02, 3.46466303e-02, 2.49444246e-02, 1.71860456e-02,\n","       3.98879051e-02, 1.69314146e-02, 1.46080256e-02, 5.09059429e-03,\n","       6.17430210e-02, 3.35680246e-02, 1.77735090e-02, 1.84850693e-02,\n","       2.37078667e-02, 1.83190107e-02, 3.07881832e-03, 3.56459618e-03,\n","       2.95519829e-02, 5.18596172e-03, 3.64075899e-02, 3.32514048e-02,\n","       6.34394884e-02, 5.26080132e-02, 2.80508995e-02, 1.05249882e-02,\n","       2.05266476e-02, 2.82053947e-02, 1.41294003e-02, 5.72454929e-03,\n","       2.27344036e-03, 6.88445568e-03, 4.88853455e-03, 1.00244284e-02,\n","       6.01410866e-03, 9.85658169e-03, 3.79934311e-02, 4.07671928e-02,\n","       6.54739141e-02, 5.30046225e-02, 4.42149639e-02, 2.13785172e-02,\n","       4.35959101e-02, 3.95184755e-02, 1.57094002e-02, 2.24689245e-02,\n","       1.63006783e-03, 1.60280466e-02, 3.23748589e-03, 5.11884689e-04,\n","       1.16454363e-02, 2.97828913e-02, 3.92165184e-02, 4.84325886e-02,\n","       5.32234907e-02, 4.46875095e-02, 1.58320665e-02, 6.90495968e-03,\n","       1.67825222e-02, 8.89348984e-03, 2.17534304e-02, 2.22206116e-03,\n","       4.37939167e-03, 6.85298443e-03, 2.25615501e-03, 5.91039658e-04,\n","       6.30259514e-03, 3.06550264e-02, 2.68762112e-02, 2.55184174e-02,\n","       7.41291046e-03, 3.94868851e-03, 1.41847134e-03, 2.18391418e-04,\n","       1.49766207e-02, 1.74229145e-02, 3.24392319e-03, 7.24995136e-03,\n","       6.51514530e-03, 8.81671906e-04, 2.55095959e-03, 2.27749348e-03,\n","       7.44748116e-03, 8.57949257e-03, 4.40752506e-03, 6.80565834e-03,\n","       1.38680935e-02, 9.86933708e-04, 4.63461876e-03, 1.78158283e-03,\n","       5.14495373e-03, 3.16512585e-03, 4.51660156e-03, 1.67706013e-02,\n","       6.58893585e-03, 3.53956223e-03, 4.64558601e-04, 9.88125801e-04,\n","       1.26695633e-02, 1.80294514e-02, 5.19967079e-03, 1.77500248e-02,\n","       1.38095617e-02, 1.30105019e-02, 1.60551071e-03, 2.49409676e-03,\n","       6.16359711e-03, 3.18610668e-03, 1.71203613e-02, 1.39905214e-02,\n","       2.03239918e-03, 1.14096403e-02, 6.06322289e-03, 6.30939007e-03,\n","       1.89363956e-02, 1.73599720e-02, 3.82351875e-03, 2.77543068e-03,\n","       1.56588554e-02, 1.10591650e-02, 7.85350800e-04, 1.91044807e-03,\n","       3.26849222e-02, 1.84223652e-02, 4.75347042e-03, 1.33175850e-02,\n","       1.00098848e-02, 1.79147720e-03, 1.05452538e-03, 1.63900852e-03,\n","       1.73000097e-02, 4.24408913e-03, 2.61929035e-02, 9.73594189e-03,\n","       5.03904819e-02, 1.71355009e-02, 2.99572945e-04, 5.73289394e-03,\n","       1.42509937e-02, 1.01968050e-02, 4.39286232e-03, 2.94196606e-03,\n","       1.55079365e-02, 5.82706928e-03, 5.31613827e-03, 1.98352337e-03,\n","       2.22824812e-02, 1.37928724e-02, 1.73910856e-02, 7.13860989e-03,\n","       1.76969767e-02, 1.32145882e-02, 4.03344631e-03, 8.06546211e-03,\n","       1.75209045e-02, 4.15240526e-02, 1.18219852e-02, 3.43514681e-02,\n","       2.69496441e-03, 7.39002228e-03, 1.00851059e-02, 5.35249710e-05,\n","       3.08370590e-02, 1.49394274e-02, 2.06351280e-03, 4.36139107e-03,\n","       1.07598305e-03, 1.95983648e-02, 3.56825590e-02, 2.01414824e-02,\n","       3.84510756e-02, 3.27883959e-02, 2.60984898e-03, 9.27019119e-03,\n","       7.67791271e-03, 1.31528378e-02, 7.60436058e-03, 4.39572334e-03,\n","       1.61571503e-02, 2.84765959e-02, 2.27115154e-02, 3.82733345e-03,\n","       3.99945974e-02, 6.04183674e-02, 2.91244984e-02, 1.42995119e-02,\n","       2.08210945e-02, 3.18281651e-02, 7.46500492e-03, 1.67324543e-02,\n","       8.85331631e-03, 2.15244293e-03, 4.92298603e-03, 1.08754635e-03,\n","       3.55499983e-02, 1.59295797e-02, 1.24478340e-03, 9.63687897e-04,\n","       1.43897533e-03, 2.27688551e-02, 1.10963583e-02, 2.44891644e-03,\n","       4.93410826e-02, 4.20093536e-03, 5.91301918e-03, 7.55596161e-03,\n","       3.34858894e-04, 1.04036331e-02, 2.20929384e-02, 2.41398811e-03,\n","       2.10418701e-02, 3.31449509e-03, 1.78682804e-03, 1.36071444e-02,\n","       5.97568750e-02, 2.78927088e-02, 4.89342213e-03, 6.51347637e-03,\n","       5.91301918e-03, 3.21750641e-02, 1.39108896e-02, 2.25114822e-03,\n","       8.42487812e-03, 9.91356373e-03, 6.88552856e-03, 4.15956974e-03,\n","       1.52773857e-02, 1.89989805e-02, 2.26436853e-02, 1.13040209e-02,\n","       3.96981239e-02, 3.78811359e-02, 3.12719345e-02, 7.63392448e-03,\n","       5.72315454e-02, 4.14843559e-02, 3.35931778e-03, 2.36809254e-03,\n","       4.68301773e-03, 4.79102135e-03, 5.01394272e-04, 1.87957287e-03,\n","       6.12664223e-03, 5.50687313e-03, 7.45296478e-03, 2.40683556e-04,\n","       1.32679939e-03, 7.28440285e-03, 4.99904156e-03, 8.06736946e-03,\n","       4.56655025e-03, 4.61757183e-03, 2.05993652e-03, 1.89745426e-03,\n","       4.12344933e-03, 5.01251221e-03, 1.11556053e-03, 6.83188438e-04,\n","       1.10380650e-02, 1.10149384e-04, 1.21903419e-03, 8.56399536e-04,\n","       4.79221344e-05, 4.85122204e-03, 1.72638893e-03, 1.70552731e-03,\n","       8.64136219e-03, 2.89404392e-03, 3.84461880e-03, 2.75337696e-03,\n","       1.58059597e-03, 4.19497490e-04, 3.74913216e-04, 7.89999962e-04,\n","       1.11700296e-02, 5.98239899e-03, 2.88844109e-03, 2.57492065e-03,\n","       6.26397133e-03, 4.78386879e-04, 7.57932663e-04, 1.94764137e-03,\n","       9.13810730e-03, 2.79939175e-03, 1.67965889e-04, 8.43524933e-04,\n","       1.79600716e-03, 8.08143616e-03, 2.01594830e-03, 4.95100021e-03,\n","       7.93802738e-03, 1.62694454e-02, 8.74400139e-03, 4.37486172e-03,\n","       5.96046448e-04, 4.95898724e-03, 5.75351715e-03, 8.74638557e-04,\n","       6.12699986e-03, 1.64234638e-02, 1.38643980e-02, 5.81049919e-03,\n","       1.00195408e-03, 1.35709047e-02, 3.15940380e-03, 9.09399986e-03,\n","       1.70475245e-02, 2.02560425e-03, 5.46264648e-03, 1.45113468e-03,\n","       8.82661343e-03, 4.98092175e-03, 4.24563885e-03, 3.04055214e-03,\n","       5.08424044e-02, 3.64425182e-02, 2.48668194e-02, 2.50070095e-02,\n","       1.66904926e-03, 1.45959854e-03, 9.01186466e-03, 2.89940834e-03,\n","       1.19205713e-02, 8.67605209e-04, 1.26582146e-01, 1.78146362e-02,\n","       1.29665494e-01, 1.34953499e-01, 1.17211580e-01, 2.30749846e-02,\n","       7.83518553e-02, 5.46658039e-03, 3.45023870e-02, 4.39881086e-02,\n","       5.04636765e-03, 6.38604164e-03, 1.29733086e-02, 8.73923302e-04,\n","       6.14916086e-02, 1.28048658e-02, 9.29391384e-03, 1.84106827e-03,\n","       4.35689688e-02, 4.05285358e-02, 2.89850235e-02, 1.67703629e-03,\n","       4.36345339e-02, 2.47138739e-02, 1.59816742e-02, 1.45285130e-02,\n","       3.63945961e-03, 1.74564123e-02, 1.32124424e-02, 4.05597687e-03,\n","       2.25235224e-02, 8.08119774e-03, 8.16917419e-03, 6.53851032e-03,\n","       1.68354511e-02, 2.49904394e-02, 1.70975924e-02, 2.76196003e-03,\n","       4.03693914e-02, 1.49755478e-02, 1.56319141e-03, 6.78753853e-03,\n","       3.03983688e-04, 8.90851021e-03, 1.42576694e-02, 2.44736671e-03,\n","       1.15154982e-02, 2.29779482e-02, 9.28342342e-03, 7.37500191e-03,\n","       3.53069305e-02, 4.07185555e-02, 1.71154737e-02, 4.96196747e-03,\n","       4.57738638e-02, 1.75944567e-02, 4.54151630e-03, 7.83598423e-03,\n","       8.03899765e-03, 9.98115540e-03, 5.03349304e-03, 2.91693211e-03,\n","       3.66556644e-03, 1.33024454e-02, 1.07240677e-03, 5.62548637e-04,\n","       7.90344477e-02, 4.13085222e-02, 6.05469942e-02, 1.74230337e-02,\n","       9.40874815e-02, 2.53226757e-02, 8.14366341e-03, 4.50241566e-03,\n","       6.87897205e-03, 6.08718395e-03, 7.26056099e-03, 3.80802155e-03,\n","       9.36651230e-03, 2.83529758e-02, 1.55519247e-02, 4.20451164e-04,\n","       5.63638210e-02, 3.27523947e-02, 7.42697716e-03, 1.68490410e-03,\n","       5.92714548e-02, 9.45079327e-03, 1.57896280e-02, 3.02267075e-03,\n","       1.18136406e-04, 1.64210796e-03, 1.03358030e-02, 7.18593597e-04,\n","       1.82454586e-01, 8.38570595e-02, 1.20110869e-01, 1.56576037e-01,\n","       1.79363847e-01, 2.63939977e-01, 2.34277487e-01, 3.95018101e-01,\n","       1.75056458e-01, 3.42934608e-01, 2.71431088e-01, 2.82980442e-01]), 'mean_score_time': array([0.00538898, 0.00335753, 0.00293362, 0.00309193, 0.00711751,\n","       0.0052346 , 0.0036006 , 0.00871408, 0.00691736, 0.00733137,\n","       0.00459802, 0.00412595, 0.0082494 , 0.00467288, 0.00426364,\n","       0.01252127, 0.00296342, 0.00305462, 0.00275457, 0.01236904,\n","       0.00416791, 0.00317347, 0.00364494, 0.00334585, 0.00319135,\n","       0.00455391, 0.00418043, 0.00357854, 0.0033865 , 0.0041889 ,\n","       0.00385547, 0.00366759, 0.00406694, 0.00301015, 0.00339293,\n","       0.00314581, 0.00439394, 0.0032171 , 0.00619459, 0.01008737,\n","       0.00343013, 0.00478792, 0.00362611, 0.00701153, 0.00494659,\n","       0.00323904, 0.00372803, 0.00388932, 0.00310886, 0.00298774,\n","       0.00389159, 0.00323248, 0.00387633, 0.00734901, 0.00422144,\n","       0.00432706, 0.01585436, 0.01516807, 0.00980043, 0.01343846,\n","       0.00427258, 0.01440835, 0.00445533, 0.00818312, 0.00401366,\n","       0.00319803, 0.00859487, 0.00326753, 0.00424206, 0.00353897,\n","       0.02463555, 0.01314557, 0.00351012, 0.00930488, 0.00827694,\n","       0.00429559, 0.01037049, 0.00370765, 0.00353348, 0.00439715,\n","       0.00344348, 0.00282598, 0.00434327, 0.00312197, 0.00304353,\n","       0.00338459, 0.00313461, 0.01011252, 0.00324106, 0.0035578 ,\n","       0.00740802, 0.00417256, 0.00339711, 0.00377536, 0.0037775 ,\n","       0.00367248, 0.01064587, 0.00326443, 0.00294638, 0.00292408,\n","       0.00393248, 0.00757587, 0.00332689, 0.00350904, 0.00391388,\n","       0.00349247, 0.00381458, 0.00567353, 0.004161  , 0.00457442,\n","       0.00516903, 0.0055809 , 0.00347054, 0.00405049, 0.00334811,\n","       0.00342035, 0.00320053, 0.00404954, 0.00374091, 0.003474  ,\n","       0.0061239 , 0.00628543, 0.01723695, 0.00381994, 0.00733638,\n","       0.00799441, 0.00436497, 0.00406349, 0.00345302, 0.00308692,\n","       0.00363851, 0.00317752, 0.00437486, 0.00345039, 0.00552511,\n","       0.00388503, 0.00449359, 0.00914514, 0.00676441, 0.00381947,\n","       0.00330567, 0.00354946, 0.00412679, 0.00362802, 0.00342453,\n","       0.00363457, 0.00400841, 0.00381696, 0.00336444, 0.00420809,\n","       0.00335193, 0.00459015, 0.00484586, 0.00536108, 0.00491798,\n","       0.0036726 , 0.02163303, 0.01590896, 0.0125525 , 0.00382245,\n","       0.00334895, 0.00295258, 0.00426829, 0.02100992, 0.00468457,\n","       0.00522971, 0.00511611, 0.00313842, 0.00539982, 0.00379729,\n","       0.00482857, 0.00517642, 0.00507355, 0.00469053, 0.00405884,\n","       0.00389564, 0.01485014, 0.00333738, 0.0028441 , 0.00865507,\n","       0.01550162, 0.00392044, 0.00483429, 0.0031873 , 0.00321054,\n","       0.00338483, 0.0033865 , 0.00380003, 0.01588368, 0.00365281,\n","       0.00560033, 0.00735152, 0.00321531, 0.00330341, 0.00349176,\n","       0.00659561, 0.00490892, 0.0031985 , 0.00458944, 0.010939  ,\n","       0.00325704, 0.00326848, 0.00327611, 0.00344694, 0.00475383,\n","       0.00371397, 0.00408602, 0.00446665, 0.00354505, 0.00942671,\n","       0.00504148, 0.00295556, 0.004403  , 0.00855207, 0.00391102,\n","       0.00466549, 0.003286  , 0.00439632, 0.00860155, 0.00441504,\n","       0.00498247, 0.00628638, 0.00613892, 0.00450003, 0.00348353,\n","       0.00320542, 0.00451195, 0.00528204, 0.00541663, 0.00828147,\n","       0.0087744 , 0.00942802, 0.00315809, 0.00344682, 0.00419092,\n","       0.00335336, 0.0146749 , 0.00418699, 0.0693835 , 0.0049659 ,\n","       0.02354407, 0.00800049, 0.00349152, 0.00918245, 0.0034734 ,\n","       0.037907  , 0.0051105 , 0.00445962, 0.00496554, 0.00622559,\n","       0.01621008, 0.00557399, 0.00374651, 0.00420535, 0.00373769,\n","       0.004318  , 0.00283599, 0.00289249, 0.00526655, 0.01104569,\n","       0.01140201, 0.00408912, 0.00318682, 0.00418949, 0.00346005,\n","       0.00344765, 0.00364065, 0.00346506, 0.00363135, 0.0039984 ,\n","       0.00683033, 0.0049144 , 0.00307512, 0.00451756, 0.00301695,\n","       0.00514054, 0.00393152, 0.00683641, 0.00398409, 0.00332415,\n","       0.00372005, 0.00523603, 0.00453591, 0.00356543, 0.00856209,\n","       0.00369954, 0.0035162 , 0.00377524, 0.00404954, 0.00583947,\n","       0.00331247, 0.00435591, 0.00358987, 0.00606763, 0.00372398,\n","       0.02529144, 0.0045445 , 0.00918341, 0.0053885 , 0.01008654,\n","       0.00468826, 0.00735652, 0.00415003, 0.00377715, 0.00637257,\n","       0.00373447, 0.00377762, 0.0031302 , 0.0042119 , 0.00352108,\n","       0.00349557, 0.00433445, 0.00546801, 0.00399947, 0.00354099,\n","       0.00353134, 0.00806653, 0.00794697, 0.0039221 , 0.00592792,\n","       0.00329792, 0.00472796, 0.00654507, 0.00399649, 0.00292897,\n","       0.00345051, 0.00537395, 0.00414789, 0.00340736, 0.00393248,\n","       0.00811255, 0.00371647, 0.0033567 , 0.00803137, 0.00354993,\n","       0.00389051, 0.00320089, 0.00304329, 0.00364006, 0.00294399,\n","       0.0033524 , 0.0040549 , 0.00453961, 0.00411642, 0.00375092,\n","       0.00660598, 0.00625908, 0.00466466, 0.00387037, 0.00409889,\n","       0.00488687, 0.00503492, 0.02245307, 0.00621879, 0.00308669,\n","       0.00282693, 0.00344944, 0.00877655, 0.00404596, 0.00339198,\n","       0.00559461, 0.004197  , 0.00853932, 0.0046488 , 0.00381005,\n","       0.00447154, 0.00382996, 0.00374901, 0.00323606, 0.00301898,\n","       0.00318694, 0.0030005 , 0.00314355, 0.01089299, 0.01189256,\n","       0.00517499, 0.00819337, 0.00972748, 0.0034616 , 0.00325942,\n","       0.00430262, 0.00382447, 0.006266  , 0.00617409, 0.00318527,\n","       0.00303996, 0.00413752, 0.00395083, 0.00315881, 0.00368512,\n","       0.00993764, 0.00943375, 0.00352812, 0.00350738, 0.00374889,\n","       0.00378299, 0.00461507, 0.00680339, 0.00366712, 0.0037955 ,\n","       0.00296259, 0.00288701, 0.00323105, 0.0024451 , 0.00367951,\n","       0.00325692, 0.0030911 , 0.00345147, 0.00319588, 0.00382161,\n","       0.00327146, 0.00315511, 0.00555837, 0.00366688, 0.00368893,\n","       0.00373149, 0.00307655, 0.00425446, 0.00346243, 0.0064106 ,\n","       0.00302494, 0.00401354, 0.00389767, 0.00477743, 0.00326455,\n","       0.00538516, 0.00362921, 0.00339055, 0.00590134, 0.0044682 ,\n","       0.00381351, 0.00422132, 0.00337696, 0.00305343, 0.00358546,\n","       0.00401735, 0.00343251, 0.00368392, 0.00447953, 0.00754738,\n","       0.00366056, 0.0039227 , 0.00452459, 0.00353348, 0.00651002,\n","       0.0042156 , 0.00382161, 0.00593698, 0.00296092, 0.00382686,\n","       0.00438201, 0.00336897, 0.00321364, 0.00316203, 0.00373983,\n","       0.0033499 , 0.00567651, 0.00450146, 0.0027734 , 0.00330353,\n","       0.00379789, 0.00388253, 0.00447464, 0.00425756, 0.0028795 ,\n","       0.00293946, 0.00328195, 0.00318897, 0.00316155, 0.00309765,\n","       0.00448108, 0.00419295, 0.00332737, 0.00357521, 0.00432241,\n","       0.00426126, 0.0039072 , 0.00359297, 0.00406754, 0.0035845 ,\n","       0.00302541, 0.00408697, 0.0029335 , 0.00316834, 0.00351691,\n","       0.00393414, 0.00426984, 0.0041548 , 0.00427556, 0.00351167,\n","       0.00513208, 0.00575256, 0.00399029, 0.00418067, 0.00558305,\n","       0.00446713, 0.00301015, 0.00355804, 0.00315642, 0.00342512,\n","       0.00344145, 0.00351608, 0.00631058, 0.00418556, 0.0035429 ,\n","       0.0101701 , 0.00753546, 0.00434911, 0.0058279 , 0.00913   ,\n","       0.00483096, 0.00707698, 0.00302541, 0.00320792, 0.00289357,\n","       0.00343013, 0.0073781 , 0.00804496, 0.00859153, 0.00718558,\n","       0.00589228, 0.00756598, 0.00325656, 0.003456  , 0.01461542,\n","       0.00358605, 0.00436115, 0.00780892, 0.00318408, 0.0035156 ,\n","       0.00593781, 0.00380945, 0.00361764, 0.00849855, 0.00775397,\n","       0.00331485, 0.0046885 , 0.00388956, 0.00499439, 0.00413764,\n","       0.00597858, 0.00526786, 0.00415206, 0.0060935 , 0.00293243,\n","       0.00326884, 0.0030278 , 0.00302196, 0.00325716, 0.0112735 ,\n","       0.00369   , 0.00369954, 0.00354803, 0.00368392, 0.00448382,\n","       0.00412285, 0.00465   , 0.00409138, 0.00581253, 0.00479555,\n","       0.00288749, 0.00340056, 0.00395   , 0.00350583, 0.00316656,\n","       0.00311935, 0.00525653, 0.00377798, 0.00548542, 0.00391889,\n","       0.00449193, 0.00619793, 0.00391591, 0.0037148 , 0.00499094,\n","       0.00920808, 0.00309205, 0.00336254, 0.00365961, 0.00423491,\n","       0.00349164, 0.00327194, 0.00578058, 0.00329709, 0.00332487,\n","       0.00329304, 0.00326633, 0.00327206, 0.00411963, 0.00448394,\n","       0.0050211 , 0.00369346, 0.0046643 , 0.00557756, 0.00459242,\n","       0.00253749, 0.00433218, 0.00446892, 0.00278306, 0.0038985 ,\n","       0.00687897, 0.00373554, 0.00351763, 0.00338781, 0.00349581,\n","       0.00383651, 0.00391304, 0.00368035, 0.00463283, 0.00292492,\n","       0.00483847, 0.00340402, 0.00336289, 0.00514245, 0.00334561,\n","       0.00306118, 0.00461102, 0.00473022, 0.00332737, 0.00355721,\n","       0.00314057, 0.00334144, 0.00338268, 0.003703  , 0.00591409,\n","       0.00269449, 0.00328457, 0.00293255, 0.00369036, 0.00812423,\n","       0.00497055, 0.00347698, 0.00380194, 0.00446951, 0.00401664,\n","       0.00457442, 0.00510395, 0.00393081, 0.00380456, 0.00328922,\n","       0.00258899, 0.00301397, 0.00307   , 0.00368857, 0.00489557,\n","       0.00478399, 0.00321102, 0.00404406, 0.00437307, 0.00377393,\n","       0.00387609, 0.00353897, 0.00359142, 0.00410247, 0.00410795,\n","       0.00455391, 0.00297678, 0.00387239, 0.00393593, 0.00312853,\n","       0.00323594, 0.00625014, 0.00310206, 0.00764203, 0.00337839,\n","       0.00324786, 0.00372958, 0.01038551, 0.00349355, 0.00603795,\n","       0.0060854 , 0.00372446, 0.0034833 , 0.00769591, 0.00853896,\n","       0.00311291, 0.00403154, 0.01347733, 0.00589502, 0.01764202,\n","       0.00445104, 0.00651491, 0.00377548, 0.00327349, 0.0082165 ,\n","       0.00570393, 0.0075773 , 0.0152086 , 0.004058  , 0.00440931,\n","       0.00306535, 0.00342536, 0.00392962, 0.00355363, 0.00320089,\n","       0.00447714, 0.00918841, 0.00434446, 0.00647902, 0.00338995,\n","       0.00810122, 0.00420451, 0.00440502, 0.00368154, 0.00302839,\n","       0.00847006, 0.00385046, 0.00329542, 0.00508642, 0.00301504,\n","       0.00379944, 0.00331509, 0.00323701, 0.00368893, 0.00338066,\n","       0.00568616, 0.00361836, 0.00392115, 0.00434542, 0.00517607,\n","       0.00340402, 0.00377595, 0.00402153, 0.0031929 , 0.00320661,\n","       0.01043928, 0.00393713, 0.00374699, 0.00460315, 0.00459552,\n","       0.00975907, 0.00745809, 0.00468659, 0.00426245, 0.00432742,\n","       0.00540352, 0.00300527, 0.00401103, 0.00529993, 0.00310791,\n","       0.00318301, 0.00429392, 0.00344372, 0.00477159, 0.00338197,\n","       0.00348413, 0.00461507, 0.00656259, 0.00409412, 0.00503099,\n","       0.00403249, 0.00379455, 0.00264549, 0.00310004, 0.00402248,\n","       0.00293529, 0.00373697, 0.05134773, 0.00329041, 0.03069651,\n","       0.05003357, 0.00769842, 0.01923347, 0.00397444, 0.00537503,\n","       0.00381696, 0.00401855, 0.01842606]), 'std_score_time': array([2.19893456e-03, 2.82406807e-04, 2.94446945e-05, 3.25083733e-04,\n","       3.99446487e-03, 1.98483467e-04, 4.59432602e-04, 5.55908680e-03,\n","       3.55565548e-03, 4.20355797e-03, 1.35910511e-03, 3.88026237e-04,\n","       4.28450108e-03, 1.20675564e-03, 5.46455383e-04, 7.32135773e-03,\n","       2.42829323e-04, 1.57594681e-04, 3.03983688e-05, 9.03999805e-03,\n","       1.00409985e-03, 4.23192978e-05, 1.94072723e-04, 3.10063362e-04,\n","       6.94990158e-05, 1.09493732e-03, 1.14059448e-03, 7.25984573e-05,\n","       7.34329224e-05, 1.03795528e-03, 2.55584717e-04, 1.83343887e-04,\n","       1.10816956e-03, 2.76923180e-04, 3.44038010e-04, 3.81112099e-04,\n","       6.15239143e-04, 4.34756279e-04, 3.10134888e-03, 6.92141056e-03,\n","       4.06026840e-04, 1.84392929e-03, 4.27007675e-04, 3.82959843e-03,\n","       1.57034397e-03, 1.71780586e-04, 8.59498978e-05, 7.34329224e-05,\n","       1.30057335e-04, 2.18153000e-05, 7.75694847e-04, 1.11341476e-04,\n","       1.94430351e-04, 3.90911102e-03, 5.41448593e-04, 9.16004181e-04,\n","       9.18960571e-03, 9.36090946e-03, 5.75947762e-03, 9.99641418e-03,\n","       2.61425972e-04, 1.11341476e-04, 7.35521317e-04, 4.25708294e-03,\n","       9.27567482e-04, 3.61204147e-05, 5.52189350e-03, 2.64644623e-05,\n","       7.28964806e-04, 5.78045845e-04, 3.94153595e-03, 9.51254368e-03,\n","       2.30073929e-05, 6.00278378e-03, 4.47201729e-03, 1.02949142e-03,\n","       5.09548187e-03, 2.33650208e-05, 1.52468681e-04, 6.80923462e-04,\n","       5.67436218e-04, 1.80006027e-04, 3.11851501e-04, 1.98960304e-04,\n","       3.59654427e-04, 3.54528427e-04, 5.60283661e-06, 7.00163841e-03,\n","       2.19345093e-05, 5.84959984e-04, 4.02200222e-03, 9.77516174e-04,\n","       1.17182732e-04, 2.32458115e-04, 3.83615494e-04, 7.95125961e-05,\n","       7.93099403e-03, 1.55448914e-04, 6.27040863e-05, 3.11136246e-05,\n","       3.95536423e-04, 4.28092480e-03, 3.31401825e-05, 6.05106354e-04,\n","       3.06129456e-04, 2.61425972e-04, 2.16603279e-04, 1.94942951e-03,\n","       1.67012215e-04, 2.44379044e-05, 9.88125801e-04, 1.89709663e-03,\n","       4.51683998e-04, 1.09457970e-03, 4.21047211e-04, 1.33275986e-04,\n","       3.64780426e-05, 4.38451767e-04, 4.81963158e-04, 2.58922577e-04,\n","       1.75297260e-03, 9.45568085e-04, 1.35569572e-02, 4.16994095e-04,\n","       2.89249420e-03, 3.72958183e-03, 5.32150269e-04, 5.10334969e-04,\n","       6.17027283e-04, 1.25050545e-04, 3.63588333e-04, 2.34842300e-05,\n","       1.22106075e-03, 7.24792480e-05, 2.38609314e-03, 6.81161880e-04,\n","       9.53555107e-04, 5.49709797e-03, 3.44061852e-03, 6.15119934e-05,\n","       1.35421753e-04, 1.22427940e-04, 2.00986862e-04, 7.86781311e-06,\n","       3.05533409e-04, 1.27673149e-04, 1.02150440e-03, 1.15621090e-03,\n","       8.45193863e-05, 7.97033310e-04, 3.29017639e-05, 9.80019569e-04,\n","       6.84976578e-04, 1.03282928e-03, 5.22971153e-04, 2.69651413e-04,\n","       1.75400972e-02, 5.55205345e-03, 7.75241852e-03, 2.49505043e-04,\n","       2.44021416e-04, 2.67028809e-05, 1.34003162e-03, 2.08306313e-03,\n","       2.35676765e-04, 1.47533417e-03, 1.50787830e-03, 1.69396400e-04,\n","       2.49981880e-04, 3.64780426e-05, 1.30856037e-03, 1.28853321e-03,\n","       1.52635574e-03, 6.68406487e-04, 1.08957291e-04, 2.54511833e-04,\n","       1.01320744e-02, 3.55243683e-05, 5.10215759e-05, 5.39088249e-03,\n","       1.22884512e-02, 6.09278679e-04, 1.41942501e-03, 2.26497650e-06,\n","       8.55922699e-05, 1.39951706e-04, 5.84125519e-05, 5.05805016e-04,\n","       1.25405788e-02, 9.89437103e-05, 1.80065632e-03, 3.20041180e-03,\n","       4.36306000e-05, 7.85589218e-05, 5.42998314e-04, 1.42240524e-03,\n","       1.76393986e-03, 4.85181808e-05, 8.42452049e-04, 7.81095028e-03,\n","       4.48226929e-05, 8.05854797e-05, 2.78949738e-05, 1.44124031e-04,\n","       1.27387047e-03, 2.57134438e-04, 6.04867935e-04, 1.03843212e-03,\n","       4.30107117e-04, 6.50942326e-03, 2.08032131e-03, 5.63859940e-05,\n","       6.92963600e-04, 4.29391861e-03, 6.03914261e-04, 4.04477119e-04,\n","       7.09295273e-05, 9.75489616e-04, 5.20050526e-03, 8.54015350e-04,\n","       8.32557678e-04, 2.68745422e-03, 2.55191326e-03, 5.29885292e-04,\n","       5.57422638e-04, 1.29580498e-04, 9.62853432e-04, 2.27510929e-03,\n","       6.49452209e-04, 5.06734848e-03, 2.74050236e-03, 5.12909889e-03,\n","       3.94105911e-04, 1.74999237e-04, 9.29117203e-04, 2.18391418e-04,\n","       1.11439228e-02, 8.67009163e-04, 6.51284456e-02, 3.49283218e-05,\n","       3.16786766e-03, 4.38654423e-03, 5.91635704e-04, 6.20174408e-03,\n","       1.38640404e-04, 3.43219042e-02, 3.22580338e-04, 1.50561333e-03,\n","       1.51157379e-03, 1.95121765e-03, 9.04798508e-03, 2.50816345e-03,\n","       5.48362732e-06, 3.96370888e-04, 6.43730164e-06, 4.27007675e-04,\n","       1.29938126e-04, 1.34706497e-04, 2.61652470e-03, 6.96229935e-03,\n","       1.43206120e-03, 1.86610222e-03, 2.37822533e-04, 3.57627869e-06,\n","       6.99758530e-05, 1.08599663e-04, 2.43425369e-04, 1.55091286e-04,\n","       4.64916229e-05, 3.47256660e-04, 3.20732594e-03, 1.39653683e-03,\n","       2.16007233e-04, 2.97546387e-04, 2.77996063e-04, 9.83476639e-04,\n","       5.75542450e-04, 1.40738487e-03, 8.77022743e-04, 1.62005424e-04,\n","       3.30209732e-04, 1.92081928e-03, 9.81807709e-04, 3.51309776e-04,\n","       5.07092476e-03, 1.40428543e-04, 1.00135803e-05, 2.63571739e-04,\n","       1.07669830e-03, 2.48944759e-03, 1.08361244e-04, 1.42884254e-03,\n","       3.08990479e-04, 4.01616096e-04, 3.14116478e-04, 1.98342800e-02,\n","       1.02543831e-03, 1.30558014e-03, 1.92642212e-03, 6.61730766e-03,\n","       1.80482864e-04, 2.00164318e-03, 1.08838081e-04, 2.40206718e-04,\n","       2.99537182e-03, 6.26444817e-04, 6.21676445e-04, 2.91109085e-04,\n","       8.36133957e-04, 5.49554825e-05, 1.34706497e-05, 9.06705856e-04,\n","       2.09701061e-03, 5.08308411e-04, 7.67707825e-05, 1.92523003e-04,\n","       3.88467312e-03, 3.95584106e-03, 1.70469284e-05, 6.80088997e-04,\n","       2.20894814e-04, 2.08508968e-03, 3.86905670e-03, 1.30355358e-03,\n","       1.38282776e-05, 5.92589378e-04, 2.25591660e-03, 6.68883324e-04,\n","       3.54051590e-05, 3.33786011e-06, 2.80654430e-03, 3.71694565e-04,\n","       8.05854797e-05, 2.32458115e-04, 5.80549240e-05, 6.55651093e-05,\n","       1.35064125e-04, 3.34978104e-05, 6.82950020e-04, 4.00543213e-05,\n","       2.33650208e-05, 2.81929970e-04, 6.77466393e-04, 8.68439674e-04,\n","       1.49130821e-04, 2.94196606e-03, 2.72405148e-03, 8.57353210e-04,\n","       1.78456306e-04, 1.85966492e-04, 8.07046890e-04, 8.28981400e-04,\n","       1.90591812e-02, 2.83396244e-03, 2.27570534e-04, 1.35898590e-04,\n","       1.33514404e-04, 5.48446178e-03, 8.77141953e-04, 2.46047974e-04,\n","       1.17552280e-03, 5.68270683e-04, 4.73248959e-03, 3.01957130e-04,\n","       2.23040581e-04, 1.96456909e-04, 5.81741333e-05, 4.99486923e-05,\n","       3.13997269e-04, 1.16944313e-04, 3.04937363e-04, 5.65052032e-05,\n","       6.84261322e-05, 3.28600407e-03, 6.84332848e-03, 7.61151314e-04,\n","       3.59570980e-03, 6.10351562e-03, 1.74522400e-04, 2.42471695e-04,\n","       9.40680504e-04, 3.07559967e-04, 1.83308125e-03, 2.40206718e-03,\n","       6.19888306e-05, 1.68919563e-04, 1.20043755e-03, 9.93013382e-04,\n","       2.28881836e-05, 4.20212746e-04, 4.38249111e-03, 4.78196144e-03,\n","       2.20060349e-04, 5.24520874e-05, 4.55141068e-04, 4.85897064e-04,\n","       8.79049301e-04, 3.28242779e-03, 2.78949738e-05, 1.84416771e-04,\n","       1.11341476e-04, 2.50339508e-05, 2.71797180e-04, 6.26921654e-04,\n","       6.03675842e-04, 6.90221786e-05, 3.38554382e-05, 3.94701958e-04,\n","       2.99215317e-05, 6.11543655e-04, 7.95125961e-05, 1.91926956e-05,\n","       7.63535500e-04, 3.22103500e-04, 1.67965889e-04, 1.49488449e-04,\n","       1.55448914e-04, 7.54475594e-04, 1.74403191e-04, 1.31964684e-03,\n","       2.70605087e-05, 7.91549683e-04, 6.71386719e-04, 6.34193420e-05,\n","       1.78456306e-04, 2.24494934e-03, 2.25067139e-04, 2.45571136e-05,\n","       2.41756439e-03, 7.87019730e-04, 1.27553940e-04, 9.27567482e-04,\n","       5.00679016e-06, 4.55379486e-05, 2.84552574e-04, 1.05547905e-03,\n","       1.79529190e-04, 2.11119652e-04, 8.94427299e-04, 3.30853462e-03,\n","       2.65479088e-04, 9.15527344e-05, 9.15408134e-04, 1.32441521e-04,\n","       2.59900093e-03, 4.35113907e-05, 1.56402588e-04, 2.30801105e-03,\n","       5.38825989e-05, 8.53061676e-04, 1.48880482e-03, 4.89950180e-05,\n","       1.14440918e-05, 1.29938126e-05, 3.82900238e-04, 4.99486923e-05,\n","       5.40494919e-04, 1.08850002e-03, 5.99265099e-04, 7.84397125e-05,\n","       1.10864639e-05, 1.92523003e-04, 4.31537628e-04, 1.84416771e-04,\n","       5.06639481e-05, 1.67369843e-04, 7.71284103e-05, 2.37107277e-04,\n","       1.10507011e-04, 2.15768814e-05, 1.33585930e-03, 1.68919563e-04,\n","       3.83853912e-05, 3.61204147e-05, 5.47528267e-04, 9.54627991e-04,\n","       7.11917877e-04, 1.11818314e-04, 5.13434410e-04, 1.54972076e-06,\n","       5.35249710e-05, 1.05094910e-03, 4.76837158e-07, 2.76565552e-05,\n","       2.22921371e-04, 6.57081604e-04, 9.93967056e-04, 7.65919685e-04,\n","       4.02450562e-04, 1.20639801e-04, 1.13093853e-03, 1.95932388e-03,\n","       5.75780869e-05, 2.95639038e-05, 1.01304054e-03, 2.19941139e-04,\n","       1.34825706e-04, 3.17692757e-04, 1.63555145e-04, 5.18798828e-04,\n","       1.22427940e-04, 8.78572464e-05, 2.98249722e-03, 1.05535984e-03,\n","       1.32083893e-04, 4.67717648e-03, 3.83353233e-03, 9.96947289e-04,\n","       2.03704834e-03, 4.28080559e-03, 1.10304356e-03, 3.18288803e-03,\n","       3.06367874e-05, 1.90734863e-06, 7.23600388e-05, 4.94003296e-04,\n","       1.54972076e-04, 4.56976891e-03, 4.20844555e-03, 3.97932529e-03,\n","       2.64644623e-03, 9.20295715e-04, 4.74452972e-05, 1.67965889e-04,\n","       1.11764669e-02, 1.35898590e-04, 3.28063965e-04, 3.84807587e-03,\n","       2.73942947e-04, 4.81486320e-04, 2.34103203e-03, 6.56604767e-04,\n","       1.83463097e-04, 4.11736965e-03, 4.22298908e-03, 3.51667404e-05,\n","       6.25610352e-04, 1.41382217e-04, 1.43766403e-03, 2.28524208e-04,\n","       2.30550766e-04, 7.67946243e-04, 2.60114670e-04, 1.89352036e-03,\n","       5.44786453e-05, 1.95145607e-04, 6.99758530e-05, 1.81913376e-04,\n","       8.11815262e-05, 7.79235363e-03, 6.81877136e-05, 5.64575195e-04,\n","       3.41057777e-04, 1.56044960e-04, 9.39011574e-04, 6.25014305e-04,\n","       6.95109367e-04, 1.14560127e-04, 1.88243389e-03, 9.66548920e-04,\n","       5.05447388e-05, 5.08546829e-04, 6.31928444e-04, 4.46915627e-04,\n","       1.18613243e-04, 1.04546547e-04, 9.09686089e-04, 4.39882278e-04,\n","       3.63707542e-04, 1.07049942e-04, 4.99129295e-04, 2.85792351e-03,\n","       7.03334808e-06, 8.51154327e-05, 1.15811825e-03, 2.00903416e-03,\n","       2.55107880e-04, 1.87277794e-04, 7.79509544e-04, 1.36697292e-03,\n","       3.94582748e-04, 2.60233879e-04, 2.51066685e-03, 1.85012817e-04,\n","       1.26004219e-04, 1.59740448e-05, 4.95910645e-05, 5.00679016e-06,\n","       6.98566437e-04, 1.07002258e-03, 1.22714043e-03, 5.94854355e-05,\n","       1.63853168e-03, 2.56443024e-03, 1.31654739e-03, 6.57558441e-04,\n","       1.25515461e-03, 1.41215324e-03, 3.35931778e-04, 2.51412392e-04,\n","       3.58498096e-03, 4.68254089e-04, 3.23534012e-04, 7.40289688e-05,\n","       2.05874443e-04, 1.61528587e-04, 5.40852547e-04, 2.70485878e-04,\n","       1.05392933e-03, 3.60012054e-05, 1.91760063e-03, 3.83973122e-04,\n","       3.95774841e-04, 1.68037415e-03, 1.96337700e-04, 1.60932541e-05,\n","       5.14984131e-04, 1.22594833e-03, 1.63555145e-04, 3.15904617e-04,\n","       9.41753387e-06, 1.10387802e-04, 6.24656677e-05, 2.08973885e-04,\n","       9.90986824e-04, 4.10437584e-04, 5.81383705e-04, 1.04904175e-05,\n","       4.94718552e-05, 4.53197956e-03, 3.20672989e-04, 3.16262245e-04,\n","       3.37004662e-04, 9.18507576e-04, 6.09397888e-04, 1.19960308e-03,\n","       1.28185749e-03, 2.84910202e-04, 5.85317612e-05, 3.19957733e-04,\n","       4.08887863e-04, 1.25050545e-04, 7.18832016e-05, 6.01530075e-04,\n","       1.80828571e-03, 3.23891640e-04, 4.29153442e-05, 8.95023346e-04,\n","       1.12700462e-03, 3.49998474e-04, 3.98278236e-04, 1.89542770e-05,\n","       1.15513802e-04, 2.38418579e-06, 4.99010086e-04, 8.43882561e-04,\n","       1.31011009e-04, 9.42468643e-04, 8.05735588e-04, 1.01566315e-04,\n","       1.14083290e-04, 1.42192841e-03, 7.60555267e-05, 4.45795059e-03,\n","       2.77519226e-04, 5.09023666e-05, 3.89575958e-04, 4.90856171e-03,\n","       7.65323639e-05, 1.47509575e-03, 2.54964828e-03, 6.35385513e-05,\n","       5.10454178e-04, 4.35709953e-03, 2.89607048e-03, 1.82032585e-04,\n","       6.18577003e-04, 9.52553749e-03, 2.79390812e-03, 1.53064728e-04,\n","       7.56025314e-04, 3.30090523e-04, 2.78353691e-04, 2.25543976e-04,\n","       3.98361683e-03, 5.09023666e-04, 3.54754925e-03, 5.60343266e-03,\n","       1.02162361e-04, 1.88589096e-04, 1.29461288e-04, 4.55617905e-04,\n","       8.36610794e-04, 3.02553177e-04, 5.30481339e-05, 1.68919563e-04,\n","       4.93240356e-03, 9.28401947e-04, 3.06892395e-03, 1.85847282e-04,\n","       3.96013260e-03, 4.18424606e-04, 5.07831573e-05, 1.16467476e-04,\n","       1.76429749e-04, 4.99987602e-03, 2.67505646e-04, 1.44481659e-04,\n","       1.57952309e-03, 1.74045563e-04, 4.04596329e-04, 3.72052193e-04,\n","       7.41481781e-05, 1.66058540e-04, 5.35249710e-05, 1.32715702e-03,\n","       5.63859940e-05, 2.66194344e-04, 6.31570816e-04, 9.58919525e-04,\n","       2.40921974e-04, 4.28080559e-04, 1.01840496e-03, 2.80857086e-04,\n","       3.94582748e-05, 4.59444523e-03, 4.28080559e-04, 4.61101532e-04,\n","       1.09195709e-03, 1.01947784e-03, 6.17206097e-03, 2.84898281e-03,\n","       6.13451004e-04, 2.41518021e-04, 1.57475471e-04, 1.71065331e-03,\n","       1.64985657e-04, 9.63807106e-04, 1.10590458e-03, 8.49962234e-05,\n","       4.20808792e-05, 9.89913940e-04, 1.72376633e-04, 4.32610512e-04,\n","       6.81877136e-05, 5.80549240e-05, 1.00994110e-03, 8.14557076e-04,\n","       7.00950623e-05, 8.54849815e-04, 2.27570534e-04, 2.04443932e-04,\n","       6.70671463e-04, 2.18987465e-04, 6.84380531e-04, 3.15904617e-05,\n","       5.64098358e-04, 4.82244492e-02, 1.23500824e-04, 2.74714231e-02,\n","       4.61556911e-02, 4.09066677e-03, 1.14555359e-02, 2.93493271e-04,\n","       2.05528736e-03, 2.54034996e-04, 2.38418579e-04, 7.55870342e-03]), 'param_learning_rate': masked_array(data=[0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04],\n","             mask=[False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False],\n","       fill_value='?',\n","            dtype=object), 'param_max_depth': masked_array(data=[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","                   4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","                   4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6,\n","                   6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n","                   6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n","                   6, 6, 6, 6, 6, 6, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n","                   8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n","                   8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n","                   10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n","                   10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n","                   10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n","                   10, 10, 10, 10, 10, 10, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","                   4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","                   4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","                   4, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n","                   6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n","                   6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 8, 8, 8, 8,\n","                   8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n","                   8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n","                   8, 8, 8, 8, 8, 8, 8, 8, 10, 10, 10, 10, 10, 10, 10, 10,\n","                   10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n","                   10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n","                   10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 4, 4,\n","                   4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","                   4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","                   4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6, 6, 6,\n","                   6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n","                   6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n","                   6, 6, 6, 6, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n","                   8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n","                   8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 10, 10,\n","                   10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n","                   10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n","                   10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n","                   10, 10, 10, 10, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","                   4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","                   4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 6,\n","                   6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n","                   6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n","                   6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 8, 8, 8, 8, 8, 8, 8,\n","                   8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n","                   8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n","                   8, 8, 8, 8, 8, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n","                   10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n","                   10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n","                   10, 10, 10, 10, 10, 10, 10, 10, 10, 10],\n","             mask=[False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False],\n","       fill_value='?',\n","            dtype=object), 'param_max_features': masked_array(data=[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5,\n","                   5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 7, 7, 7, 7,\n","                   7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3,\n","                   3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5,\n","                   5, 5, 5, 5, 5, 5, 5, 5, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n","                   7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n","                   3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n","                   5, 5, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n","                   3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5,\n","                   5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 7, 7, 7, 7,\n","                   7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3,\n","                   3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5,\n","                   5, 5, 5, 5, 5, 5, 5, 5, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n","                   7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n","                   3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n","                   5, 5, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n","                   3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5,\n","                   5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 7, 7, 7, 7,\n","                   7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3,\n","                   3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5,\n","                   5, 5, 5, 5, 5, 5, 5, 5, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n","                   7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n","                   3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n","                   5, 5, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n","                   3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5,\n","                   5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 7, 7, 7, 7,\n","                   7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3,\n","                   3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5,\n","                   5, 5, 5, 5, 5, 5, 5, 5, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n","                   7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n","                   3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n","                   5, 5, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n","                   3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5,\n","                   5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 7, 7, 7, 7,\n","                   7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3,\n","                   3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5,\n","                   5, 5, 5, 5, 5, 5, 5, 5, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n","                   7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n","                   3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n","                   5, 5, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n","                   3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5,\n","                   5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 7, 7, 7, 7,\n","                   7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7],\n","             mask=[False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False],\n","       fill_value='?',\n","            dtype=object), 'param_n_estimators': masked_array(data=[20, 20, 20, 20, 50, 50, 50, 50, 70, 70, 70, 70, 100,\n","                   100, 100, 100, 20, 20, 20, 20, 50, 50, 50, 50, 70, 70,\n","                   70, 70, 100, 100, 100, 100, 20, 20, 20, 20, 50, 50, 50,\n","                   50, 70, 70, 70, 70, 100, 100, 100, 100, 20, 20, 20, 20,\n","                   50, 50, 50, 50, 70, 70, 70, 70, 100, 100, 100, 100, 20,\n","                   20, 20, 20, 50, 50, 50, 50, 70, 70, 70, 70, 100, 100,\n","                   100, 100, 20, 20, 20, 20, 50, 50, 50, 50, 70, 70, 70,\n","                   70, 100, 100, 100, 100, 20, 20, 20, 20, 50, 50, 50, 50,\n","                   70, 70, 70, 70, 100, 100, 100, 100, 20, 20, 20, 20, 50,\n","                   50, 50, 50, 70, 70, 70, 70, 100, 100, 100, 100, 20, 20,\n","                   20, 20, 50, 50, 50, 50, 70, 70, 70, 70, 100, 100, 100,\n","                   100, 20, 20, 20, 20, 50, 50, 50, 50, 70, 70, 70, 70,\n","                   100, 100, 100, 100, 20, 20, 20, 20, 50, 50, 50, 50, 70,\n","                   70, 70, 70, 100, 100, 100, 100, 20, 20, 20, 20, 50, 50,\n","                   50, 50, 70, 70, 70, 70, 100, 100, 100, 100, 20, 20, 20,\n","                   20, 50, 50, 50, 50, 70, 70, 70, 70, 100, 100, 100, 100,\n","                   20, 20, 20, 20, 50, 50, 50, 50, 70, 70, 70, 70, 100,\n","                   100, 100, 100, 20, 20, 20, 20, 50, 50, 50, 50, 70, 70,\n","                   70, 70, 100, 100, 100, 100, 20, 20, 20, 20, 50, 50, 50,\n","                   50, 70, 70, 70, 70, 100, 100, 100, 100, 20, 20, 20, 20,\n","                   50, 50, 50, 50, 70, 70, 70, 70, 100, 100, 100, 100, 20,\n","                   20, 20, 20, 50, 50, 50, 50, 70, 70, 70, 70, 100, 100,\n","                   100, 100, 20, 20, 20, 20, 50, 50, 50, 50, 70, 70, 70,\n","                   70, 100, 100, 100, 100, 20, 20, 20, 20, 50, 50, 50, 50,\n","                   70, 70, 70, 70, 100, 100, 100, 100, 20, 20, 20, 20, 50,\n","                   50, 50, 50, 70, 70, 70, 70, 100, 100, 100, 100, 20, 20,\n","                   20, 20, 50, 50, 50, 50, 70, 70, 70, 70, 100, 100, 100,\n","                   100, 20, 20, 20, 20, 50, 50, 50, 50, 70, 70, 70, 70,\n","                   100, 100, 100, 100, 20, 20, 20, 20, 50, 50, 50, 50, 70,\n","                   70, 70, 70, 100, 100, 100, 100, 20, 20, 20, 20, 50, 50,\n","                   50, 50, 70, 70, 70, 70, 100, 100, 100, 100, 20, 20, 20,\n","                   20, 50, 50, 50, 50, 70, 70, 70, 70, 100, 100, 100, 100,\n","                   20, 20, 20, 20, 50, 50, 50, 50, 70, 70, 70, 70, 100,\n","                   100, 100, 100, 20, 20, 20, 20, 50, 50, 50, 50, 70, 70,\n","                   70, 70, 100, 100, 100, 100, 20, 20, 20, 20, 50, 50, 50,\n","                   50, 70, 70, 70, 70, 100, 100, 100, 100, 20, 20, 20, 20,\n","                   50, 50, 50, 50, 70, 70, 70, 70, 100, 100, 100, 100, 20,\n","                   20, 20, 20, 50, 50, 50, 50, 70, 70, 70, 70, 100, 100,\n","                   100, 100, 20, 20, 20, 20, 50, 50, 50, 50, 70, 70, 70,\n","                   70, 100, 100, 100, 100, 20, 20, 20, 20, 50, 50, 50, 50,\n","                   70, 70, 70, 70, 100, 100, 100, 100, 20, 20, 20, 20, 50,\n","                   50, 50, 50, 70, 70, 70, 70, 100, 100, 100, 100, 20, 20,\n","                   20, 20, 50, 50, 50, 50, 70, 70, 70, 70, 100, 100, 100,\n","                   100, 20, 20, 20, 20, 50, 50, 50, 50, 70, 70, 70, 70,\n","                   100, 100, 100, 100, 20, 20, 20, 20, 50, 50, 50, 50, 70,\n","                   70, 70, 70, 100, 100, 100, 100, 20, 20, 20, 20, 50, 50,\n","                   50, 50, 70, 70, 70, 70, 100, 100, 100, 100, 20, 20, 20,\n","                   20, 50, 50, 50, 50, 70, 70, 70, 70, 100, 100, 100, 100,\n","                   20, 20, 20, 20, 50, 50, 50, 50, 70, 70, 70, 70, 100,\n","                   100, 100, 100, 20, 20, 20, 20, 50, 50, 50, 50, 70, 70,\n","                   70, 70, 100, 100, 100, 100, 20, 20, 20, 20, 50, 50, 50,\n","                   50, 70, 70, 70, 70, 100, 100, 100, 100, 20, 20, 20, 20,\n","                   50, 50, 50, 50, 70, 70, 70, 70, 100, 100, 100, 100, 20,\n","                   20, 20, 20, 50, 50, 50, 50, 70, 70, 70, 70, 100, 100,\n","                   100, 100, 20, 20, 20, 20, 50, 50, 50, 50, 70, 70, 70,\n","                   70, 100, 100, 100, 100, 20, 20, 20, 20, 50, 50, 50, 50,\n","                   70, 70, 70, 70, 100, 100, 100, 100, 20, 20, 20, 20, 50,\n","                   50, 50, 50, 70, 70, 70, 70, 100, 100, 100, 100, 20, 20,\n","                   20, 20, 50, 50, 50, 50, 70, 70, 70, 70, 100, 100, 100,\n","                   100],\n","             mask=[False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False],\n","       fill_value='?',\n","            dtype=object), 'param_subsample': masked_array(data=[0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5,\n","                   0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7,\n","                   0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9,\n","                   0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2,\n","                   0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5,\n","                   0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7,\n","                   0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9,\n","                   0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2,\n","                   0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5,\n","                   0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7,\n","                   0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9,\n","                   0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2,\n","                   0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5,\n","                   0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7,\n","                   0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9,\n","                   0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2,\n","                   0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5,\n","                   0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7,\n","                   0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9,\n","                   0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2,\n","                   0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5,\n","                   0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7,\n","                   0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9,\n","                   0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2,\n","                   0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5,\n","                   0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7,\n","                   0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9,\n","                   0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2,\n","                   0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5,\n","                   0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7,\n","                   0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9,\n","                   0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2,\n","                   0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5,\n","                   0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7,\n","                   0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9,\n","                   0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2,\n","                   0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5,\n","                   0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7,\n","                   0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9,\n","                   0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2,\n","                   0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5,\n","                   0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7,\n","                   0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9,\n","                   0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2,\n","                   0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5,\n","                   0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7,\n","                   0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9,\n","                   0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2,\n","                   0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5,\n","                   0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7,\n","                   0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9,\n","                   0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2,\n","                   0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5,\n","                   0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7,\n","                   0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9,\n","                   0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2,\n","                   0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5,\n","                   0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7,\n","                   0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9,\n","                   0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2,\n","                   0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5,\n","                   0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7,\n","                   0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9,\n","                   0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2,\n","                   0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5,\n","                   0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7,\n","                   0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9,\n","                   0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2,\n","                   0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5,\n","                   0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2],\n","             mask=[False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False],\n","       fill_value='?',\n","            dtype=object), 'params': [{'learning_rate': 0.01, 'max_depth': 4, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.2}], 'split0_test_score': array([0.66666667, 0.66666667, 0.66666667, 0.66666667, 0.9037037 ,\n","       0.88888889, 0.85185185, 0.85185185, 0.91851852, 0.91851852,\n","       0.91111111, 0.87407407, 0.91851852, 0.92592593, 0.91851852,\n","       0.88888889, 0.66666667, 0.66666667, 0.66666667, 0.66666667,\n","       0.91851852, 0.91111111, 0.8962963 , 0.87407407, 0.92592593,\n","       0.92592593, 0.91111111, 0.87407407, 0.93333333, 0.92592593,\n","       0.92592593, 0.9037037 , 0.66666667, 0.66666667, 0.66666667,\n","       0.66666667, 0.93333333, 0.93333333, 0.88888889, 0.84444444,\n","       0.93333333, 0.93333333, 0.92592593, 0.8962963 , 0.94814815,\n","       0.94074074, 0.92592593, 0.88148148, 0.66666667, 0.66666667,\n","       0.66666667, 0.66666667, 0.8962963 , 0.88888889, 0.85925926,\n","       0.85925926, 0.91851852, 0.91851852, 0.8962963 , 0.86666667,\n","       0.91851852, 0.92592593, 0.91851852, 0.88888889, 0.66666667,\n","       0.66666667, 0.66666667, 0.66666667, 0.92592593, 0.91111111,\n","       0.87407407, 0.84444444, 0.92592593, 0.91851852, 0.92592593,\n","       0.88148148, 0.92592593, 0.92592593, 0.94074074, 0.8962963 ,\n","       0.66666667, 0.66666667, 0.66666667, 0.66666667, 0.93333333,\n","       0.91851852, 0.8962963 , 0.87407407, 0.93333333, 0.93333333,\n","       0.92592593, 0.88148148, 0.93333333, 0.94074074, 0.93333333,\n","       0.9037037 , 0.66666667, 0.66666667, 0.66666667, 0.66666667,\n","       0.91851852, 0.88888889, 0.88148148, 0.82962963, 0.92592593,\n","       0.91851852, 0.91851852, 0.86666667, 0.93333333, 0.91851852,\n","       0.92592593, 0.86666667, 0.66666667, 0.66666667, 0.66666667,\n","       0.66666667, 0.91851852, 0.91111111, 0.88888889, 0.79259259,\n","       0.92592593, 0.92592593, 0.91851852, 0.87407407, 0.92592593,\n","       0.93333333, 0.94074074, 0.91851852, 0.66666667, 0.66666667,\n","       0.66666667, 0.66666667, 0.93333333, 0.92592593, 0.9037037 ,\n","       0.87407407, 0.93333333, 0.92592593, 0.92592593, 0.87407407,\n","       0.93333333, 0.93333333, 0.94814815, 0.8962963 , 0.66666667,\n","       0.66666667, 0.66666667, 0.66666667, 0.88148148, 0.9037037 ,\n","       0.88148148, 0.81481481, 0.91851852, 0.91851852, 0.91111111,\n","       0.82222222, 0.92592593, 0.91851852, 0.92592593, 0.8962963 ,\n","       0.66666667, 0.66666667, 0.66666667, 0.66666667, 0.92592593,\n","       0.91111111, 0.84444444, 0.81481481, 0.92592593, 0.92592593,\n","       0.91851852, 0.87407407, 0.93333333, 0.92592593, 0.92592593,\n","       0.88888889, 0.66666667, 0.66666667, 0.66666667, 0.66666667,\n","       0.93333333, 0.91851852, 0.88888889, 0.81481481, 0.93333333,\n","       0.93333333, 0.91111111, 0.88148148, 0.93333333, 0.93333333,\n","       0.92592593, 0.91851852, 0.85185185, 0.85925926, 0.83703704,\n","       0.77777778, 0.92592593, 0.92592593, 0.91111111, 0.9037037 ,\n","       0.91851852, 0.92592593, 0.92592593, 0.91111111, 0.92592593,\n","       0.94074074, 0.93333333, 0.92592593, 0.8962963 , 0.9037037 ,\n","       0.88148148, 0.78518519, 0.93333333, 0.93333333, 0.94814815,\n","       0.88148148, 0.94074074, 0.93333333, 0.92592593, 0.9037037 ,\n","       0.94814815, 0.94814815, 0.94814815, 0.91851852, 0.91851852,\n","       0.88148148, 0.85185185, 0.78518519, 0.94814815, 0.93333333,\n","       0.93333333, 0.8962963 , 0.94814815, 0.94074074, 0.94074074,\n","       0.91111111, 0.95555556, 0.94814815, 0.94814815, 0.93333333,\n","       0.9037037 , 0.85185185, 0.84444444, 0.79259259, 0.92592593,\n","       0.92592593, 0.91851852, 0.91111111, 0.93333333, 0.92592593,\n","       0.93333333, 0.8962963 , 0.94074074, 0.93333333, 0.94814815,\n","       0.92592593, 0.9037037 , 0.88148148, 0.81481481, 0.79259259,\n","       0.92592593, 0.94814815, 0.93333333, 0.8962963 , 0.93333333,\n","       0.93333333, 0.94074074, 0.92592593, 0.94074074, 0.95555556,\n","       0.94074074, 0.92592593, 0.93333333, 0.91851852, 0.81481481,\n","       0.86666667, 0.93333333, 0.94074074, 0.94074074, 0.8962963 ,\n","       0.93333333, 0.94074074, 0.95555556, 0.9037037 , 0.93333333,\n","       0.94074074, 0.94074074, 0.92592593, 0.85925926, 0.87407407,\n","       0.84444444, 0.77777778, 0.93333333, 0.93333333, 0.94074074,\n","       0.8962963 , 0.92592593, 0.93333333, 0.92592593, 0.9037037 ,\n","       0.93333333, 0.94074074, 0.93333333, 0.8962963 , 0.9037037 ,\n","       0.85185185, 0.82962963, 0.78518519, 0.93333333, 0.94074074,\n","       0.92592593, 0.87407407, 0.92592593, 0.93333333, 0.94074074,\n","       0.93333333, 0.94074074, 0.93333333, 0.94814815, 0.92592593,\n","       0.93333333, 0.8962963 , 0.87407407, 0.79259259, 0.93333333,\n","       0.93333333, 0.92592593, 0.88888889, 0.93333333, 0.93333333,\n","       0.95555556, 0.91111111, 0.93333333, 0.93333333, 0.95555556,\n","       0.93333333, 0.83703704, 0.85185185, 0.84444444, 0.77777778,\n","       0.91851852, 0.92592593, 0.91851852, 0.8962963 , 0.92592593,\n","       0.92592593, 0.94074074, 0.9037037 , 0.93333333, 0.94814815,\n","       0.92592593, 0.91851852, 0.91851852, 0.8962963 , 0.84444444,\n","       0.79259259, 0.93333333, 0.91851852, 0.93333333, 0.85185185,\n","       0.92592593, 0.92592593, 0.94814815, 0.9037037 , 0.93333333,\n","       0.93333333, 0.94074074, 0.93333333, 0.93333333, 0.9037037 ,\n","       0.83703704, 0.84444444, 0.93333333, 0.93333333, 0.92592593,\n","       0.85925926, 0.93333333, 0.93333333, 0.93333333, 0.91851852,\n","       0.93333333, 0.95555556, 0.94814815, 0.93333333, 0.9037037 ,\n","       0.88888889, 0.9037037 , 0.88148148, 0.93333333, 0.93333333,\n","       0.92592593, 0.88888889, 0.93333333, 0.95555556, 0.92592593,\n","       0.91851852, 0.94074074, 0.94074074, 0.95555556, 0.91111111,\n","       0.92592593, 0.91111111, 0.91111111, 0.87407407, 0.93333333,\n","       0.94814815, 0.93333333, 0.88888889, 0.95555556, 0.94074074,\n","       0.94814815, 0.91111111, 0.95555556, 0.94814815, 0.94814815,\n","       0.91851852, 0.93333333, 0.94074074, 0.9037037 , 0.88888889,\n","       0.94814815, 0.94814815, 0.93333333, 0.93333333, 0.94814815,\n","       0.96296296, 0.94814815, 0.93333333, 0.95555556, 0.96296296,\n","       0.94814815, 0.91851852, 0.88888889, 0.91111111, 0.87407407,\n","       0.85925926, 0.91851852, 0.93333333, 0.93333333, 0.88888889,\n","       0.93333333, 0.94074074, 0.94074074, 0.9037037 , 0.93333333,\n","       0.92592593, 0.93333333, 0.9037037 , 0.93333333, 0.91851852,\n","       0.8962963 , 0.85925926, 0.93333333, 0.93333333, 0.93333333,\n","       0.91851852, 0.93333333, 0.93333333, 0.94814815, 0.93333333,\n","       0.95555556, 0.94814815, 0.94814815, 0.92592593, 0.93333333,\n","       0.92592593, 0.9037037 , 0.9037037 , 0.93333333, 0.93333333,\n","       0.94074074, 0.93333333, 0.93333333, 0.94074074, 0.94814815,\n","       0.91851852, 0.93333333, 0.95555556, 0.94814815, 0.92592593,\n","       0.91851852, 0.92592593, 0.9037037 , 0.87407407, 0.94074074,\n","       0.94074074, 0.9037037 , 0.9037037 , 0.93333333, 0.94074074,\n","       0.93333333, 0.91111111, 0.92592593, 0.94074074, 0.94074074,\n","       0.91111111, 0.92592593, 0.94074074, 0.9037037 , 0.81481481,\n","       0.93333333, 0.94814815, 0.93333333, 0.9037037 , 0.93333333,\n","       0.94074074, 0.94074074, 0.91851852, 0.94814815, 0.94814815,\n","       0.94814815, 0.91111111, 0.93333333, 0.93333333, 0.91111111,\n","       0.80740741, 0.93333333, 0.95555556, 0.94814815, 0.92592593,\n","       0.93333333, 0.94814815, 0.94814815, 0.92592593, 0.93333333,\n","       0.95555556, 0.95555556, 0.93333333, 0.91851852, 0.87407407,\n","       0.9037037 , 0.85925926, 0.93333333, 0.93333333, 0.93333333,\n","       0.87407407, 0.94074074, 0.94814815, 0.94074074, 0.8962963 ,\n","       0.92592593, 0.93333333, 0.93333333, 0.9037037 , 0.91851852,\n","       0.91111111, 0.8962963 , 0.86666667, 0.93333333, 0.94814815,\n","       0.94074074, 0.8962963 , 0.92592593, 0.94814815, 0.94074074,\n","       0.92592593, 0.93333333, 0.93333333, 0.94814815, 0.94814815,\n","       0.93333333, 0.92592593, 0.93333333, 0.8962963 , 0.93333333,\n","       0.94074074, 0.94814815, 0.9037037 , 0.93333333, 0.94074074,\n","       0.94074074, 0.91851852, 0.93333333, 0.95555556, 0.94814815,\n","       0.92592593, 0.91111111, 0.91111111, 0.88888889, 0.91111111,\n","       0.92592593, 0.94074074, 0.91851852, 0.91851852, 0.94074074,\n","       0.93333333, 0.93333333, 0.9037037 , 0.94814815, 0.94814815,\n","       0.94074074, 0.9037037 , 0.93333333, 0.93333333, 0.91111111,\n","       0.82962963, 0.94074074, 0.94814815, 0.94074074, 0.92592593,\n","       0.94814815, 0.94074074, 0.93333333, 0.91851852, 0.96296296,\n","       0.95555556, 0.94074074, 0.91851852, 0.94074074, 0.93333333,\n","       0.91111111, 0.88888889, 0.96296296, 0.94074074, 0.96296296,\n","       0.91851852, 0.94814815, 0.96296296, 0.94814815, 0.94074074,\n","       0.97037037, 0.96296296, 0.95555556, 0.93333333, 0.91851852,\n","       0.94074074, 0.88888889, 0.8962963 , 0.93333333, 0.92592593,\n","       0.92592593, 0.91111111, 0.94814815, 0.94814815, 0.94074074,\n","       0.92592593, 0.93333333, 0.94814815, 0.94074074, 0.91111111,\n","       0.93333333, 0.92592593, 0.95555556, 0.87407407, 0.94814815,\n","       0.93333333, 0.94814815, 0.8962963 , 0.95555556, 0.94814815,\n","       0.95555556, 0.91851852, 0.93333333, 0.95555556, 0.94814815,\n","       0.91111111, 0.93333333, 0.93333333, 0.94074074, 0.8962963 ,\n","       0.93333333, 0.95555556, 0.96296296, 0.91111111, 0.94074074,\n","       0.94074074, 0.94814815, 0.91111111, 0.93333333, 0.94814815,\n","       0.94814815, 0.92592593, 0.88888889, 0.91111111, 0.88888889,\n","       0.88888889, 0.93333333, 0.94814815, 0.93333333, 0.8962963 ,\n","       0.94814815, 0.92592593, 0.94074074, 0.91851852, 0.93333333,\n","       0.94074074, 0.94074074, 0.91111111, 0.92592593, 0.93333333,\n","       0.94074074, 0.91851852, 0.93333333, 0.93333333, 0.94814815,\n","       0.92592593, 0.92592593, 0.94074074, 0.93333333, 0.92592593,\n","       0.94074074, 0.94814815, 0.94814815, 0.92592593, 0.93333333,\n","       0.94074074, 0.94814815, 0.88888889, 0.93333333, 0.93333333,\n","       0.94074074, 0.8962963 , 0.93333333, 0.95555556, 0.95555556,\n","       0.9037037 , 0.93333333, 0.94814815, 0.95555556, 0.9037037 ,\n","       0.91851852, 0.94074074, 0.92592593, 0.87407407, 0.92592593,\n","       0.93333333, 0.93333333, 0.92592593, 0.92592593, 0.93333333,\n","       0.94074074, 0.91111111, 0.93333333, 0.94814815, 0.93333333,\n","       0.8962963 , 0.92592593, 0.94074074, 0.91111111, 0.91851852,\n","       0.92592593, 0.94074074, 0.92592593, 0.91851852, 0.93333333,\n","       0.94814815, 0.94074074, 0.91851852, 0.93333333, 0.94814815,\n","       0.94814815, 0.91111111, 0.94074074, 0.93333333, 0.91851852,\n","       0.8962963 , 0.93333333, 0.94074074, 0.94074074, 0.92592593,\n","       0.93333333, 0.94814815, 0.94814815, 0.93333333, 0.93333333,\n","       0.94074074, 0.94814815, 0.92592593]), 'split1_test_score': array([0.66666667, 0.66666667, 0.66666667, 0.66666667, 0.93333333,\n","       0.93333333, 0.92592593, 0.9037037 , 0.93333333, 0.93333333,\n","       0.92592593, 0.94814815, 0.93333333, 0.93333333, 0.93333333,\n","       0.94814815, 0.66666667, 0.66666667, 0.66666667, 0.66666667,\n","       0.93333333, 0.93333333, 0.93333333, 0.92592593, 0.93333333,\n","       0.93333333, 0.93333333, 0.93333333, 0.92592593, 0.92592593,\n","       0.93333333, 0.92592593, 0.66666667, 0.66666667, 0.66666667,\n","       0.66666667, 0.93333333, 0.92592593, 0.92592593, 0.88148148,\n","       0.92592593, 0.92592593, 0.93333333, 0.93333333, 0.92592593,\n","       0.92592593, 0.93333333, 0.93333333, 0.66666667, 0.66666667,\n","       0.66666667, 0.66666667, 0.93333333, 0.93333333, 0.94074074,\n","       0.88148148, 0.93333333, 0.93333333, 0.93333333, 0.94074074,\n","       0.93333333, 0.93333333, 0.93333333, 0.93333333, 0.66666667,\n","       0.66666667, 0.66666667, 0.66666667, 0.93333333, 0.93333333,\n","       0.92592593, 0.92592593, 0.91851852, 0.93333333, 0.93333333,\n","       0.93333333, 0.92592593, 0.93333333, 0.92592593, 0.93333333,\n","       0.66666667, 0.66666667, 0.66666667, 0.66666667, 0.87407407,\n","       0.91851852, 0.92592593, 0.93333333, 0.87407407, 0.86666667,\n","       0.93333333, 0.91851852, 0.85925926, 0.86666667, 0.92592593,\n","       0.92592593, 0.66666667, 0.66666667, 0.66666667, 0.66666667,\n","       0.94814815, 0.93333333, 0.94814815, 0.94074074, 0.93333333,\n","       0.93333333, 0.93333333, 0.94074074, 0.93333333, 0.93333333,\n","       0.93333333, 0.93333333, 0.66666667, 0.66666667, 0.66666667,\n","       0.66666667, 0.93333333, 0.93333333, 0.93333333, 0.93333333,\n","       0.92592593, 0.93333333, 0.93333333, 0.92592593, 0.92592593,\n","       0.93333333, 0.93333333, 0.93333333, 0.66666667, 0.66666667,\n","       0.66666667, 0.66666667, 0.87407407, 0.93333333, 0.92592593,\n","       0.94074074, 0.86666667, 0.87407407, 0.93333333, 0.91851852,\n","       0.86666667, 0.86666667, 0.94074074, 0.91851852, 0.66666667,\n","       0.66666667, 0.66666667, 0.66666667, 0.93333333, 0.94814815,\n","       0.94814815, 0.88148148, 0.93333333, 0.93333333, 0.93333333,\n","       0.93333333, 0.93333333, 0.93333333, 0.93333333, 0.94814815,\n","       0.66666667, 0.66666667, 0.66666667, 0.66666667, 0.92592593,\n","       0.93333333, 0.93333333, 0.95555556, 0.92592593, 0.93333333,\n","       0.93333333, 0.92592593, 0.92592593, 0.93333333, 0.93333333,\n","       0.92592593, 0.66666667, 0.66666667, 0.66666667, 0.66666667,\n","       0.87407407, 0.94074074, 0.91851852, 0.93333333, 0.86666667,\n","       0.92592593, 0.92592593, 0.92592593, 0.85185185, 0.87407407,\n","       0.93333333, 0.93333333, 0.94814815, 0.93333333, 0.94814815,\n","       0.92592593, 0.94814815, 0.93333333, 0.93333333, 0.94074074,\n","       0.93333333, 0.93333333, 0.93333333, 0.93333333, 0.94074074,\n","       0.93333333, 0.93333333, 0.93333333, 0.93333333, 0.92592593,\n","       0.91851852, 0.88888889, 0.93333333, 0.93333333, 0.93333333,\n","       0.93333333, 0.93333333, 0.93333333, 0.94074074, 0.93333333,\n","       0.93333333, 0.93333333, 0.94074074, 0.93333333, 0.92592593,\n","       0.92592593, 0.91851852, 0.95555556, 0.92592593, 0.93333333,\n","       0.92592593, 0.93333333, 0.91851852, 0.92592593, 0.94074074,\n","       0.93333333, 0.86666667, 0.93333333, 0.93333333, 0.93333333,\n","       0.94074074, 0.92592593, 0.96296296, 0.88148148, 0.94074074,\n","       0.93333333, 0.93333333, 0.94814815, 0.93333333, 0.93333333,\n","       0.93333333, 0.94814815, 0.93333333, 0.93333333, 0.93333333,\n","       0.93333333, 0.94814815, 0.92592593, 0.91851852, 0.88148148,\n","       0.92592593, 0.93333333, 0.93333333, 0.92592593, 0.87407407,\n","       0.94074074, 0.93333333, 0.93333333, 0.86666667, 0.92592593,\n","       0.93333333, 0.93333333, 0.87407407, 0.92592593, 0.91111111,\n","       0.9037037 , 0.86666667, 0.86666667, 0.93333333, 0.92592593,\n","       0.85185185, 0.91111111, 0.94074074, 0.92592593, 0.85185185,\n","       0.88888889, 0.93333333, 0.93333333, 0.93333333, 0.94814815,\n","       0.94814815, 0.94074074, 0.93333333, 0.93333333, 0.93333333,\n","       0.92592593, 0.93333333, 0.93333333, 0.93333333, 0.93333333,\n","       0.92592593, 0.93333333, 0.93333333, 0.93333333, 0.91851852,\n","       0.93333333, 0.92592593, 0.8962963 , 0.91851852, 0.92592593,\n","       0.93333333, 0.93333333, 0.91851852, 0.92592593, 0.94074074,\n","       0.93333333, 0.86666667, 0.93333333, 0.93333333, 0.93333333,\n","       0.87407407, 0.92592593, 0.91851852, 0.91851852, 0.86666667,\n","       0.86666667, 0.92592593, 0.92592593, 0.85185185, 0.88148148,\n","       0.94074074, 0.93333333, 0.85185185, 0.86666667, 0.91851852,\n","       0.93333333, 0.96296296, 0.94074074, 0.94074074, 0.93333333,\n","       0.94074074, 0.93333333, 0.93333333, 0.94814815, 0.93333333,\n","       0.92592593, 0.93333333, 0.94814815, 0.93333333, 0.93333333,\n","       0.93333333, 0.93333333, 0.93333333, 0.92592593, 0.93333333,\n","       0.86666667, 0.9037037 , 0.9037037 , 0.93333333, 0.93333333,\n","       0.92592593, 0.91851852, 0.93333333, 0.93333333, 0.91851852,\n","       0.91851852, 0.93333333, 0.93333333, 0.87407407, 0.93333333,\n","       0.93333333, 0.88888889, 0.86666667, 0.91851852, 0.86666667,\n","       0.93333333, 0.86666667, 0.86666667, 0.92592593, 0.92592593,\n","       0.85185185, 0.86666667, 0.92592593, 0.93333333, 0.93333333,\n","       0.93333333, 0.93333333, 0.91851852, 0.93333333, 0.93333333,\n","       0.93333333, 0.94814815, 0.93333333, 0.93333333, 0.93333333,\n","       0.93333333, 0.94074074, 0.94074074, 0.93333333, 0.93333333,\n","       0.93333333, 0.93333333, 0.93333333, 0.93333333, 0.92592593,\n","       0.92592593, 0.93333333, 0.93333333, 0.93333333, 0.92592593,\n","       0.92592593, 0.93333333, 0.91851852, 0.92592593, 0.92592593,\n","       0.93333333, 0.92592593, 0.93333333, 0.93333333, 0.91851852,\n","       0.92592593, 0.93333333, 0.94074074, 0.92592593, 0.92592593,\n","       0.91111111, 0.91851852, 0.93333333, 0.85925926, 0.86666667,\n","       0.93333333, 0.93333333, 0.94814815, 0.94814815, 0.94814815,\n","       0.93333333, 0.93333333, 0.93333333, 0.93333333, 0.94814815,\n","       0.93333333, 0.93333333, 0.93333333, 0.93333333, 0.94074074,\n","       0.94074074, 0.93333333, 0.94814815, 0.93333333, 0.92592593,\n","       0.93333333, 0.94074074, 0.92592593, 0.91851852, 0.93333333,\n","       0.93333333, 0.85925926, 0.93333333, 0.94074074, 0.93333333,\n","       0.86666667, 0.92592593, 0.93333333, 0.93333333, 0.86666667,\n","       0.86666667, 0.91851852, 0.91851852, 0.85185185, 0.91851852,\n","       0.93333333, 0.94814815, 0.85185185, 0.86666667, 0.93333333,\n","       0.93333333, 0.85185185, 0.86666667, 0.88888889, 0.93333333,\n","       0.93333333, 0.93333333, 0.92592593, 0.94074074, 0.94074074,\n","       0.93333333, 0.93333333, 0.93333333, 0.93333333, 0.93333333,\n","       0.93333333, 0.93333333, 0.93333333, 0.93333333, 0.93333333,\n","       0.93333333, 0.92592593, 0.92592593, 0.94814815, 0.91851852,\n","       0.92592593, 0.91851852, 0.92592593, 0.93333333, 0.86666667,\n","       0.91851852, 0.93333333, 0.93333333, 0.86666667, 0.92592593,\n","       0.94074074, 0.93333333, 0.86666667, 0.85185185, 0.91111111,\n","       0.91851852, 0.86666667, 0.86666667, 0.91851852, 0.92592593,\n","       0.85185185, 0.86666667, 0.93333333, 0.93333333, 0.85925926,\n","       0.86666667, 0.93333333, 0.93333333, 0.93333333, 0.93333333,\n","       0.94814815, 0.93333333, 0.92592593, 0.93333333, 0.93333333,\n","       0.93333333, 0.93333333, 0.93333333, 0.93333333, 0.93333333,\n","       0.93333333, 0.94074074, 0.94074074, 0.93333333, 0.91851852,\n","       0.92592593, 0.93333333, 0.91851852, 0.92592593, 0.92592593,\n","       0.93333333, 0.93333333, 0.86666667, 0.92592593, 0.93333333,\n","       0.93333333, 0.92592593, 0.92592593, 0.92592593, 0.93333333,\n","       0.85185185, 0.91851852, 0.94074074, 0.91851852, 0.85185185,\n","       0.86666667, 0.92592593, 0.92592593, 0.85925926, 0.86666667,\n","       0.92592593, 0.93333333, 0.86666667, 0.88148148, 0.88888889,\n","       0.92592593, 0.93333333, 0.93333333, 0.93333333, 0.91851852,\n","       0.94074074, 0.94074074, 0.93333333, 0.93333333, 0.93333333,\n","       0.93333333, 0.93333333, 0.93333333, 0.94074074, 0.93333333,\n","       0.93333333, 0.93333333, 0.93333333, 0.93333333, 0.93333333,\n","       0.93333333, 0.93333333, 0.93333333, 0.92592593, 0.93333333,\n","       0.92592593, 0.92592593, 0.92592593, 0.93333333, 0.91851852,\n","       0.93333333, 0.93333333, 0.93333333, 0.93333333, 0.92592593,\n","       0.94074074, 0.91851852, 0.85925926, 0.93333333, 0.88888889,\n","       0.93333333, 0.85925926, 0.92592593, 0.93333333, 0.93333333,\n","       0.85925926, 0.86666667, 0.93333333, 0.93333333, 0.93333333,\n","       0.92592593, 0.93333333, 0.93333333, 0.93333333, 0.93333333,\n","       0.93333333, 0.93333333, 0.93333333, 0.93333333, 0.93333333,\n","       0.93333333, 0.94074074, 0.94074074, 0.93333333, 0.93333333,\n","       0.93333333, 0.93333333, 0.93333333, 0.92592593, 0.91851852,\n","       0.86666667, 0.93333333, 0.93333333, 0.85925926, 0.92592593,\n","       0.93333333, 0.93333333, 0.86666667, 0.92592593, 0.92592593,\n","       0.93333333, 0.85185185, 0.93333333, 0.93333333, 0.91851852,\n","       0.85925926, 0.86666667, 0.92592593, 0.93333333, 0.85185185,\n","       0.86666667, 0.93333333, 0.93333333, 0.86666667, 0.8962963 ,\n","       0.91111111, 0.93333333, 0.93333333, 0.93333333, 0.93333333,\n","       0.88888889, 0.93333333, 0.93333333, 0.93333333, 0.93333333,\n","       0.93333333, 0.93333333, 0.93333333, 0.93333333, 0.93333333,\n","       0.93333333, 0.93333333, 0.93333333, 0.91851852, 0.91111111,\n","       0.93333333, 0.94814815, 0.87407407, 0.93333333, 0.93333333,\n","       0.93333333, 0.85925926, 0.92592593, 0.93333333, 0.93333333,\n","       0.86666667, 0.94074074, 0.91851852, 0.93333333, 0.86666667,\n","       0.91111111, 0.92592593, 0.92592593, 0.85925926, 0.86666667,\n","       0.88148148, 0.93333333, 0.85185185, 0.93333333, 0.92592593,\n","       0.93333333, 0.85925926, 0.86666667, 0.93333333, 0.93333333,\n","       0.92592593, 0.93333333, 0.93333333, 0.92592593, 0.93333333,\n","       0.93333333, 0.93333333, 0.93333333, 0.92592593, 0.93333333,\n","       0.94814815, 0.93333333, 0.93333333, 0.93333333, 0.93333333,\n","       0.93333333, 0.9037037 , 0.93333333, 0.93333333, 0.95555556,\n","       0.86666667, 0.92592593, 0.93333333, 0.94814815, 0.92592593,\n","       0.92592593, 0.93333333, 0.93333333, 0.85925926, 0.91851852,\n","       0.93333333, 0.93333333, 0.85185185, 0.86666667, 0.92592593,\n","       0.93333333, 0.85185185, 0.86666667, 0.93333333, 0.93333333,\n","       0.85185185, 0.86666667, 0.92592593, 0.93333333, 0.85185185,\n","       0.8962963 , 0.87407407, 0.93333333]), 'mean_test_score': array([0.66666667, 0.66666667, 0.66666667, 0.66666667, 0.91851852,\n","       0.91111111, 0.88888889, 0.87777778, 0.92592593, 0.92592593,\n","       0.91851852, 0.91111111, 0.92592593, 0.92962963, 0.92592593,\n","       0.91851852, 0.66666667, 0.66666667, 0.66666667, 0.66666667,\n","       0.92592593, 0.92222222, 0.91481481, 0.9       , 0.92962963,\n","       0.92962963, 0.92222222, 0.9037037 , 0.92962963, 0.92592593,\n","       0.92962963, 0.91481481, 0.66666667, 0.66666667, 0.66666667,\n","       0.66666667, 0.93333333, 0.92962963, 0.90740741, 0.86296296,\n","       0.92962963, 0.92962963, 0.92962963, 0.91481481, 0.93703704,\n","       0.93333333, 0.92962963, 0.90740741, 0.66666667, 0.66666667,\n","       0.66666667, 0.66666667, 0.91481481, 0.91111111, 0.9       ,\n","       0.87037037, 0.92592593, 0.92592593, 0.91481481, 0.9037037 ,\n","       0.92592593, 0.92962963, 0.92592593, 0.91111111, 0.66666667,\n","       0.66666667, 0.66666667, 0.66666667, 0.92962963, 0.92222222,\n","       0.9       , 0.88518519, 0.92222222, 0.92592593, 0.92962963,\n","       0.90740741, 0.92592593, 0.92962963, 0.93333333, 0.91481481,\n","       0.66666667, 0.66666667, 0.66666667, 0.66666667, 0.9037037 ,\n","       0.91851852, 0.91111111, 0.9037037 , 0.9037037 , 0.9       ,\n","       0.92962963, 0.9       , 0.8962963 , 0.9037037 , 0.92962963,\n","       0.91481481, 0.66666667, 0.66666667, 0.66666667, 0.66666667,\n","       0.93333333, 0.91111111, 0.91481481, 0.88518519, 0.92962963,\n","       0.92592593, 0.92592593, 0.9037037 , 0.93333333, 0.92592593,\n","       0.92962963, 0.9       , 0.66666667, 0.66666667, 0.66666667,\n","       0.66666667, 0.92592593, 0.92222222, 0.91111111, 0.86296296,\n","       0.92592593, 0.92962963, 0.92592593, 0.9       , 0.92592593,\n","       0.93333333, 0.93703704, 0.92592593, 0.66666667, 0.66666667,\n","       0.66666667, 0.66666667, 0.9037037 , 0.92962963, 0.91481481,\n","       0.90740741, 0.9       , 0.9       , 0.92962963, 0.8962963 ,\n","       0.9       , 0.9       , 0.94444444, 0.90740741, 0.66666667,\n","       0.66666667, 0.66666667, 0.66666667, 0.90740741, 0.92592593,\n","       0.91481481, 0.84814815, 0.92592593, 0.92592593, 0.92222222,\n","       0.87777778, 0.92962963, 0.92592593, 0.92962963, 0.92222222,\n","       0.66666667, 0.66666667, 0.66666667, 0.66666667, 0.92592593,\n","       0.92222222, 0.88888889, 0.88518519, 0.92592593, 0.92962963,\n","       0.92592593, 0.9       , 0.92962963, 0.92962963, 0.92962963,\n","       0.90740741, 0.66666667, 0.66666667, 0.66666667, 0.66666667,\n","       0.9037037 , 0.92962963, 0.9037037 , 0.87407407, 0.9       ,\n","       0.92962963, 0.91851852, 0.9037037 , 0.89259259, 0.9037037 ,\n","       0.92962963, 0.92592593, 0.9       , 0.8962963 , 0.89259259,\n","       0.85185185, 0.93703704, 0.92962963, 0.92222222, 0.92222222,\n","       0.92592593, 0.92962963, 0.92962963, 0.92222222, 0.93333333,\n","       0.93703704, 0.93333333, 0.92962963, 0.91481481, 0.91481481,\n","       0.9       , 0.83703704, 0.93333333, 0.93333333, 0.94074074,\n","       0.90740741, 0.93703704, 0.93333333, 0.93333333, 0.91851852,\n","       0.94074074, 0.94074074, 0.94444444, 0.92592593, 0.92222222,\n","       0.9037037 , 0.88518519, 0.87037037, 0.93703704, 0.93333333,\n","       0.92962963, 0.91481481, 0.93333333, 0.93333333, 0.94074074,\n","       0.92222222, 0.91111111, 0.94074074, 0.94074074, 0.93333333,\n","       0.92222222, 0.88888889, 0.9037037 , 0.83703704, 0.93333333,\n","       0.92962963, 0.92592593, 0.92962963, 0.93333333, 0.92962963,\n","       0.93333333, 0.92222222, 0.93703704, 0.93333333, 0.94074074,\n","       0.92962963, 0.92592593, 0.9037037 , 0.86666667, 0.83703704,\n","       0.92592593, 0.94074074, 0.93333333, 0.91111111, 0.9037037 ,\n","       0.93703704, 0.93703704, 0.92962963, 0.9037037 , 0.94074074,\n","       0.93703704, 0.92962963, 0.9037037 , 0.92222222, 0.86296296,\n","       0.88518519, 0.9       , 0.9037037 , 0.93703704, 0.91111111,\n","       0.89259259, 0.92592593, 0.94814815, 0.91481481, 0.89259259,\n","       0.91481481, 0.93703704, 0.92962963, 0.8962963 , 0.91111111,\n","       0.8962963 , 0.85925926, 0.93333333, 0.93333333, 0.93703704,\n","       0.91111111, 0.92962963, 0.93333333, 0.92962963, 0.91851852,\n","       0.92962963, 0.93703704, 0.93333333, 0.91481481, 0.91111111,\n","       0.89259259, 0.87777778, 0.84074074, 0.92592593, 0.93333333,\n","       0.92962963, 0.9037037 , 0.92222222, 0.92962963, 0.94074074,\n","       0.93333333, 0.9037037 , 0.93333333, 0.94074074, 0.92962963,\n","       0.9037037 , 0.91111111, 0.8962963 , 0.85555556, 0.9       ,\n","       0.9       , 0.92592593, 0.90740741, 0.89259259, 0.90740741,\n","       0.94814815, 0.92222222, 0.89259259, 0.9       , 0.93703704,\n","       0.93333333, 0.9       , 0.8962963 , 0.89259259, 0.85555556,\n","       0.92962963, 0.92962963, 0.92592593, 0.92222222, 0.92962963,\n","       0.92592593, 0.93703704, 0.92592593, 0.93333333, 0.94074074,\n","       0.92962963, 0.92592593, 0.92592593, 0.91111111, 0.88888889,\n","       0.82962963, 0.91851852, 0.91111111, 0.93333333, 0.89259259,\n","       0.92592593, 0.92222222, 0.94074074, 0.91851852, 0.92592593,\n","       0.92592593, 0.93703704, 0.93333333, 0.9037037 , 0.91851852,\n","       0.88518519, 0.86666667, 0.9       , 0.92592593, 0.8962963 ,\n","       0.8962963 , 0.9       , 0.9       , 0.92962963, 0.92222222,\n","       0.89259259, 0.91111111, 0.93703704, 0.93333333, 0.91851852,\n","       0.91111111, 0.91851852, 0.9       , 0.93333333, 0.93333333,\n","       0.92962963, 0.91851852, 0.93333333, 0.94444444, 0.92962963,\n","       0.92592593, 0.94074074, 0.94074074, 0.94444444, 0.92222222,\n","       0.92962963, 0.92222222, 0.92222222, 0.9037037 , 0.92962963,\n","       0.93703704, 0.93333333, 0.91111111, 0.94444444, 0.93333333,\n","       0.93703704, 0.92222222, 0.93703704, 0.93703704, 0.93703704,\n","       0.92592593, 0.92962963, 0.93703704, 0.91851852, 0.9037037 ,\n","       0.93703704, 0.94074074, 0.93703704, 0.92962963, 0.93703704,\n","       0.93703704, 0.93333333, 0.93333333, 0.90740741, 0.91481481,\n","       0.94074074, 0.92592593, 0.91851852, 0.92962963, 0.91111111,\n","       0.8962963 , 0.92592593, 0.93333333, 0.93333333, 0.91851852,\n","       0.93333333, 0.93703704, 0.93703704, 0.91851852, 0.93703704,\n","       0.93333333, 0.93333333, 0.92592593, 0.93333333, 0.92222222,\n","       0.91481481, 0.9       , 0.92962963, 0.92592593, 0.93333333,\n","       0.92592593, 0.8962963 , 0.93333333, 0.94444444, 0.93333333,\n","       0.91111111, 0.93703704, 0.94074074, 0.92962963, 0.9       ,\n","       0.8962963 , 0.91111111, 0.91111111, 0.89259259, 0.92592593,\n","       0.93703704, 0.94074074, 0.89259259, 0.9037037 , 0.94074074,\n","       0.92592593, 0.89259259, 0.91111111, 0.91851852, 0.92962963,\n","       0.92592593, 0.92962963, 0.91481481, 0.90740741, 0.94074074,\n","       0.93703704, 0.91851852, 0.91851852, 0.93333333, 0.93703704,\n","       0.93333333, 0.92222222, 0.92962963, 0.93703704, 0.93703704,\n","       0.92222222, 0.92592593, 0.93333333, 0.92592593, 0.86666667,\n","       0.92962963, 0.93333333, 0.92962963, 0.91851852, 0.9       ,\n","       0.92962963, 0.93703704, 0.92592593, 0.90740741, 0.93703704,\n","       0.94444444, 0.92222222, 0.9       , 0.89259259, 0.91111111,\n","       0.86296296, 0.9       , 0.91111111, 0.93333333, 0.92592593,\n","       0.89259259, 0.90740741, 0.94074074, 0.92962963, 0.8962963 ,\n","       0.91111111, 0.94444444, 0.93333333, 0.92592593, 0.9037037 ,\n","       0.92592593, 0.8962963 , 0.92962963, 0.93333333, 0.93333333,\n","       0.9037037 , 0.93703704, 0.94074074, 0.93703704, 0.91481481,\n","       0.92962963, 0.93703704, 0.93703704, 0.91851852, 0.91851852,\n","       0.91851852, 0.91481481, 0.89259259, 0.92962963, 0.93703704,\n","       0.93703704, 0.91481481, 0.8962963 , 0.93703704, 0.93703704,\n","       0.92962963, 0.92962963, 0.92962963, 0.93703704, 0.94074074,\n","       0.89259259, 0.92222222, 0.93703704, 0.90740741, 0.89259259,\n","       0.9037037 , 0.93703704, 0.91481481, 0.8962963 , 0.9037037 ,\n","       0.93333333, 0.92592593, 0.9       , 0.91851852, 0.91851852,\n","       0.92592593, 0.92222222, 0.92222222, 0.91111111, 0.91481481,\n","       0.93333333, 0.94074074, 0.92592593, 0.92592593, 0.93703704,\n","       0.93333333, 0.93333333, 0.91851852, 0.94444444, 0.94074074,\n","       0.93703704, 0.91851852, 0.93333333, 0.93333333, 0.92222222,\n","       0.88148148, 0.93703704, 0.94074074, 0.93333333, 0.92962963,\n","       0.93703704, 0.93333333, 0.92962963, 0.92592593, 0.94074074,\n","       0.94444444, 0.93703704, 0.92592593, 0.93703704, 0.92962963,\n","       0.92592593, 0.9037037 , 0.91111111, 0.93703704, 0.92592593,\n","       0.92592593, 0.9037037 , 0.94444444, 0.94074074, 0.93703704,\n","       0.91481481, 0.91481481, 0.94444444, 0.93333333, 0.92592593,\n","       0.93333333, 0.91111111, 0.91481481, 0.93333333, 0.92962963,\n","       0.92962963, 0.92222222, 0.94074074, 0.94074074, 0.93703704,\n","       0.92962963, 0.93703704, 0.94444444, 0.93703704, 0.92222222,\n","       0.93333333, 0.92962963, 0.94444444, 0.9       , 0.93333333,\n","       0.9       , 0.94074074, 0.91481481, 0.90740741, 0.93703704,\n","       0.94444444, 0.92592593, 0.9       , 0.94074074, 0.93703704,\n","       0.92222222, 0.89259259, 0.93333333, 0.93703704, 0.90740741,\n","       0.8962963 , 0.91111111, 0.94444444, 0.92222222, 0.8962963 ,\n","       0.9037037 , 0.94074074, 0.92222222, 0.9       , 0.92222222,\n","       0.92962963, 0.92962963, 0.91111111, 0.92222222, 0.91111111,\n","       0.88888889, 0.93333333, 0.94074074, 0.93333333, 0.91481481,\n","       0.94074074, 0.92962963, 0.93703704, 0.92592593, 0.93333333,\n","       0.93703704, 0.93703704, 0.92222222, 0.92222222, 0.92222222,\n","       0.93703704, 0.93333333, 0.9037037 , 0.93333333, 0.94074074,\n","       0.92962963, 0.89259259, 0.93333333, 0.93333333, 0.92962963,\n","       0.9037037 , 0.94444444, 0.93333333, 0.92962963, 0.9       ,\n","       0.92592593, 0.93703704, 0.90740741, 0.8962963 , 0.9       ,\n","       0.91111111, 0.91481481, 0.89259259, 0.94444444, 0.94074074,\n","       0.91851852, 0.8962963 , 0.90740741, 0.94444444, 0.91851852,\n","       0.92222222, 0.93703704, 0.92962963, 0.9       , 0.92962963,\n","       0.93333333, 0.93333333, 0.92962963, 0.92592593, 0.93333333,\n","       0.94444444, 0.92222222, 0.93333333, 0.94074074, 0.93333333,\n","       0.91481481, 0.91481481, 0.93703704, 0.92222222, 0.93703704,\n","       0.8962963 , 0.93333333, 0.92962963, 0.93333333, 0.92962963,\n","       0.93703704, 0.93703704, 0.92592593, 0.8962963 , 0.93333333,\n","       0.94074074, 0.92222222, 0.8962963 , 0.9       , 0.92222222,\n","       0.91481481, 0.89259259, 0.9037037 , 0.93703704, 0.92962963,\n","       0.89259259, 0.90740741, 0.93703704, 0.93333333, 0.89259259,\n","       0.91851852, 0.91111111, 0.92962963]), 'std_test_score': array([0.        , 0.        , 0.        , 0.        , 0.01481481,\n","       0.02222222, 0.03703704, 0.02592593, 0.00740741, 0.00740741,\n","       0.00740741, 0.03703704, 0.00740741, 0.0037037 , 0.00740741,\n","       0.02962963, 0.        , 0.        , 0.        , 0.        ,\n","       0.00740741, 0.01111111, 0.01851852, 0.02592593, 0.0037037 ,\n","       0.0037037 , 0.01111111, 0.02962963, 0.0037037 , 0.        ,\n","       0.0037037 , 0.01111111, 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.0037037 , 0.01851852, 0.01851852,\n","       0.0037037 , 0.0037037 , 0.0037037 , 0.01851852, 0.01111111,\n","       0.00740741, 0.0037037 , 0.02592593, 0.        , 0.        ,\n","       0.        , 0.        , 0.01851852, 0.02222222, 0.04074074,\n","       0.01111111, 0.00740741, 0.00740741, 0.01851852, 0.03703704,\n","       0.00740741, 0.0037037 , 0.00740741, 0.02222222, 0.        ,\n","       0.        , 0.        , 0.        , 0.0037037 , 0.01111111,\n","       0.02592593, 0.04074074, 0.0037037 , 0.00740741, 0.0037037 ,\n","       0.02592593, 0.        , 0.0037037 , 0.00740741, 0.01851852,\n","       0.        , 0.        , 0.        , 0.        , 0.02962963,\n","       0.        , 0.01481481, 0.02962963, 0.02962963, 0.03333333,\n","       0.0037037 , 0.01851852, 0.03703704, 0.03703704, 0.0037037 ,\n","       0.01111111, 0.        , 0.        , 0.        , 0.        ,\n","       0.01481481, 0.02222222, 0.03333333, 0.05555556, 0.0037037 ,\n","       0.00740741, 0.00740741, 0.03703704, 0.        , 0.00740741,\n","       0.0037037 , 0.03333333, 0.        , 0.        , 0.        ,\n","       0.        , 0.00740741, 0.01111111, 0.02222222, 0.07037037,\n","       0.        , 0.0037037 , 0.00740741, 0.02592593, 0.        ,\n","       0.        , 0.0037037 , 0.00740741, 0.        , 0.        ,\n","       0.        , 0.        , 0.02962963, 0.0037037 , 0.01111111,\n","       0.03333333, 0.03333333, 0.02592593, 0.0037037 , 0.02222222,\n","       0.03333333, 0.03333333, 0.0037037 , 0.01111111, 0.        ,\n","       0.        , 0.        , 0.        , 0.02592593, 0.02222222,\n","       0.03333333, 0.03333333, 0.00740741, 0.00740741, 0.01111111,\n","       0.05555556, 0.0037037 , 0.00740741, 0.0037037 , 0.02592593,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.01111111, 0.04444444, 0.07037037, 0.        , 0.0037037 ,\n","       0.00740741, 0.02592593, 0.0037037 , 0.0037037 , 0.0037037 ,\n","       0.01851852, 0.        , 0.        , 0.        , 0.        ,\n","       0.02962963, 0.01111111, 0.01481481, 0.05925926, 0.03333333,\n","       0.0037037 , 0.00740741, 0.02222222, 0.04074074, 0.02962963,\n","       0.0037037 , 0.00740741, 0.04814815, 0.03703704, 0.05555556,\n","       0.07407407, 0.01111111, 0.0037037 , 0.01111111, 0.01851852,\n","       0.00740741, 0.0037037 , 0.0037037 , 0.01111111, 0.00740741,\n","       0.0037037 , 0.        , 0.0037037 , 0.01851852, 0.01111111,\n","       0.01851852, 0.05185185, 0.        , 0.        , 0.00740741,\n","       0.02592593, 0.0037037 , 0.        , 0.00740741, 0.01481481,\n","       0.00740741, 0.00740741, 0.0037037 , 0.00740741, 0.0037037 ,\n","       0.02222222, 0.03333333, 0.08518519, 0.01111111, 0.        ,\n","       0.0037037 , 0.01851852, 0.01481481, 0.00740741, 0.        ,\n","       0.01111111, 0.04444444, 0.00740741, 0.00740741, 0.        ,\n","       0.01851852, 0.03703704, 0.05925926, 0.04444444, 0.00740741,\n","       0.0037037 , 0.00740741, 0.01851852, 0.        , 0.0037037 ,\n","       0.        , 0.02592593, 0.0037037 , 0.        , 0.00740741,\n","       0.0037037 , 0.02222222, 0.02222222, 0.05185185, 0.04444444,\n","       0.        , 0.00740741, 0.        , 0.01481481, 0.02962963,\n","       0.0037037 , 0.0037037 , 0.0037037 , 0.03703704, 0.01481481,\n","       0.0037037 , 0.0037037 , 0.02962963, 0.0037037 , 0.04814815,\n","       0.01851852, 0.03333333, 0.03703704, 0.0037037 , 0.01481481,\n","       0.04074074, 0.01481481, 0.00740741, 0.01111111, 0.04074074,\n","       0.02592593, 0.0037037 , 0.0037037 , 0.03703704, 0.03703704,\n","       0.05185185, 0.08148148, 0.        , 0.        , 0.0037037 ,\n","       0.01481481, 0.0037037 , 0.        , 0.0037037 , 0.01481481,\n","       0.0037037 , 0.0037037 , 0.        , 0.01851852, 0.00740741,\n","       0.04074074, 0.04814815, 0.05555556, 0.00740741, 0.00740741,\n","       0.0037037 , 0.02962963, 0.0037037 , 0.0037037 , 0.        ,\n","       0.        , 0.03703704, 0.        , 0.00740741, 0.0037037 ,\n","       0.02962963, 0.01481481, 0.02222222, 0.06296296, 0.03333333,\n","       0.03333333, 0.        , 0.01851852, 0.04074074, 0.02592593,\n","       0.00740741, 0.01111111, 0.04074074, 0.03333333, 0.01851852,\n","       0.        , 0.06296296, 0.04444444, 0.04814815, 0.07777778,\n","       0.01111111, 0.0037037 , 0.00740741, 0.02592593, 0.0037037 ,\n","       0.        , 0.0037037 , 0.02222222, 0.        , 0.00740741,\n","       0.0037037 , 0.00740741, 0.00740741, 0.01481481, 0.04444444,\n","       0.03703704, 0.01481481, 0.00740741, 0.        , 0.04074074,\n","       0.        , 0.0037037 , 0.00740741, 0.01481481, 0.00740741,\n","       0.00740741, 0.0037037 , 0.        , 0.02962963, 0.01481481,\n","       0.04814815, 0.02222222, 0.03333333, 0.00740741, 0.02962963,\n","       0.03703704, 0.03333333, 0.03333333, 0.0037037 , 0.0037037 ,\n","       0.04074074, 0.04444444, 0.01111111, 0.        , 0.01481481,\n","       0.02222222, 0.01481481, 0.01851852, 0.        , 0.        ,\n","       0.0037037 , 0.02962963, 0.        , 0.01111111, 0.0037037 ,\n","       0.00740741, 0.        , 0.        , 0.01111111, 0.01111111,\n","       0.0037037 , 0.01111111, 0.01111111, 0.02962963, 0.0037037 ,\n","       0.01111111, 0.        , 0.02222222, 0.01111111, 0.00740741,\n","       0.01111111, 0.01111111, 0.01851852, 0.01111111, 0.01111111,\n","       0.00740741, 0.0037037 , 0.0037037 , 0.01481481, 0.01481481,\n","       0.01111111, 0.00740741, 0.0037037 , 0.0037037 , 0.01111111,\n","       0.02592593, 0.01481481, 0.        , 0.04814815, 0.04814815,\n","       0.00740741, 0.00740741, 0.02962963, 0.01851852, 0.03703704,\n","       0.03703704, 0.00740741, 0.        , 0.        , 0.02962963,\n","       0.        , 0.0037037 , 0.0037037 , 0.01481481, 0.0037037 ,\n","       0.00740741, 0.        , 0.02222222, 0.        , 0.0037037 ,\n","       0.01851852, 0.04074074, 0.0037037 , 0.00740741, 0.        ,\n","       0.00740741, 0.03703704, 0.        , 0.0037037 , 0.        ,\n","       0.04444444, 0.01111111, 0.00740741, 0.0037037 , 0.03333333,\n","       0.02962963, 0.00740741, 0.00740741, 0.04074074, 0.00740741,\n","       0.0037037 , 0.00740741, 0.04074074, 0.03703704, 0.00740741,\n","       0.00740741, 0.04074074, 0.04444444, 0.02962963, 0.0037037 ,\n","       0.00740741, 0.0037037 , 0.01111111, 0.03333333, 0.        ,\n","       0.0037037 , 0.01481481, 0.01481481, 0.        , 0.0037037 ,\n","       0.        , 0.01111111, 0.0037037 , 0.0037037 , 0.0037037 ,\n","       0.01111111, 0.        , 0.00740741, 0.02222222, 0.05185185,\n","       0.0037037 , 0.01481481, 0.0037037 , 0.01481481, 0.03333333,\n","       0.01111111, 0.0037037 , 0.00740741, 0.04074074, 0.01111111,\n","       0.0037037 , 0.01111111, 0.03333333, 0.04074074, 0.        ,\n","       0.05555556, 0.03333333, 0.04444444, 0.01481481, 0.        ,\n","       0.04074074, 0.04074074, 0.00740741, 0.0037037 , 0.03703704,\n","       0.04444444, 0.01111111, 0.        , 0.00740741, 0.02962963,\n","       0.02222222, 0.03703704, 0.0037037 , 0.        , 0.        ,\n","       0.02962963, 0.0037037 , 0.00740741, 0.0037037 , 0.01851852,\n","       0.0037037 , 0.0037037 , 0.0037037 , 0.01481481, 0.        ,\n","       0.00740741, 0.01851852, 0.02592593, 0.0037037 , 0.01111111,\n","       0.0037037 , 0.01851852, 0.02962963, 0.01111111, 0.0037037 ,\n","       0.0037037 , 0.0037037 , 0.0037037 , 0.01111111, 0.00740741,\n","       0.04074074, 0.0037037 , 0.0037037 , 0.01111111, 0.04074074,\n","       0.03703704, 0.01111111, 0.01111111, 0.03703704, 0.03703704,\n","       0.00740741, 0.00740741, 0.03333333, 0.03703704, 0.02962963,\n","       0.        , 0.01111111, 0.01111111, 0.02222222, 0.0037037 ,\n","       0.00740741, 0.        , 0.00740741, 0.00740741, 0.0037037 ,\n","       0.        , 0.        , 0.01481481, 0.0037037 , 0.00740741,\n","       0.0037037 , 0.01481481, 0.        , 0.        , 0.01111111,\n","       0.05185185, 0.0037037 , 0.00740741, 0.00740741, 0.0037037 ,\n","       0.01111111, 0.00740741, 0.0037037 , 0.00740741, 0.02222222,\n","       0.01111111, 0.0037037 , 0.00740741, 0.0037037 , 0.0037037 ,\n","       0.01481481, 0.01481481, 0.05185185, 0.0037037 , 0.03703704,\n","       0.00740741, 0.04444444, 0.01851852, 0.00740741, 0.0037037 ,\n","       0.05555556, 0.04814815, 0.01111111, 0.        , 0.00740741,\n","       0.00740741, 0.02222222, 0.01851852, 0.        , 0.0037037 ,\n","       0.0037037 , 0.01111111, 0.00740741, 0.00740741, 0.0037037 ,\n","       0.0037037 , 0.0037037 , 0.0037037 , 0.0037037 , 0.01111111,\n","       0.        , 0.0037037 , 0.01111111, 0.02592593, 0.01481481,\n","       0.03333333, 0.00740741, 0.01851852, 0.04814815, 0.01111111,\n","       0.01111111, 0.00740741, 0.03333333, 0.01481481, 0.01111111,\n","       0.01111111, 0.04074074, 0.        , 0.0037037 , 0.01111111,\n","       0.03703704, 0.04444444, 0.01851852, 0.01111111, 0.04444444,\n","       0.03703704, 0.00740741, 0.01111111, 0.03333333, 0.02592593,\n","       0.01851852, 0.0037037 , 0.02222222, 0.01111111, 0.02222222,\n","       0.        , 0.        , 0.00740741, 0.        , 0.01851852,\n","       0.00740741, 0.0037037 , 0.0037037 , 0.00740741, 0.        ,\n","       0.0037037 , 0.0037037 , 0.01111111, 0.0037037 , 0.01111111,\n","       0.0037037 , 0.01481481, 0.02962963, 0.        , 0.00740741,\n","       0.0037037 , 0.03333333, 0.00740741, 0.        , 0.0037037 ,\n","       0.03703704, 0.0037037 , 0.01481481, 0.0037037 , 0.03333333,\n","       0.01481481, 0.01111111, 0.01851852, 0.03703704, 0.03333333,\n","       0.02962963, 0.01851852, 0.04074074, 0.01111111, 0.01481481,\n","       0.01481481, 0.03703704, 0.04074074, 0.01111111, 0.01481481,\n","       0.0037037 , 0.0037037 , 0.0037037 , 0.02592593, 0.0037037 ,\n","       0.        , 0.        , 0.0037037 , 0.        , 0.        ,\n","       0.0037037 , 0.01111111, 0.        , 0.00740741, 0.        ,\n","       0.01851852, 0.01111111, 0.0037037 , 0.01111111, 0.01851852,\n","       0.02962963, 0.00740741, 0.0037037 , 0.01481481, 0.0037037 ,\n","       0.01111111, 0.0037037 , 0.00740741, 0.03703704, 0.01481481,\n","       0.00740741, 0.01111111, 0.04444444, 0.03333333, 0.0037037 ,\n","       0.01851852, 0.04074074, 0.03703704, 0.0037037 , 0.0037037 ,\n","       0.04074074, 0.04074074, 0.01111111, 0.        , 0.04074074,\n","       0.02222222, 0.03703704, 0.0037037 ]), 'rank_test_score': array([721, 721, 721, 721, 446, 522, 686, 698, 322, 322, 446, 510, 322,\n","       227, 322, 446, 721, 721, 721, 721, 322, 398, 479, 601, 227, 227,\n","       398, 565, 227, 322, 227, 479, 721, 721, 721, 721, 139, 227, 545,\n","       707, 227, 227, 227, 479,  63, 139, 227, 545, 721, 721, 721, 721,\n","       479, 522, 601, 702, 322, 322, 479, 565, 322, 227, 322, 522, 721,\n","       721, 721, 721, 227, 398, 601, 691, 398, 322, 227, 545, 322, 227,\n","       139, 479, 721, 721, 721, 721, 565, 446, 522, 565, 565, 601, 227,\n","       601, 639, 565, 227, 479, 721, 721, 721, 721, 139, 522, 477, 691,\n","       227, 322, 322, 565, 139, 322, 227, 601, 721, 721, 721, 721, 322,\n","       398, 522, 707, 322, 227, 322, 601, 322, 139,  63, 322, 721, 721,\n","       721, 721, 565, 227, 479, 545, 601, 601, 227, 639, 601, 601,   3,\n","       545, 721, 721, 721, 721, 545, 322, 477, 715, 322, 322, 398, 698,\n","       227, 322, 227, 398, 721, 721, 721, 721, 322, 398, 686, 691, 322,\n","       227, 322, 601, 227, 227, 227, 545, 721, 721, 721, 721, 565, 227,\n","       565, 701, 601, 227, 446, 565, 662, 565, 227, 322, 601, 639, 662,\n","       714,  63, 227, 398, 398, 322, 227, 227, 398, 139,  63, 139, 227,\n","       479, 479, 601, 717, 139, 139,  23, 545,  63, 139, 139, 446,  23,\n","        23,   3, 322, 398, 565, 691, 702,  63, 139, 227, 479, 139, 139,\n","        23, 398, 510,  23,  23, 139, 398, 686, 565, 717, 139, 227, 322,\n","       227, 139, 227, 139, 398,  63, 139,  23, 227, 322, 565, 704, 717,\n","       322,  23, 139, 522, 565,  63,  63, 227, 565,  23,  63, 227, 565,\n","       398, 707, 691, 601, 565,  63, 522, 662, 322,   1, 479, 662, 479,\n","        63, 227, 639, 510, 639, 711, 139, 139,  63, 522, 227, 139, 227,\n","       446, 227,  63, 139, 479, 522, 662, 698, 716, 322, 139, 227, 565,\n","       398, 227,  23, 139, 565, 139,  23, 227, 565, 522, 639, 712, 601,\n","       601, 322, 545, 662, 545,   1, 398, 662, 601,  63, 139, 638, 639,\n","       662, 712, 227, 227, 322, 398, 227, 322,  63, 322, 139,  23, 227,\n","       322, 322, 522, 686, 720, 446, 522, 139, 662, 322, 398,  23, 446,\n","       322, 322,  63, 139, 565, 446, 691, 704, 601, 322, 639, 639, 601,\n","       601, 227, 398, 662, 510,  63, 139, 446, 522, 446, 601, 139, 139,\n","       227, 446, 139,   3, 227, 322,  23,  23,   3, 398, 227, 398, 398,\n","       565, 227,  63, 139, 522,   3, 139,  63, 398,  63,  63,  63, 322,\n","       227,  63, 446, 565,  63,  23,  63, 227,  63, 138, 139, 139, 545,\n","       479,  23, 322, 446, 227, 510, 639, 322, 139, 139, 446, 139,  63,\n","        63, 446,  63, 139, 139, 322, 139, 398, 479, 601, 227, 322, 139,\n","       322, 639, 139,   3, 139, 510,  63,  23, 227, 601, 639, 522, 522,\n","       662, 322,  63,  23, 662, 565,  23, 322, 662, 510, 446, 227, 322,\n","       227, 479, 545,  23,  63, 446, 446, 139,  63, 139, 398, 227,  63,\n","        63, 398, 322, 139, 322, 704, 227, 139, 227, 446, 601, 227,  63,\n","       322, 545,  63,   3, 398, 601, 662, 522, 707, 601, 510, 139, 322,\n","       662, 545,  23, 227, 639, 510,   3, 139, 322, 565, 322, 639, 227,\n","       139, 139, 565,  63,  23,  63, 479, 227,  63,  63, 446, 446, 446,\n","       479, 662, 227,  63,  63, 479, 639,  63,  63, 227, 227, 227,  63,\n","        23, 662, 398,  63, 545, 662, 565,  63, 479, 639, 565, 139, 322,\n","       601, 446, 446, 322, 398, 398, 522, 479, 139,  23, 322, 322,  63,\n","       139, 139, 446,   3,  23,  63, 446, 139, 139, 398, 697,  63,  23,\n","       139, 227,  63, 139, 227, 322,  62,   3,  63, 322,  63, 227, 322,\n","       565, 522,  63, 397, 322, 565,   3,  23,  63, 479, 479,   3, 139,\n","       322, 139, 522, 479, 139, 227, 227, 398,  23,  23,  63, 227,  63,\n","         3,  63, 398, 139, 227,   3, 601, 139, 601,  23, 479, 545,  63,\n","         3, 322, 601,  23,  63, 398, 662, 139,  63, 545, 639, 510,   3,\n","       398, 639, 565,  23, 398, 601, 398, 227, 227, 522, 398, 522, 686,\n","       139,  23, 139, 479,  23, 227,  63, 322, 139,  63,  63, 398, 398,\n","       398,  63, 139, 565, 139,  23, 227, 662, 139, 139, 227, 565,   3,\n","       139, 227, 601, 322,  63, 545, 639, 601, 510, 479, 662,   3,  23,\n","       446, 639, 545,   3, 446, 398,  63, 227, 601, 227, 139, 139, 227,\n","       322, 139,   3, 398, 139,  23, 139, 479, 479,  63, 398,  63, 639,\n","       139, 227, 139, 227,  63,  63, 322, 639, 139,  23, 398, 639, 601,\n","       398, 479, 662, 565,  63, 227, 662, 545,  63, 139, 662, 446, 510,\n","       227], dtype=int32)}\n"]}],"source":["print(grid.best_estimator_)\n","print(grid.best_score_)\n","print(grid.best_params_)\n","print(grid.cv_results_)"]},{"cell_type":"code","execution_count":92,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy scores for each training fold:  [0.9798387096774194, 0.9759036144578314, 0.9839357429718876, 0.9799196787148594, 0.9959839357429718]\n","Accuracy scores for each testing fold:  [0.9682539682539683, 1.0, 0.7903225806451613, 0.9838709677419355, 0.7580645161290323]\n","Traning Mean accuracy score:  0.9831163363129939\n","Testing Mean accuracy score:  0.9001024065540195\n"]}],"source":["opt_gb_clf = GradientBoostingClassifier(learning_rate=0.02, \n","                                        max_depth= 6, \n","                                        subsample=0.5, \n","                                        n_estimators = 70, \n","                                        max_features = 7)\n","\n","\n","opt_gb_clf.fit(X_train, y_train)\n","splitSmote(opt_gb_clf)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["the best until now\n","\n","learning_rate=0.03, \n","                                        max_depth= 6, \n","                                        subsample=0.5, \n","                                        n_estimators = 50, \n","                                        max_features = 7)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 4.4 LogisticRegression"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Before optimization"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy scores for each fold:  [0.9047619047619048, 0.9516129032258065, 0.7903225806451613, 0.9354838709677419, 0.6451612903225806]\n","Mean accuracy score:  0.8454685099846391\n"]}],"source":["#LogisticRegression\n","\n","#Grid search cross validation\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.linear_model import LogisticRegression\n","grid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\",\"elasticnet\"], 'solver'  : ['newton-cg', 'lbfgs', 'liblinear']}\n","logreg=LogisticRegression()\n","lr=GridSearchCV(logreg,grid,cv=10)\n","# lr.fit(X_train,y_train)\n","\n","# a=lr.best_params_\n","# b=lr.best_score_\n","# print(\"tuned hpyerparameters :(best parameters) \",lr.best_params_)\n","# print(\"accuracy :\",lr.best_score_)\n","# print(\"Best estimator: \",lr.best_estimator_ )\n","\n","#y_pred.append(pd.Series(lr.predict(X_test), name='LogisticRegression'))\n","\n","# precision, recall, fscore, _ = score(y_test, lr.predict(X_test), average='weighted')\n","# accuracy = accuracy_score(y_test, lr.predict(X_test))\n","# metrics.append(pd.Series({'precision':precision, 'recall':recall,'fscore':fscore, 'accuracy':accuracy}))\n","\n","#LogisticRegression\n","#from sklearn.linear_model import LogisticRegressionCV\n","#lr= LogisticRegressionCV(Cs=a['C'], penalty=a['penalty'], solver=a['solver']).fit(X_train, y_train)\n","\n","splitSmote (lr)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["After optimization"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#TODO: write LogisticRegression optimization code here"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 4.5 Random Forest"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Before optimization"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\rayoo\\AppData\\Local\\Temp\\ipykernel_45220\\3765096017.py:21: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  model.fit(x_sm, y_sm)\n","C:\\Users\\rayoo\\AppData\\Local\\Temp\\ipykernel_45220\\3765096017.py:21: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  model.fit(x_sm, y_sm)\n","C:\\Users\\rayoo\\AppData\\Local\\Temp\\ipykernel_45220\\3765096017.py:21: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  model.fit(x_sm, y_sm)\n","C:\\Users\\rayoo\\AppData\\Local\\Temp\\ipykernel_45220\\3765096017.py:21: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  model.fit(x_sm, y_sm)\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy scores for each fold:  [0.9365079365079365, 1.0, 0.7903225806451613, 0.9838709677419355, 0.7419354838709677]\n","Mean accuracy score:  0.8905273937532001\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\rayoo\\AppData\\Local\\Temp\\ipykernel_45220\\3765096017.py:21: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  model.fit(x_sm, y_sm)\n"]}],"source":["#RandomForestClassifier\n","RF= RandomForestClassifier(criterion=\"gini\",\n","                           max_depth=8,\n","                           min_samples_split=10,\n","                           random_state= 0)\n","# RF.fit(X_train,y_train)\n","\n","# #y_pred.append(pd.Series(RF.predict(X_test), name='RandomForestClassifier'))\n","\n","# precision, recall, fscore, _ = score(y_test, RF.predict(X_test), average='weighted')\n","# accuracy = accuracy_score(y_test, RF.predict(X_test))\n","# metrics.append(pd.Series({'precision':precision, 'recall':recall,'fscore':fscore, 'accuracy':accuracy}))\n","\n","\n","splitSmote (RF)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["After optimization"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#TODO: Write RandomForest optimization code here"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 5. Training"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 5.1 Before Optimazation "]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["KNN\n","Accuracy scores for each fold:  [0.9365079365079365, 0.9193548387096774, 0.7903225806451613, 0.9516129032258065, 0.7741935483870968]\n","Mean accuracy score:  0.8743983614951356\n","GradientBoosting\n","Accuracy scores for each fold:  [0.9682539682539683, 1.0, 0.7903225806451613, 0.9838709677419355, 0.7419354838709677]\n","Mean accuracy score:  0.8968766001024064\n","LogisticRegression\n","Accuracy scores for each fold:  [0.9047619047619048, 1.0, 0.8225806451612904, 0.967741935483871, 0.6612903225806451]\n","Mean accuracy score:  0.8712749615975423\n","RandomForest\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\rayoo\\AppData\\Local\\Temp\\ipykernel_45220\\3765096017.py:21: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  model.fit(x_sm, y_sm)\n","C:\\Users\\rayoo\\AppData\\Local\\Temp\\ipykernel_45220\\3765096017.py:21: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  model.fit(x_sm, y_sm)\n","C:\\Users\\rayoo\\AppData\\Local\\Temp\\ipykernel_45220\\3765096017.py:21: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  model.fit(x_sm, y_sm)\n","C:\\Users\\rayoo\\AppData\\Local\\Temp\\ipykernel_45220\\3765096017.py:21: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  model.fit(x_sm, y_sm)\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy scores for each fold:  [0.9523809523809523, 1.0, 0.7903225806451613, 0.967741935483871, 0.7741935483870968]\n","Mean accuracy score:  0.8969278033794164\n","DecisionTree\n","Accuracy scores for each fold:  [0.9841269841269841, 0.967741935483871, 0.7903225806451613, 0.9516129032258065, 0.7096774193548387]\n","Mean accuracy score:  0.8806963645673322\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\rayoo\\AppData\\Local\\Temp\\ipykernel_45220\\3765096017.py:21: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  model.fit(x_sm, y_sm)\n"]}],"source":["# Base models \n","kCls=KNeighborsClassifier()\n","print(\"KNN\")\n","splitSmote (kCls)\n","\n","gb_clf2 = GradientBoostingClassifier()\n","print(\"GradientBoosting\")\n","splitSmote (gb_clf2)\n","\n","logreg=LogisticRegression()\n","print(\"LogisticRegression\")\n","splitSmote (logreg)\n","\n","RF= RandomForestClassifier()\n","print(\"RandomForest\")\n","splitSmote (RF)\n","\n","clf=DecisionTreeClassifier()\n","print(\"DecisionTree\")\n","splitSmote (clf)\n","\n","\n","\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### After Optimization"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#TODO: write the code here"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 5. Evaluation"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 5.1 Before Optimization"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["\n","def report( y_test, pred ):\n","    #report \n","    print(classification_report(y_test,pred,target_names=['0','1','2','3','4','5']))\n","\n","\n","def confusionMatrix():\n","    #confusion_matrix\n","    #the result will show how mwny sucessful predition and wrong from each class\n","\n","    cm = confusion_matrix(y_test, pred)\n","    plt.figure(figsize=(10,7))\n","\n","    sns.heatmap(cm, annot=True, fmt='.2g', cmap='Blues')\n","\n","    # TN   FP\n","    # FN   TP"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"ename":"TypeError","evalue":"first argument must be an iterable of pandas objects, you passed an object of type \"DataFrame\"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m metrics \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mconcat(metrics, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,names\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mDecisionTree\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mDecisionTreeOpt\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mKNN\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mGradientBoosting\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mLogisticRegression\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mRandomForest\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m      2\u001b[0m metrics\u001b[39m.\u001b[39mcolumns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mDecisionTree\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mDecisionTreeOpt\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mKNN\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mGradientBoosting\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mLogisticRegression\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mRandomForest\u001b[39m\u001b[39m'\u001b[39m]\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/reshape/concat.py:368\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[39m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, allowed_args\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mobjs\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    147\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconcat\u001b[39m(\n\u001b[1;32m    148\u001b[0m     objs: Iterable[NDFrame] \u001b[39m|\u001b[39m Mapping[HashableT, NDFrame],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    157\u001b[0m     copy: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    158\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m    159\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[39m    Concatenate pandas objects along a particular axis.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[39m    1   3   4\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 368\u001b[0m     op \u001b[39m=\u001b[39m _Concatenator(\n\u001b[1;32m    369\u001b[0m         objs,\n\u001b[1;32m    370\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[1;32m    371\u001b[0m         ignore_index\u001b[39m=\u001b[39;49mignore_index,\n\u001b[1;32m    372\u001b[0m         join\u001b[39m=\u001b[39;49mjoin,\n\u001b[1;32m    373\u001b[0m         keys\u001b[39m=\u001b[39;49mkeys,\n\u001b[1;32m    374\u001b[0m         levels\u001b[39m=\u001b[39;49mlevels,\n\u001b[1;32m    375\u001b[0m         names\u001b[39m=\u001b[39;49mnames,\n\u001b[1;32m    376\u001b[0m         verify_integrity\u001b[39m=\u001b[39;49mverify_integrity,\n\u001b[1;32m    377\u001b[0m         copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m    378\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[1;32m    379\u001b[0m     )\n\u001b[1;32m    381\u001b[0m     \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39mget_result()\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/reshape/concat.py:403\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    390\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    391\u001b[0m     objs: Iterable[NDFrame] \u001b[39m|\u001b[39m Mapping[HashableT, NDFrame],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    400\u001b[0m     sort\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    401\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(objs, (ABCSeries, ABCDataFrame, \u001b[39mstr\u001b[39m)):\n\u001b[0;32m--> 403\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    404\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mfirst argument must be an iterable of pandas \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    405\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mobjects, you passed an object of type \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(objs)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    406\u001b[0m         )\n\u001b[1;32m    408\u001b[0m     \u001b[39mif\u001b[39;00m join \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mouter\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    409\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintersect \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n","\u001b[0;31mTypeError\u001b[0m: first argument must be an iterable of pandas objects, you passed an object of type \"DataFrame\""]}],"source":["metrics = pd.concat(metrics, axis=1,names=['DecisionTree','DecisionTreeOpt','KNN','GradientBoosting','LogisticRegression','RandomForest'])\n"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>DecisionTree</th>\n","      <th>DecisionTreeOpt</th>\n","      <th>KNN</th>\n","      <th>GradientBoosting</th>\n","      <th>LogisticRegression</th>\n","      <th>RandomForest</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>precision</th>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.733490</td>\n","      <td>0.963374</td>\n","      <td>0.961147</td>\n","      <td>0.944874</td>\n","    </tr>\n","    <tr>\n","      <th>recall</th>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.776596</td>\n","      <td>0.968085</td>\n","      <td>0.968085</td>\n","      <td>0.968085</td>\n","    </tr>\n","    <tr>\n","      <th>fscore</th>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.749230</td>\n","      <td>0.963933</td>\n","      <td>0.962621</td>\n","      <td>0.954647</td>\n","    </tr>\n","    <tr>\n","      <th>accuracy</th>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.776596</td>\n","      <td>0.968085</td>\n","      <td>0.968085</td>\n","      <td>0.968085</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["           DecisionTree  DecisionTreeOpt       KNN  GradientBoosting  \\\n","precision           1.0              1.0  0.733490          0.963374   \n","recall              1.0              1.0  0.776596          0.968085   \n","fscore              1.0              1.0  0.749230          0.963933   \n","accuracy            1.0              1.0  0.776596          0.968085   \n","\n","           LogisticRegression  RandomForest  \n","precision            0.961147      0.944874  \n","recall               0.968085      0.968085  \n","fscore               0.962621      0.954647  \n","accuracy             0.968085      0.968085  "]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["metrics.columns=['DecisionTree','DecisionTreeOpt','KNN','GradientBoosting','LogisticRegression','RandomForest']\n","metrics"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 5.2 After Optimization"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#TODO: write the code here"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
