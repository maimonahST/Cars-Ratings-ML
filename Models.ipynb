{"cells":[{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","import seaborn as sns\n","import matplotlib.pylab as plt\n","from imblearn.over_sampling import SMOTE\n","#%matplotlib inline\n","\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn import preprocessing\n","from sklearn.preprocessing import OrdinalEncoder\n","from sklearn.impute import SimpleImputer\n","from sklearn.model_selection import StratifiedKFold\n","\n","from scipy.stats import norm\n","from scipy import stats\n","from sklearn.impute import KNNImputer\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import precision_recall_fscore_support as score\n","\n","#DS\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import GridSearchCV, cross_val_score\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import classification_report\n","\n","#KNN\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","#GradientBoostingClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","\n","#LogisticRegressionCV\n","from sklearn.linear_model import LogisticRegressionCV\n","\n","#RandomForestClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","\n","def warn(*args, **kwargs):\n","    pass\n","import warnings\n","warnings.warn = warn"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 1. Loading the dataset"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["#loading the data from CSV file \n","data=pd.read_csv('final_Binary.csv')\n","data.head()\n","\n","'''dataFeatures= ['Side Chest Airbag-Driver', 'Side Chest Airbag-Passenger',\n","       'AEB Vulnerable Road Users', 'Side Head Airbag-Driver',\n","       'Side Head Airbag-Passenger', 'Seatbelt Reminder-Passenger',\n","       'AEB Car-to-Car', 'Belt Loadlimiter-Rear', 'Belt Pretensioner-Rear',\n","       'Side Head Airbag-Rear', 'Lane Assist System', 'Seatbelt Reminder-Rear',\n","       'Safety Assist', 'Speed Assistance', 'Adult Occupant',\n","       'Centre Airbag-Driver', 'Child Occupant', 'Tested Model',\n","       'Isofix/i-Size-Passenger'] '''\n","\n","dataFeatures= ['Adult Occupant', 'Belt Pretensioner-Rear', 'Safety Assist',\n","       'Child Occupant', 'Class', 'Side Pelvis Airbag-Rear',\n","       'AEB Vulnerable Road Users']\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/plain":["Index(['Adult Occupant', 'Belt Pretensioner-Rear', 'Safety Assist',\n","       'Child Occupant', 'Class', 'Side Pelvis Airbag-Rear',\n","       'AEB Vulnerable Road Users', 'Rate'],\n","      dtype='object')"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# print the columns in the dataset\n","data.columns"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":["'# Create a MinMaxScaler object for numrical data\\nscaler = MinMaxScaler()\\n\\n# Scaling the raw input features \\nfeature_cols=data.columns[:-1]\\nX= scaler.fit_transform(data[feature_cols])\\n\\nprint(f\"The range of feature inputs are within {X.min()} to {X.max()}\")'"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["'''# Create a MinMaxScaler object for numrical data\n","scaler = MinMaxScaler()\n","\n","# Scaling the raw input features \n","feature_cols=data.columns[:-1]\n","X= scaler.fit_transform(data[feature_cols])\n","\n","print(f\"The range of feature inputs are within {X.min()} to {X.max()}\")'''"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 2. Split the dataset "]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Training dataset shape, X_train: (217, 7), y_train: (217,)\n","Testing dataset shape, X_test: (94, 7), y_test: (94,)\n"]}],"source":["\n","from sklearn.model_selection import StratifiedShuffleSplit\n","\n","feature_cols=data.columns[:-1]\n","# Get the split indexes\n","strat_shuf_split = StratifiedShuffleSplit(n_splits=1, \n","                                          test_size=0.3, random_state=0)\n","\n","train_idx, test_idx = next(strat_shuf_split.split(data[feature_cols], data['Rate']))\n","\n","# Create the dataframes\n","\n","\n","X_train = data.loc[train_idx, dataFeatures]\n","y_train = data.loc[train_idx, 'Rate']\n","\n","X_test  = data.loc[test_idx, dataFeatures]\n","y_test  = data.loc[test_idx, 'Rate']\n","\n","print(f\"Training dataset shape, X_train: {X_train.shape}, y_train: {y_train.shape}\")\n","print(f\"Testing dataset shape, X_test: {X_test.shape}, y_test: {y_test.shape}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 3. Smoot "]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["X= data.loc[:,dataFeatures]\n","y= data.loc[:,[\"Rate\"]]\n","\n","def splitSmote (model):\n","    # Initialize the Stratified K-fold Cross-validator with 5 splits\n","    sk=StratifiedKFold(n_splits=5)\n","\n","    # Initialize the array to store the accuracy scores\n","    tr_accuracy_scores = []\n","    tst_accuracy_scores = []\n","\n","    # Perform cross-validation\n","    for train_index, test_index in sk.split(X, y):\n","        # Split the data into training and test sets\n","        x_train_fold, x_test_fold = X.loc[train_index,:], X.loc[test_index,:]\n","        y_train_fold, y_test_fold = y.loc[train_index,:], y.loc[test_index,:]\n","\n","        #smote = SMOTE(sampling_strategy='minority')\n","        smote = SMOTE(sampling_strategy=0.5)\n","        x_sm, y_sm = smote.fit_resample(x_train_fold, y_train_fold)\n","        #Fit the model to the training data\n","        model.fit(x_sm, np.ravel(y_sm))\n","        # Make predictions on the test data\n","        yTrain_pred = model.predict(x_train_fold)\n","        yTest_pred = model.predict(x_test_fold)\n","\n","        # Calculate the accuracy score and append it to the list\n","        tr_accuracy_scores.append(accuracy_score(y_train_fold, yTrain_pred))\n","        tst_accuracy_scores.append(accuracy_score(y_test_fold, yTest_pred))\n","\n","\n","\n","    # Print the accuracy scores for each fold\n","    print(\"Accuracy scores for each training fold: \", tr_accuracy_scores)\n","    print(\"Accuracy scores for each testing fold: \", tst_accuracy_scores)\n","\n","    # Calculate the mean accuracy scores\n","    print(\"Traning Mean accuracy score: \", np.mean(tr_accuracy_scores))\n","    print(\"Testing Mean accuracy score: \", np.mean(tst_accuracy_scores))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 4. Data normalization "]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":["1    0.718894\n","0    0.281106\n","Name: Rate, dtype: float64"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["y_train.value_counts(normalize=True)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":["1    0.723404\n","0    0.276596\n","Name: Rate, dtype: float64"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["y_test.value_counts(normalize=True)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 4. Models "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 4.1 Decision Tree "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["before optimization"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy scores for each training fold:  [1.0, 1.0, 1.0, 1.0, 1.0]\n","Accuracy scores for each testing fold:  [0.9682539682539683, 0.967741935483871, 0.7903225806451613, 0.9838709677419355, 0.7258064516129032]\n","Traning Mean accuracy score:  1.0\n","Testing Mean accuracy score:  0.8871991807475679\n"]}],"source":["# DecisionTreeClassifier\n","#importing the classfier\n","metrics=[]\n","\n","clf=DecisionTreeClassifier(random_state=0)\n","#clf2=clf.fit(X_train,y_train)\n","\n","\n","#y_pred.append(pd.Series(clf2.predict(X_test), name='DecisionTreeClassifier'))\n","# Preciision, recall, f-score from the multi-class support function\n","\n","# precision, recall, fscore, _ = score(y_test, clf2.predict(X_test), average='weighted')\n","# accuracy = accuracy_score(y_test, clf2.predict(X_test))\n","# metrics.append(pd.Series({'precision':precision, 'recall':recall,'fscore':fscore, 'accuracy':accuracy}))\n","\n","\n","splitSmote (clf)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["After optimization"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# DecisionTree opt\n","\n","\n","#optimization\n","param_grid={\n","\"max_depth\":[2,4,6],\n","\"min_samples_split\":[2,5,10],\n","\"min_samples_leaf\":[1,2,4]}\n","grid_search= GridSearchCV(estimator=clf,param_grid=param_grid,cv=5)\n","\n","# grid_search.fit(X_train,y_train)\n","# print(\"Best hyper-param: \",grid_search.best_params_ )\n","# print(\"Best estimator: \",grid_search.best_estimator_ )\n","# print(\"Best score: \",grid_search.best_score_ )\n","\n","\n","# #precision, recall, fscore, _ = score(y_test, grid_search.predict(X_test), average='weighted')\n","# accuracy = accuracy_score(y_test, grid_search.predict(X_test))\n","# metrics.append(pd.Series({'precision':precision, 'recall':recall,'fscore':fscore, 'accuracy':accuracy}))\n","\n","\n","#splitSmote (kCls)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["grid_search\n","Accuracy scores for each training fold:  [0.9717741935483871, 0.9558232931726908, 0.9879518072289156, 0.9598393574297188, 0.9759036144578314]\n","Accuracy scores for each testing fold:  [0.9841269841269841, 0.9516129032258065, 0.7903225806451613, 1.0, 0.7096774193548387]\n","Traning Mean accuracy score:  0.9702584531675088\n","Testing Mean accuracy score:  0.8871479774705582\n"]}],"source":["#DT aftar opt\n","\n","print(\"grid_search\")\n","\n","splitSmote(grid_search)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 4.2 KNN"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Before optimization"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy scores for each training fold:  [0.9395161290322581, 0.9236947791164659, 0.9678714859437751, 0.9518072289156626, 0.9598393574297188]\n","Accuracy scores for each testing fold:  [0.9365079365079365, 0.9032258064516129, 0.7903225806451613, 0.967741935483871, 0.8064516129032258]\n","Traning Mean accuracy score:  0.9485457960875762\n","Testing Mean accuracy score:  0.8808499743983615\n"]}],"source":["# KNN\n","#importing the classfier\n","\n","kCls=KNeighborsClassifier()\n","splitSmote (kCls)\n","#kCls.fit(X_train,y_train)\n","\n","#y_pred.append(pd.Series(kCls.predict(X_test), name='KNeighborsClassifier'))\n","\n","#precision, recall, fscore, _ = score(y_test, kCls.predict(X_test), average='weighted')\n","#accuracy = accuracy_score(y_test, kCls.predict(X_test))\n","#metrics.append(pd.Series({'precision':precision, 'recall':recall,'fscore':fscore, 'accuracy':accuracy}))\n","\n","#kCls=KNeighborsClassifier(n_neighbors=9)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["After optimization"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["#TODO: Write the code here"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 4.3 GradientBoosting"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Before optimization"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy scores for each training fold:  [1.0, 0.9959839357429718, 1.0, 0.9959839357429718, 1.0]\n","Accuracy scores for each testing fold:  [0.9682539682539683, 1.0, 0.7903225806451613, 0.9838709677419355, 0.7419354838709677]\n","Traning Mean accuracy score:  0.9983935742971888\n","Testing Mean accuracy score:  0.8968766001024064\n"]}],"source":["# GradientBoostingClassifier\n","#importing the classfier\n","\n","gb_clf = GradientBoostingClassifier()\n","splitSmote(gb_clf)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["After optimization"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy scores for each training fold:  [0.9919354838709677, 0.9357429718875502, 0.9759036144578314, 0.9076305220883534, 1.0]\n","Accuracy scores for each testing fold:  [0.8888888888888888, 0.967741935483871, 0.7903225806451613, 0.9193548387096774, 0.7258064516129032]\n","Traning Mean accuracy score:  0.9622425184609404\n","Testing Mean accuracy score:  0.8584229390681004\n"]}],"source":["# optimizing the GradientBoostingClassifier using RandomizedSearchCV\n","from sklearn.model_selection import GridSearchCV\n","parameters = {'n_estimators' : [20,50,70,100], \n","                'learning_rate' : [0.01, 0.02, 0.03, 0.04], \n","                'max_depth' : [4,6,8,10],\n","                'subsample' : [0.9, 0.7 ,0.5 , 0.2 ],\n","                'max_features' :[3,5,7]\n","                }\n","\n","grid = GridSearchCV(estimator= gb_clf, \n","                    param_grid= parameters, \n","                    cv= 2,\n","                    n_jobs= -1)\n","\n","\n","grid.fit(X_train, y_train)\n","splitSmote(grid)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["GradientBoostingClassifier(learning_rate=0.03, max_depth=10, max_features=7,\n","                           n_estimators=50, subsample=0.5)\n","0.9555555555555555\n","{'learning_rate': 0.03, 'max_depth': 10, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.5}\n","{'mean_fit_time': array([0.03366113, 0.03366113, 0.03366137, 0.03282058, 0.08698392,\n","       0.09165919, 0.08392   , 0.10865557, 0.1333636 , 0.12502873,\n","       0.18311083, 0.1080035 , 0.22033167, 0.21404874, 0.21664703,\n","       0.15798306, 0.04161191, 0.03761387, 0.03843236, 0.03364956,\n","       0.08798349, 0.08682549, 0.1039784 , 0.074736  , 0.1290133 ,\n","       0.1586473 , 0.1462847 , 0.13832605, 0.17935586, 0.18728554,\n","       0.17457378, 0.23766184, 0.03341353, 0.04157186, 0.06230128,\n","       0.03730798, 0.10393441, 0.07867301, 0.10792136, 0.07870579,\n","       0.15336263, 0.10895145, 0.11232948, 0.16425502, 0.16823101,\n","       0.18632019, 0.27920461, 0.24617243, 0.06559062, 0.07358515,\n","       0.07994664, 0.04796791, 0.21806026, 0.16100621, 0.18208361,\n","       0.11635172, 0.33365035, 0.20930088, 0.2936765 , 0.16133726,\n","       0.46682811, 0.31423879, 0.37311065, 0.32913756, 0.04796863,\n","       0.05596054, 0.05697024, 0.05596256, 0.13759851, 0.15953898,\n","       0.20205414, 0.17357409, 0.17336667, 0.17872131, 0.23808062,\n","       0.15910494, 0.23606157, 0.31567705, 0.31272757, 0.2549051 ,\n","       0.0421195 , 0.03798652, 0.04185724, 0.03333521, 0.09206092,\n","       0.11197579, 0.09998369, 0.11666429, 0.13729262, 0.1562351 ,\n","       0.15564716, 0.13498235, 0.22494948, 0.23294449, 0.28529453,\n","       0.2498095 , 0.05074   , 0.0700264 , 0.06919253, 0.04255295,\n","       0.22724152, 0.2026211 , 0.25596726, 0.1325413 , 0.39362884,\n","       0.29257214, 0.32566929, 0.17028868, 0.41592586, 0.56798816,\n","       0.44889951, 0.27990162, 0.0456773 , 0.05773878, 0.05666065,\n","       0.04796827, 0.14721322, 0.22834516, 0.23296058, 0.1192323 ,\n","       0.18508983, 0.2087388 , 0.20835912, 0.15977716, 0.38200867,\n","       0.34884501, 0.38819861, 0.24671352, 0.04596806, 0.03725648,\n","       0.03741527, 0.06531906, 0.17959023, 0.1292758 , 0.14764738,\n","       0.15563226, 0.14363945, 0.16586196, 0.19605041, 0.17953753,\n","       0.32462323, 0.31263173, 0.36382401, 0.23032773, 0.0733037 ,\n","       0.07281065, 0.10470331, 0.05996001, 0.1833663 , 0.22329926,\n","       0.25667262, 0.15061522, 0.29152012, 0.47953129, 0.33500135,\n","       0.20122242, 0.65353525, 0.72885275, 0.46811604, 0.3222816 ,\n","       0.06182504, 0.06274188, 0.08794153, 0.04396987, 0.12854004,\n","       0.15892291, 0.21256518, 0.12941909, 0.18798518, 0.30528355,\n","       0.2352246 , 0.16917896, 0.28329599, 0.45820463, 0.36225772,\n","       0.32094073, 0.03331625, 0.04126823, 0.04526484, 0.05401075,\n","       0.11596906, 0.13398707, 0.12455058, 0.16298354, 0.1449182 ,\n","       0.22902417, 0.19169807, 0.15827608, 0.22459972, 0.26660383,\n","       0.29195917, 0.29595661, 0.05408144, 0.03737462, 0.03330541,\n","       0.04459441, 0.08392513, 0.11202562, 0.08326781, 0.09534228,\n","       0.14130521, 0.12066734, 0.128631  , 0.14110029, 0.22243679,\n","       0.18366802, 0.17643857, 0.17310846, 0.0366993 , 0.04785943,\n","       0.03622723, 0.03272843, 0.1235044 , 0.09134722, 0.09956324,\n","       0.07866716, 0.1224612 , 0.12910652, 0.12151384, 0.11311734,\n","       0.24429405, 0.19564533, 0.1817857 , 0.17778635, 0.03703988,\n","       0.03733861, 0.03671062, 0.02639985, 0.07959366, 0.0858444 ,\n","       0.12128294, 0.07731283, 0.11259043, 0.15465486, 0.11326873,\n","       0.13643169, 0.15999103, 0.1726588 , 0.20228255, 0.20760632,\n","       0.05067241, 0.05436361, 0.08360338, 0.05433106, 0.15398228,\n","       0.14161849, 0.14060342, 0.10024655, 0.25822783, 0.25796139,\n","       0.20427942, 0.19229996, 0.28730977, 0.29130745, 0.34202373,\n","       0.20497036, 0.04596603, 0.06197464, 0.03799117, 0.04999471,\n","       0.10294473, 0.1290586 , 0.14638901, 0.1250664 , 0.17971194,\n","       0.176754  , 0.17416549, 0.16283584, 0.32538247, 0.27665234,\n","       0.25983453, 0.24732876, 0.06630814, 0.06616867, 0.03737688,\n","       0.05368328, 0.13066518, 0.13330817, 0.13330793, 0.12935877,\n","       0.14895844, 0.17794371, 0.21120977, 0.17794514, 0.2291882 ,\n","       0.24554706, 0.31247711, 0.22482646, 0.06665742, 0.06657052,\n","       0.09168863, 0.0580107 , 0.25973213, 0.23535073, 0.20737064,\n","       0.14194942, 0.28407061, 0.29606187, 0.27339888, 0.21314514,\n","       0.43316066, 0.39716959, 0.40192819, 0.26456773, 0.04478562,\n","       0.0457654 , 0.05811036, 0.04176962, 0.13722348, 0.15796494,\n","       0.16666448, 0.12931824, 0.20394218, 0.24595392, 0.23719323,\n","       0.1666292 , 0.27894139, 0.35843325, 0.32184029, 0.24233377,\n","       0.0376581 , 0.04996765, 0.04600739, 0.04167628, 0.09604943,\n","       0.1252197 , 0.12789643, 0.10827136, 0.14150465, 0.22123003,\n","       0.237679  , 0.16543889, 0.2540338 , 0.28272963, 0.31139386,\n","       0.2521168 , 0.06911922, 0.08140492, 0.08516729, 0.05399883,\n","       0.18302667, 0.26226389, 0.21694851, 0.14636767, 0.29404902,\n","       0.33064151, 0.29353762, 0.19556725, 0.5082531 , 0.63717914,\n","       0.45573556, 0.27856755, 0.04405439, 0.05675125, 0.06244707,\n","       0.0421164 , 0.15820718, 0.19190907, 0.17890394, 0.12920284,\n","       0.30437839, 0.47137034, 0.31302094, 0.22049952, 0.46707094,\n","       0.71534681, 0.5227443 , 0.34723067, 0.07897246, 0.08762157,\n","       0.05432022, 0.06194162, 0.14132631, 0.24625123, 0.20866704,\n","       0.22208381, 0.20856178, 0.24906337, 0.27014315, 0.28368211,\n","       0.28328419, 0.51226223, 0.47924352, 0.28481078, 0.03997171,\n","       0.05829883, 0.06631565, 0.03727353, 0.11223447, 0.12025571,\n","       0.14128625, 0.07562268, 0.12930083, 0.13798618, 0.17102659,\n","       0.12532938, 0.23802543, 0.26293242, 0.18371642, 0.17867291,\n","       0.03370297, 0.04636443, 0.03302813, 0.03770077, 0.11299002,\n","       0.09127748, 0.11258638, 0.09190905, 0.11260951, 0.12129283,\n","       0.14965093, 0.14566755, 0.22040033, 0.22860456, 0.17964697,\n","       0.20795059, 0.0373261 , 0.03733528, 0.04095566, 0.03328741,\n","       0.08730888, 0.09997964, 0.10863435, 0.07932866, 0.10332274,\n","       0.13299274, 0.1169734 , 0.14567125, 0.19957829, 0.23070276,\n","       0.1629442 , 0.18773127, 0.04568148, 0.05399537, 0.04999816,\n","       0.04134727, 0.18034363, 0.13609385, 0.12637031, 0.09979916,\n","       0.22500551, 0.27730286, 0.27519643, 0.21121478, 0.30392313,\n","       0.28402686, 0.27137303, 0.3123014 , 0.05401599, 0.0521574 ,\n","       0.06396198, 0.05278671, 0.13341427, 0.12476075, 0.15831137,\n","       0.12121868, 0.18326187, 0.26260221, 0.22066844, 0.14496148,\n","       0.29156148, 0.39182484, 0.38244987, 0.32913411, 0.04410672,\n","       0.05535614, 0.05449855, 0.03780258, 0.10791695, 0.16617775,\n","       0.11657357, 0.09963858, 0.14558589, 0.16288614, 0.20018315,\n","       0.14199305, 0.2329489 , 0.32557023, 0.38428628, 0.32729435,\n","       0.06265414, 0.06477618, 0.06664228, 0.04999077, 0.27861547,\n","       0.19528604, 0.18772817, 0.11829042, 0.30356574, 0.2752372 ,\n","       0.32720304, 0.16361284, 0.37575221, 0.47738111, 0.43740475,\n","       0.31179166, 0.03997326, 0.04397047, 0.05196595, 0.03997362,\n","       0.12791729, 0.16098297, 0.144328  , 0.14559996, 0.15759158,\n","       0.21355426, 0.2673099 , 0.16661382, 0.29162991, 0.42707276,\n","       0.42707312, 0.33513296, 0.03597546, 0.04397142, 0.05047846,\n","       0.03863883, 0.10172319, 0.11776042, 0.1140703 , 0.11748743,\n","       0.14546824, 0.21741998, 0.17267489, 0.1479013 , 0.28780615,\n","       0.33177936, 0.26782227, 0.21985364, 0.05596244, 0.06795418,\n","       0.07195318, 0.05596304, 0.22385073, 0.25982738, 0.22784805,\n","       0.15589595, 0.25974464, 0.3064121 , 0.27443361, 0.21795702,\n","       0.52974117, 0.45174885, 0.50778937, 0.31282604, 0.04619122,\n","       0.066957  , 0.04115272, 0.0413543 , 0.12099624, 0.1582706 ,\n","       0.22892988, 0.14559948, 0.19094193, 0.30494523, 0.21601248,\n","       0.18331981, 0.27173424, 0.46402287, 0.35363233, 0.2356801 ,\n","       0.04998338, 0.04868257, 0.04402483, 0.03197885, 0.13301742,\n","       0.12901962, 0.15300429, 0.14769232, 0.13191211, 0.19033456,\n","       0.24229991, 0.15435803, 0.27610469, 0.36173809, 0.30808246,\n","       0.2401284 , 0.04397082, 0.03197849, 0.03597689, 0.03197849,\n","       0.09223211, 0.08577013, 0.08976746, 0.07377744, 0.16846955,\n","       0.12449884, 0.12050128, 0.11650407, 0.21643722, 0.20444489,\n","       0.16447258, 0.2384938 , 0.03597665, 0.03597569, 0.03597629,\n","       0.05996001, 0.10513842, 0.0879426 , 0.08394408, 0.09475815,\n","       0.11678147, 0.11678314, 0.11678433, 0.18074107, 0.22471225,\n","       0.17298567, 0.24783528, 0.15189898, 0.03597605, 0.03997314,\n","       0.03597605, 0.03197861, 0.07594907, 0.10393059, 0.07994676,\n","       0.09593582, 0.10393047, 0.11592245, 0.15334177, 0.10453594,\n","       0.22325408, 0.16933215, 0.22929251, 0.16533458, 0.07587016,\n","       0.05188656, 0.05425489, 0.07195175, 0.15589607, 0.16788805,\n","       0.13590968, 0.10792816, 0.22535813, 0.18938243, 0.19737637,\n","       0.14540803, 0.27858019, 0.27458227, 0.25059855, 0.25059831,\n","       0.03723001, 0.07320929, 0.06921101, 0.06121647, 0.11192608,\n","       0.15589631, 0.15189862, 0.13990641, 0.17588305, 0.17588282,\n","       0.1798805 , 0.17588305, 0.37590241, 0.25982714, 0.30925405,\n","       0.21185899, 0.03597748, 0.05995989, 0.03597522, 0.03997326,\n","       0.09593523, 0.16135168, 0.16073132, 0.16534972, 0.12966907,\n","       0.19762361, 0.17012894, 0.15069175, 0.21884584, 0.23421335,\n","       0.28145623, 0.20986509, 0.05596185, 0.07802188, 0.06130838,\n","       0.05355036, 0.22401059, 0.22905493, 0.19187236, 0.12791574,\n","       0.30617976, 0.29230654, 0.32616591, 0.19824982, 0.38822436,\n","       0.4307121 , 0.42970562, 0.28308797, 0.04397058, 0.05196571,\n","       0.06720197, 0.05128908, 0.13001287, 0.15405977, 0.14580524,\n","       0.13963342, 0.18147635, 0.24868357, 0.2414366 , 0.16389394,\n","       0.29180574, 0.32378399, 0.29180539, 0.2198534 , 0.03197908,\n","       0.04397106, 0.04397023, 0.0399735 , 0.11992049, 0.12791514,\n","       0.12391758, 0.09993315, 0.17588353, 0.19986665, 0.22901487,\n","       0.1638912 , 0.2464962 , 0.34174597, 0.33374953, 0.27546847,\n","       0.05581796, 0.08990538, 0.07711625, 0.06145346, 0.18523932,\n","       0.2891705 , 0.28037715, 0.1397754 , 0.27901816, 0.36473727,\n","       0.39999342, 0.1801033 , 0.49748456, 0.70154333, 0.68201983,\n","       0.46976054, 0.04950774, 0.09787285, 0.07963741, 0.05393088,\n","       0.14563978, 0.21708918, 0.19172108, 0.23348737, 0.21607959,\n","       0.43237126, 0.39549959, 0.19217229, 0.44226515, 0.57138193,\n","       0.49984193, 0.28731775, 0.04970026, 0.07096064, 0.0623455 ,\n","       0.04971194, 0.10431087, 0.16260576, 0.13328338, 0.15854502,\n","       0.14933884, 0.20401883, 0.20176399, 0.16606462, 0.23468137,\n","       0.29138422, 0.30462325, 0.25196433]), 'std_fit_time': array([0.00000000e+00, 0.00000000e+00, 2.38418579e-07, 8.83638859e-03,\n","       3.99708748e-03, 8.67235661e-03, 0.00000000e+00, 6.69014454e-03,\n","       6.61730766e-04, 1.00266933e-03, 1.72257423e-04, 6.83188438e-04,\n","       3.71265411e-03, 1.12904310e-02, 1.59612894e-02, 1.24955177e-03,\n","       3.09228897e-04, 4.30607796e-03, 4.13656235e-03, 6.46948814e-04,\n","       4.71818447e-03, 4.43542004e-03, 1.26848221e-02, 3.44514847e-05,\n","       1.24146938e-02, 8.80217552e-03, 1.30976439e-02, 3.75568867e-03,\n","       3.34143639e-03, 3.99625301e-03, 7.20858574e-04, 2.97563076e-02,\n","       7.52210617e-05, 2.84910202e-04, 3.65436077e-03, 3.97896767e-03,\n","       3.96573544e-03, 4.64165211e-03, 2.00271606e-05, 4.03380394e-03,\n","       5.24628162e-03, 7.60185719e-03, 4.39620018e-03, 1.48948431e-02,\n","       2.04229355e-03, 2.44128704e-03, 2.25181580e-02, 3.03161144e-02,\n","       7.24363327e-03, 8.74674320e-03, 7.99429417e-03, 0.00000000e+00,\n","       2.17804909e-02, 1.48758888e-02, 2.57775784e-02, 1.15638971e-02,\n","       1.82533264e-02, 7.41159916e-03, 1.89656019e-02, 1.38820410e-02,\n","       1.57384872e-02, 1.50589943e-02, 7.39301443e-02, 2.19633579e-02,\n","       2.38418579e-07, 7.99691677e-03, 1.00815296e-03, 7.99465179e-03,\n","       6.30569458e-03, 1.11985207e-02, 9.18447971e-03, 2.16752291e-02,\n","       3.44947577e-02, 1.31486654e-02, 1.40200853e-02, 2.58442163e-02,\n","       4.40623760e-02, 8.85283947e-03, 1.27915144e-02, 3.64983082e-03,\n","       8.56685638e-03, 4.61411476e-03, 8.30173492e-04, 3.71932983e-05,\n","       7.89940357e-03, 3.96239758e-03, 9.35530663e-03, 5.22971153e-04,\n","       1.33848190e-02, 2.24745274e-03, 2.03131437e-02, 1.68728828e-03,\n","       2.62964964e-02, 5.02815247e-02, 2.11396217e-02, 1.23624802e-02,\n","       5.22279739e-03, 1.79147720e-02, 2.75886059e-03, 2.57968903e-03,\n","       2.05938816e-02, 2.75552273e-03, 1.13264322e-02, 1.13629103e-02,\n","       5.88736534e-02, 3.20154428e-02, 2.29008198e-02, 1.03954077e-02,\n","       7.98094273e-03, 5.93698025e-02, 7.82158375e-02, 1.11782551e-03,\n","       4.94825840e-03, 8.92806053e-03, 7.29775429e-03, 1.19209290e-07,\n","       1.38847828e-02, 5.80878258e-02, 2.14933157e-02, 1.89316273e-03,\n","       4.41744328e-02, 1.81345940e-02, 8.53359699e-03, 1.15132332e-03,\n","       4.67602015e-02, 2.95855999e-02, 2.89690495e-02, 1.48233175e-02,\n","       1.27294064e-02, 4.01926041e-03, 3.81755829e-03, 1.73511505e-02,\n","       3.09953690e-02, 1.73518658e-02, 5.35631180e-03, 2.02503204e-02,\n","       1.57252550e-02, 2.89686918e-02, 2.54619122e-02, 9.53912735e-04,\n","       8.58052969e-02, 2.58461237e-02, 6.25947714e-02, 4.21822071e-03,\n","       8.36253166e-03, 8.85272026e-03, 2.32115984e-02, 3.99768353e-03,\n","       1.87582970e-02, 2.81071663e-03, 1.86610222e-03, 1.47058964e-02,\n","       2.11281776e-02, 2.85768509e-03, 3.64896059e-02, 3.06241512e-02,\n","       1.53629661e-01, 7.03182220e-02, 1.70707703e-04, 6.51981831e-02,\n","       1.65700912e-03, 2.17688084e-03, 3.57627869e-07, 3.99827957e-03,\n","       7.36784935e-03, 1.82099342e-02, 3.40468884e-02, 9.49859619e-03,\n","       3.88731956e-02, 4.67021465e-02, 9.82773304e-03, 1.05712414e-02,\n","       2.47195959e-02, 3.20792198e-04, 1.23419762e-02, 1.23548508e-02,\n","       2.72989273e-05, 7.90560246e-03, 3.90899181e-03, 4.83548641e-03,\n","       8.03387165e-03, 6.76989555e-04, 8.70609283e-03, 1.29593611e-02,\n","       1.23271942e-02, 1.29537582e-02, 2.49328613e-02, 7.60602951e-03,\n","       8.77177715e-03, 5.86982965e-02, 4.13380861e-02, 2.13518143e-02,\n","       4.60982323e-03, 4.05824184e-03, 1.12056732e-04, 1.25735998e-02,\n","       3.06367874e-05, 1.19866133e-02, 6.66499138e-04, 1.27137899e-02,\n","       8.03303719e-03, 4.71103191e-03, 3.32820415e-03, 7.79879093e-03,\n","       3.75574827e-02, 9.98616219e-04, 1.09181404e-02, 1.43996477e-02,\n","       4.60243225e-03, 8.21948051e-03, 7.69162178e-03, 6.71482086e-03,\n","       7.46893883e-03, 6.98804855e-04, 8.47208500e-03, 3.98564339e-03,\n","       1.05365515e-02, 9.18602943e-03, 8.78310204e-03, 9.18567181e-03,\n","       5.00396490e-02, 2.75301933e-03, 2.21225023e-02, 1.89709663e-03,\n","       2.93195248e-03, 5.35380840e-03, 3.26931477e-03, 2.41374969e-03,\n","       4.91547585e-03, 5.89764118e-03, 6.63077831e-03, 2.63392925e-03,\n","       7.32910633e-03, 1.32342577e-02, 6.65056705e-03, 1.28931999e-02,\n","       1.05860233e-02, 2.08175182e-03, 1.43460035e-02, 9.00244713e-03,\n","       8.01408291e-03, 4.42230701e-03, 7.67660141e-03, 4.31036949e-03,\n","       3.99839878e-03, 8.40044022e-03, 7.05122948e-04, 3.46779823e-04,\n","       2.15945244e-02, 1.99373960e-02, 2.48749256e-02, 1.82327032e-02,\n","       2.12754011e-02, 1.72786713e-02, 9.26029682e-03, 5.06424904e-03,\n","       4.65929508e-03, 1.26422644e-02, 4.64737415e-03, 8.69131088e-03,\n","       5.03957272e-03, 4.35447693e-03, 2.16848850e-02, 2.43281126e-02,\n","       2.90769339e-02, 1.04241371e-02, 1.57356262e-04, 3.17668915e-03,\n","       7.12704659e-03, 1.73842907e-02, 1.02188587e-02, 2.55060196e-03,\n","       1.70165300e-02, 2.40496397e-02, 3.25250626e-03, 1.30538940e-02,\n","       4.40589190e-02, 7.88998604e-03, 1.01804733e-04, 1.33695602e-02,\n","       2.31366158e-02, 1.12071037e-02, 3.30018997e-03, 1.12085342e-02,\n","       2.93562412e-02, 2.08754539e-02, 6.18016720e-02, 2.14695930e-04,\n","       9.39452648e-03, 7.30633736e-03, 2.51281261e-02, 8.08632374e-03,\n","       2.68791914e-02, 1.00213289e-02, 9.96506214e-03, 4.59432602e-04,\n","       4.04633284e-02, 2.04759836e-02, 1.80816650e-03, 9.50992107e-03,\n","       8.33972692e-02, 2.13208199e-02, 2.72531509e-02, 6.61671162e-03,\n","       3.49748135e-03, 3.12900543e-03, 5.27501106e-04, 7.82072544e-03,\n","       3.02393436e-02, 8.02588463e-03, 1.67042017e-02, 3.98755074e-03,\n","       3.72501612e-02, 3.89587879e-03, 3.94570827e-03, 8.08250904e-03,\n","       3.76293659e-02, 8.25071335e-03, 1.09601021e-02, 9.07480717e-03,\n","       4.34839725e-03, 2.02655792e-06, 4.01139259e-03, 3.59773636e-04,\n","       1.19308233e-02, 1.60001516e-02, 1.77919865e-03, 9.18579102e-03,\n","       8.53097439e-03, 5.16080856e-03, 3.35316658e-02, 8.48197937e-03,\n","       2.51276493e-02, 2.03003883e-02, 5.36090136e-02, 1.42487288e-02,\n","       2.11515427e-02, 6.17909431e-03, 9.14776325e-03, 4.66620922e-03,\n","       3.61489058e-02, 2.03121901e-02, 8.63552094e-04, 7.48622417e-03,\n","       2.25977898e-02, 2.62951851e-03, 6.17289543e-03, 4.52935696e-03,\n","       4.16183472e-02, 7.93113708e-02, 2.24884748e-02, 4.72235680e-03,\n","       4.08208370e-03, 7.20334053e-03, 4.45652008e-03, 1.15156174e-04,\n","       1.70822144e-02, 1.59263611e-04, 3.14295292e-03, 4.18925285e-03,\n","       1.63375139e-02, 1.19134188e-02, 1.76303387e-02, 8.50915909e-04,\n","       4.16241884e-02, 1.37591362e-03, 2.65175104e-02, 5.46698570e-02,\n","       4.31430340e-03, 3.62765789e-03, 3.69942188e-03, 4.66871262e-03,\n","       8.08727741e-03, 4.43974733e-02, 7.19606876e-02, 5.73537350e-02,\n","       3.37358713e-02, 6.00956678e-02, 1.07270479e-02, 2.05855370e-02,\n","       2.81829834e-02, 1.03635669e-01, 2.13103294e-02, 3.25918198e-03,\n","       8.34465027e-07, 9.21440125e-03, 1.19328499e-03, 3.85987759e-03,\n","       1.22641325e-02, 1.23655796e-02, 8.66472721e-03, 2.85983086e-04,\n","       3.36217880e-03, 4.05359268e-03, 3.84186506e-02, 7.15851784e-04,\n","       5.47344685e-02, 5.37663698e-02, 8.40079784e-03, 3.88205051e-03,\n","       8.37886333e-03, 1.29774809e-02, 3.59058380e-04, 5.03170490e-03,\n","       4.39274311e-03, 6.22391701e-04, 2.06938982e-02, 1.73867941e-02,\n","       1.26100779e-02, 3.98492813e-03, 8.41486454e-03, 4.35638428e-03,\n","       4.22620773e-03, 2.07154751e-02, 4.54974174e-03, 1.60531998e-02,\n","       3.98659706e-03, 4.18937206e-03, 1.84893608e-04, 1.77621841e-05,\n","       1.26290321e-02, 4.12464142e-05, 1.67104006e-02, 4.71746922e-03,\n","       1.26872063e-02, 1.63316727e-02, 3.27348709e-04, 1.24474764e-02,\n","       7.72094727e-03, 3.66624594e-02, 3.68177891e-03, 4.87303734e-03,\n","       3.70287895e-03, 3.27968597e-03, 7.68542290e-04, 1.10387802e-04,\n","       8.28218460e-03, 6.01649284e-03, 4.45544720e-03, 3.66210938e-04,\n","       3.58939171e-04, 3.52381468e-02, 4.26040888e-02, 3.00879478e-02,\n","       4.14109230e-03, 7.94172287e-03, 1.27398968e-02, 5.43918610e-02,\n","       4.58657742e-03, 1.21784210e-02, 1.46126747e-02, 1.34776831e-02,\n","       2.48353481e-02, 7.99286366e-03, 1.62742138e-02, 2.93834209e-02,\n","       1.13725662e-04, 2.87600756e-02, 2.95242071e-02, 3.70991230e-03,\n","       1.63853168e-03, 6.74306154e-02, 3.39853764e-02, 4.65881824e-03,\n","       3.69524956e-03, 3.01396847e-03, 3.87251377e-03, 4.57775593e-03,\n","       7.94851780e-03, 5.03268242e-02, 8.70418549e-03, 2.44498253e-04,\n","       2.02735662e-02, 1.96139812e-02, 1.77333355e-02, 4.52995300e-06,\n","       4.16611433e-02, 8.30423832e-03, 1.76881552e-02, 1.00340843e-02,\n","       2.88116932e-03, 6.80017471e-03, 5.00679016e-06, 1.13248825e-05,\n","       7.19127655e-02, 3.99613380e-03, 2.88200378e-03, 1.65796280e-03,\n","       4.77373600e-02, 1.14127398e-02, 3.13987732e-02, 3.72028351e-03,\n","       7.99655914e-03, 2.62888670e-02, 1.02978945e-02, 1.59893036e-02,\n","       7.99489021e-03, 1.19925737e-02, 3.99732590e-03, 1.19209290e-07,\n","       1.59866810e-02, 1.08969212e-03, 7.57014751e-03, 1.69527531e-03,\n","       2.16822624e-02, 2.16821432e-02, 1.58941746e-02, 1.27363205e-03,\n","       2.42246389e-02, 2.42106915e-02, 2.37575769e-02, 2.02130079e-02,\n","       3.99756432e-03, 1.19920969e-02, 5.48553467e-03, 1.33371353e-03,\n","       1.37825012e-02, 1.26404762e-02, 1.50045156e-02, 5.56206703e-03,\n","       9.56010818e-03, 4.15366888e-02, 1.27820969e-02, 3.99732590e-03,\n","       4.79692221e-02, 6.79544210e-02, 1.99871063e-02, 3.99708748e-03,\n","       7.99310207e-03, 3.99768353e-03, 2.26497650e-06, 2.38418579e-07,\n","       1.59888268e-02, 1.19926929e-02, 1.99871063e-02, 1.99863911e-02,\n","       6.00426197e-02, 2.24682093e-02, 7.80963898e-03, 8.32414627e-03,\n","       2.05310583e-02, 3.45664024e-02, 7.20524788e-03, 1.21909380e-02,\n","       4.53281403e-03, 8.98170471e-03, 9.17315483e-03, 2.34842300e-05,\n","       2.90079117e-02, 8.35204124e-03, 3.66318226e-03, 4.31931019e-03,\n","       3.36238146e-02, 2.76696682e-02, 1.73325539e-02, 8.66842270e-03,\n","       4.55627441e-02, 4.33819294e-02, 4.98350859e-02, 6.37102127e-03,\n","       8.63301754e-03, 8.70859623e-03, 4.05251980e-03, 2.38418579e-07,\n","       2.10911036e-02, 2.50910521e-02, 9.09984112e-03, 1.17839575e-02,\n","       3.99863720e-03, 2.55236626e-02, 1.35287046e-02, 5.53548336e-03,\n","       8.85756016e-02, 3.69385481e-02, 3.26129198e-02, 3.36349010e-03,\n","       3.99732590e-03, 1.19209290e-07, 3.99780273e-03, 5.96046448e-07,\n","       1.11206770e-02, 5.81789017e-03, 6.16788864e-03, 1.82676315e-03,\n","       1.65711641e-02, 4.57930565e-03, 5.81741333e-04, 3.41546535e-03,\n","       2.45648623e-02, 4.57978249e-03, 4.57859039e-03, 4.92727757e-03,\n","       3.99684906e-03, 3.99589539e-03, 3.99744511e-03, 1.99869871e-02,\n","       1.47813559e-02, 4.76837158e-07, 3.99732590e-03, 9.55307484e-03,\n","       3.13854218e-03, 4.85825539e-03, 3.13544273e-03, 2.71208286e-02,\n","       1.68496370e-02, 2.89666653e-03, 3.99721861e-02, 7.99453259e-03,\n","       3.99649143e-03, 7.99548626e-03, 3.99720669e-03, 2.38418579e-07,\n","       1.19911432e-02, 7.99500942e-03, 9.53674316e-07, 7.99441338e-03,\n","       7.99465179e-03, 3.99708748e-03, 1.44338608e-03, 6.04748726e-04,\n","       2.09912062e-02, 1.44469738e-03, 1.05491877e-02, 2.55334377e-03,\n","       1.19062662e-02, 3.91268730e-03, 6.28650188e-03, 2.39850283e-02,\n","       2.79811621e-02, 3.99721861e-02, 8.34465027e-07, 1.19916201e-02,\n","       1.44823790e-02, 2.48944759e-03, 5.50448895e-03, 1.50334835e-03,\n","       1.22423172e-02, 2.49743462e-04, 2.50697136e-04, 2.37332582e-02,\n","       2.74395943e-03, 9.25230980e-03, 2.74109840e-03, 2.74097919e-03,\n","       2.39844322e-02, 1.19918585e-02, 2.39839554e-02, 3.99661064e-03,\n","       1.59883499e-02, 7.99369812e-03, 1.19909048e-02, 1.59883499e-02,\n","       1.61426067e-02, 3.99684906e-03, 2.14437246e-02, 1.19946003e-02,\n","       3.99744511e-03, 1.19922161e-02, 3.99827957e-03, 2.38418579e-07,\n","       1.59896612e-02, 6.24980927e-02, 1.68285370e-02, 3.45177650e-02,\n","       1.59116983e-02, 1.37475729e-02, 1.26683712e-03, 1.95732117e-02,\n","       6.83832169e-03, 7.19213486e-03, 1.32346153e-03, 2.35447884e-02,\n","       7.99441338e-03, 2.97299623e-02, 5.02562523e-03, 5.25724888e-03,\n","       1.61491632e-02, 1.07851028e-02, 4.76837158e-07, 7.99500942e-03,\n","       2.16023922e-02, 1.14918947e-02, 2.80127525e-02, 2.00157166e-02,\n","       4.02562618e-02, 7.31337070e-03, 2.67732143e-02, 7.07244873e-03,\n","       3.99756432e-03, 3.99756432e-03, 3.24356556e-03, 4.67395782e-03,\n","       2.09891796e-03, 1.00301504e-02, 2.26937532e-02, 1.54569149e-02,\n","       2.95779705e-02, 3.28255892e-02, 2.55800486e-02, 3.99923325e-03,\n","       5.99606037e-02, 1.99867487e-02, 1.19916201e-02, 3.99804115e-03,\n","       7.99369812e-03, 3.99827957e-03, 1.19916201e-02, 2.38418579e-07,\n","       2.38418579e-07, 7.99489021e-03, 3.99780273e-03, 3.99684906e-03,\n","       3.19786072e-02, 3.19784880e-02, 5.16152382e-03, 3.99696827e-03,\n","       1.73275471e-02, 1.23494864e-02, 1.23519897e-02, 2.76335478e-02,\n","       1.43647194e-04, 8.36074352e-03, 5.16462326e-03, 5.49471378e-03,\n","       1.40330791e-02, 2.26204395e-02, 2.34203339e-02, 1.01822615e-02,\n","       4.71730232e-02, 2.82976627e-02, 1.51402950e-02, 1.17673874e-02,\n","       9.72889662e-02, 4.39703465e-02, 9.03908014e-02, 1.35313272e-02,\n","       5.56588173e-04, 9.19950008e-03, 2.91606188e-02, 1.18998289e-02,\n","       1.23692751e-02, 2.50611305e-02, 3.09348106e-04, 7.87258148e-04,\n","       2.63725519e-02, 6.55194521e-02, 4.64450121e-02, 1.70302391e-02,\n","       1.41543269e-01, 8.72968435e-02, 1.07924461e-01, 2.86700726e-02,\n","       8.08668137e-03, 4.31025028e-03, 1.29604340e-02, 2.38180161e-04,\n","       5.16140461e-03, 3.73005867e-02, 1.66521072e-02, 3.45706940e-05,\n","       1.72601938e-02, 2.92935371e-02, 3.88704538e-02, 8.05103779e-03,\n","       4.72388268e-02, 4.42774296e-02, 2.87185907e-02, 7.53617287e-03]), 'mean_score_time': array([0.00399661, 0.        , 0.00399661, 0.02903068, 0.        ,\n","       0.01078963, 0.00399733, 0.00226009, 0.        , 0.        ,\n","       0.00399745, 0.00435483, 0.00867116, 0.00399816, 0.00833452,\n","       0.00399756, 0.        , 0.00399768, 0.00399792, 0.0039978 ,\n","       0.        , 0.00838459, 0.00399816, 0.00069571, 0.00399816,\n","       0.        , 0.00403357, 0.00399745, 0.00399745, 0.00461733,\n","       0.00399733, 0.        , 0.00434756, 0.        , 0.        ,\n","       0.0039978 , 0.        , 0.00863862, 0.01264596, 0.        ,\n","       0.00799513, 0.00399733, 0.00399685, 0.00063336, 0.00399756,\n","       0.00519001, 0.00799477, 0.00799513, 0.00399768, 0.00399756,\n","       0.00399756, 0.00799477, 0.00399733, 0.        , 0.        ,\n","       0.00399745, 0.00399745, 0.00800157, 0.00399733, 0.00399709,\n","       0.00798953, 0.00399756, 0.01199019, 0.00799489, 0.00407875,\n","       0.00399959, 0.00298977, 0.00399745, 0.00399792, 0.00153387,\n","       0.00803089, 0.00399876, 0.00399756, 0.00399721, 0.        ,\n","       0.00494289, 0.00399745, 0.00437975, 0.0086627 , 0.        ,\n","       0.00399768, 0.00399768, 0.        , 0.00799537, 0.        ,\n","       0.00402176, 0.        , 0.00799406, 0.00367725, 0.00399745,\n","       0.        , 0.        , 0.00399745, 0.00399745, 0.00628424,\n","       0.00809252, 0.00399768, 0.00154245, 0.00152338, 0.00554144,\n","       0.        , 0.00399745, 0.        , 0.00399697, 0.01198971,\n","       0.00799453, 0.01998711, 0.00399733, 0.00399804, 0.00399578,\n","       0.00799513, 0.00399768, 0.        , 0.        , 0.00441861,\n","       0.01199186, 0.00399733, 0.        , 0.00399756, 0.        ,\n","       0.00399745, 0.        , 0.00466454, 0.        , 0.0119915 ,\n","       0.00399756, 0.00399852, 0.0015403 , 0.00399709, 0.00799453,\n","       0.00799441, 0.00399733, 0.00552213, 0.00399816, 0.00603354,\n","       0.00799489, 0.00799525, 0.00399745, 0.00399733, 0.00399745,\n","       0.00799477, 0.00799477, 0.00876725, 0.00399745, 0.0039978 ,\n","       0.00799429, 0.00399673, 0.0039978 , 0.        , 0.00445998,\n","       0.00399756, 0.        , 0.        , 0.00399768, 0.00399745,\n","       0.00399709, 0.01199222, 0.00799465, 0.00399971, 0.        ,\n","       0.00399745, 0.00604367, 0.00799501, 0.0079962 , 0.00799453,\n","       0.00608337, 0.00603318, 0.        , 0.00466156, 0.02067769,\n","       0.00399709, 0.00399745, 0.        , 0.01265764, 0.        ,\n","       0.00399733, 0.00405455, 0.00874472, 0.        , 0.        ,\n","       0.00465393, 0.00401986, 0.00399756, 0.00399733, 0.00465333,\n","       0.        , 0.        , 0.        , 0.        , 0.00399721,\n","       0.00799477, 0.        , 0.00064754, 0.00799465, 0.00399673,\n","       0.0053463 , 0.00399709, 0.00466859, 0.        , 0.00861025,\n","       0.00867164, 0.00399756, 0.        , 0.01714444, 0.00204945,\n","       0.00399721, 0.0067023 , 0.01853526, 0.        , 0.00539804,\n","       0.00608528, 0.00153148, 0.00101221, 0.        , 0.0067513 ,\n","       0.        , 0.00799823, 0.        , 0.00552309, 0.00399697,\n","       0.00399721, 0.0119915 , 0.00204289, 0.02934372, 0.00152934,\n","       0.        , 0.00062895, 0.00853062, 0.00399768, 0.00799394,\n","       0.00400269, 0.00799394, 0.00399923, 0.00399733, 0.00799525,\n","       0.00223303, 0.        , 0.00399733, 0.00399745, 0.01236463,\n","       0.00399733, 0.00799584, 0.01637745, 0.0083245 , 0.00799417,\n","       0.00399685, 0.00433159, 0.01199198, 0.00869024, 0.        ,\n","       0.00399756, 0.02465045, 0.00799453, 0.00103092, 0.00399804,\n","       0.        , 0.        , 0.00402009, 0.01199174, 0.        ,\n","       0.00505292, 0.        , 0.00399733, 0.00813937, 0.00799561,\n","       0.00656343, 0.00868487, 0.        , 0.        , 0.00399709,\n","       0.00799429, 0.0039978 , 0.        , 0.        , 0.00399756,\n","       0.00871027, 0.        , 0.00399733, 0.00399721, 0.        ,\n","       0.        , 0.01665092, 0.01666558, 0.        , 0.        ,\n","       0.00799406, 0.00399733, 0.00399721, 0.00430918, 0.00399745,\n","       0.        , 0.00399804, 0.00891113, 0.01199234, 0.        ,\n","       0.00399745, 0.00799549, 0.00799596, 0.00616789, 0.00799489,\n","       0.00438392, 0.00399792, 0.00435281, 0.00869107, 0.00436163,\n","       0.00799596, 0.00399804, 0.00399709, 0.00437868, 0.00435281,\n","       0.        , 0.00399816, 0.00835824, 0.00838053, 0.00876069,\n","       0.00399828, 0.        , 0.00833189, 0.02027631, 0.00399792,\n","       0.00399792, 0.        , 0.0039978 , 0.        , 0.        ,\n","       0.        , 0.00604904, 0.        , 0.0045923 , 0.01199174,\n","       0.00399721, 0.006042  , 0.00858462, 0.00152004, 0.00867712,\n","       0.        , 0.00399697, 0.00399697, 0.        , 0.        ,\n","       0.00427282, 0.        , 0.        , 0.00399816, 0.00217557,\n","       0.00833321, 0.00433409, 0.00636518, 0.00801861, 0.03769028,\n","       0.00433958, 0.00799513, 0.00399745, 0.00399721, 0.        ,\n","       0.0039978 , 0.00799561, 0.00399768, 0.        , 0.00399816,\n","       0.00399745, 0.00214183, 0.        , 0.00450134, 0.00399745,\n","       0.0058794 , 0.00865376, 0.00399637, 0.        , 0.00399685,\n","       0.00399792, 0.02131784, 0.00399792, 0.00399745, 0.00399768,\n","       0.00399637, 0.00799513, 0.00399792, 0.00424421, 0.00799525,\n","       0.00463164, 0.0086695 , 0.00469887, 0.00255537, 0.00399816,\n","       0.00399756, 0.00471544, 0.00399733, 0.        , 0.0039978 ,\n","       0.        , 0.00799477, 0.00799394, 0.0039978 , 0.00799441,\n","       0.00399745, 0.00399733, 0.        , 0.00399721, 0.        ,\n","       0.00799417, 0.0039978 , 0.00867033, 0.0119921 , 0.00399697,\n","       0.        , 0.        , 0.00799572, 0.00399685, 0.00799489,\n","       0.00464284, 0.00799489, 0.00458086, 0.00839198, 0.00399745,\n","       0.00434875, 0.00477242, 0.00437176, 0.00062156, 0.00399816,\n","       0.00399697, 0.        , 0.0079962 , 0.00399768, 0.00866723,\n","       0.00062656, 0.        , 0.        , 0.        , 0.00399709,\n","       0.        , 0.00858414, 0.00399756, 0.00399721, 0.        ,\n","       0.00399709, 0.00208318, 0.00552166, 0.00259137, 0.00858355,\n","       0.0040009 , 0.00259519, 0.        , 0.00976825, 0.00867677,\n","       0.00799501, 0.00799501, 0.00799537, 0.        , 0.00203204,\n","       0.00399756, 0.        , 0.0039978 , 0.00465667, 0.        ,\n","       0.02510083, 0.00799429, 0.00399745, 0.        , 0.        ,\n","       0.00399733, 0.00399733, 0.00399709, 0.00799429, 0.00604916,\n","       0.00399768, 0.00399685, 0.00399733, 0.00399804, 0.        ,\n","       0.00399685, 0.        , 0.        , 0.00420451, 0.0080198 ,\n","       0.00399709, 0.00402093, 0.00799513, 0.00799441, 0.00433898,\n","       0.        , 0.00866914, 0.        , 0.        , 0.00399733,\n","       0.01003063, 0.00203109, 0.00799406, 0.00399733, 0.00399745,\n","       0.0079937 , 0.00799477, 0.00799465, 0.0119921 , 0.00399756,\n","       0.00399768, 0.00399721, 0.00799537, 0.        , 0.        ,\n","       0.        , 0.00203407, 0.0055238 , 0.00399697, 0.00399768,\n","       0.        , 0.00799537, 0.0039978 , 0.00799477, 0.00399745,\n","       0.        , 0.00399792, 0.00399721, 0.00799453, 0.00152624,\n","       0.00552487, 0.00399745, 0.        , 0.00399745, 0.01598859,\n","       0.00399721, 0.01199174, 0.00399745, 0.00399709, 0.00799692,\n","       0.00399745, 0.00799465, 0.        , 0.01598978, 0.00399768,\n","       0.00399673, 0.00399733, 0.00799417, 0.01598907, 0.00399745,\n","       0.        , 0.00799489, 0.00799477, 0.00399745, 0.00203359,\n","       0.00799489, 0.00399804, 0.00399673, 0.00399756, 0.        ,\n","       0.00467527, 0.00900018, 0.01699579, 0.00399709, 0.00799429,\n","       0.00468254, 0.        , 0.00870895, 0.01296198, 0.00799489,\n","       0.        , 0.00799358, 0.        , 0.00799716, 0.00399685,\n","       0.        , 0.        , 0.00399756, 0.00799441, 0.00799501,\n","       0.00399816, 0.        , 0.00399685, 0.00399733, 0.00399733,\n","       0.        , 0.00399733, 0.00799477, 0.00356889, 0.00799513,\n","       0.00399768, 0.        , 0.00399733, 0.00399697, 0.00399756,\n","       0.00153279, 0.00399709, 0.00399721, 0.00399768, 0.        ,\n","       0.00799501, 0.00399721, 0.        , 0.00799477, 0.00399756,\n","       0.00399685, 0.00799501, 0.00399661, 0.00399697, 0.00399721,\n","       0.00399685, 0.01003361, 0.00399685, 0.00799453, 0.00152826,\n","       0.00400031, 0.00399745, 0.        , 0.00399733, 0.00399745,\n","       0.        , 0.00399745, 0.00399721, 0.        , 0.0039978 ,\n","       0.        , 0.        , 0.00399756, 0.00399721, 0.        ,\n","       0.00399768, 0.00399733, 0.        , 0.00399804, 0.00203753,\n","       0.00204062, 0.00399721, 0.00799429, 0.        , 0.00552022,\n","       0.0055207 , 0.        , 0.        , 0.00399721, 0.00399733,\n","       0.        , 0.        , 0.        , 0.00399649, 0.00399745,\n","       0.        , 0.00399733, 0.        , 0.        , 0.00799477,\n","       0.00799716, 0.        , 0.00799513, 0.        , 0.        ,\n","       0.00799441, 0.00799465, 0.        , 0.00399733, 0.00399745,\n","       0.00399721, 0.00799465, 0.        , 0.00399733, 0.00799477,\n","       0.00399709, 0.00399709, 0.        , 0.00799525, 0.        ,\n","       0.00799429, 0.01998723, 0.        , 0.00799465, 0.00399745,\n","       0.        , 0.0039978 , 0.02001035, 0.00799274, 0.00399709,\n","       0.00472188, 0.008358  , 0.        , 0.00399733, 0.00472188,\n","       0.00399685, 0.00399756, 0.        , 0.        , 0.        ,\n","       0.00399768, 0.00537026, 0.00399792, 0.00799704, 0.00799477,\n","       0.00307965, 0.00758016, 0.00799417, 0.        , 0.00799417,\n","       0.00152731, 0.00607359, 0.00799501, 0.00153089, 0.00799549,\n","       0.00799513, 0.00799465, 0.        , 0.        , 0.00399661,\n","       0.00399756, 0.0039978 , 0.00799501, 0.00399745, 0.00799441,\n","       0.        , 0.00399768, 0.00399709, 0.        , 0.00399709,\n","       0.00799477, 0.00799537, 0.00399685, 0.00399768, 0.00799394,\n","       0.        , 0.02464795, 0.00399768, 0.00832784, 0.00152576,\n","       0.00479412, 0.00399709, 0.0039953 , 0.00799298, 0.00399745,\n","       0.        , 0.00399733, 0.        , 0.00399768, 0.00799441,\n","       0.        , 0.00799489, 0.00402176, 0.00874484, 0.00399745,\n","       0.        , 0.00799394, 0.00203514, 0.0079937 , 0.        ,\n","       0.        , 0.        , 0.00399601, 0.00399709, 0.00799263,\n","       0.00045145, 0.        , 0.        , 0.        , 0.0034883 ,\n","       0.01233959, 0.01660001, 0.00431621, 0.00404799, 0.        ,\n","       0.00399828, 0.00399816, 0.016626  , 0.00433266, 0.00399721,\n","       0.00495183, 0.        , 0.00833106, 0.00058734, 0.00246406,\n","       0.00665319, 0.00399733, 0.00799549]), 'std_score_time': array([3.99661064e-03, 0.00000000e+00, 3.99661064e-03, 2.06137896e-02,\n","       0.00000000e+00, 1.43957138e-03, 3.99732590e-03, 2.26008892e-03,\n","       0.00000000e+00, 0.00000000e+00, 3.99744511e-03, 4.35483456e-03,\n","       8.67116451e-03, 3.99816036e-03, 3.39388847e-04, 3.99756432e-03,\n","       0.00000000e+00, 3.99768353e-03, 3.99792194e-03, 3.99780273e-03,\n","       0.00000000e+00, 3.90172005e-04, 3.99816036e-03, 6.95705414e-04,\n","       3.99816036e-03, 0.00000000e+00, 4.03356552e-03, 3.99744511e-03,\n","       3.99744511e-03, 4.61733341e-03, 3.99732590e-03, 0.00000000e+00,\n","       4.34756279e-03, 0.00000000e+00, 0.00000000e+00, 3.99780273e-03,\n","       0.00000000e+00, 5.96046448e-04, 1.26459599e-02, 0.00000000e+00,\n","       9.53674316e-07, 3.99732590e-03, 3.99684906e-03, 6.33358955e-04,\n","       3.99756432e-03, 2.80463696e-03, 5.96046448e-07, 2.38418579e-07,\n","       3.99768353e-03, 3.99756432e-03, 3.99756432e-03, 5.96046448e-07,\n","       3.99732590e-03, 0.00000000e+00, 0.00000000e+00, 3.99744511e-03,\n","       3.99744511e-03, 6.91413879e-06, 3.99732590e-03, 3.99708748e-03,\n","       4.64916229e-06, 3.99756432e-03, 1.19901896e-02, 7.15255737e-07,\n","       4.07874584e-03, 3.99959087e-03, 2.98976898e-03, 3.99744511e-03,\n","       3.99792194e-03, 1.53386593e-03, 3.64780426e-05, 3.99875641e-03,\n","       3.99756432e-03, 3.99720669e-03, 0.00000000e+00, 4.94289398e-03,\n","       3.99744511e-03, 4.37974930e-03, 6.68048859e-04, 0.00000000e+00,\n","       3.99768353e-03, 3.99768353e-03, 0.00000000e+00, 7.15255737e-07,\n","       0.00000000e+00, 4.02176380e-03, 0.00000000e+00, 5.96046448e-07,\n","       3.67724895e-03, 3.99744511e-03, 0.00000000e+00, 0.00000000e+00,\n","       3.99744511e-03, 3.99744511e-03, 1.71184540e-03, 9.73939896e-05,\n","       3.99768353e-03, 1.54244900e-03, 1.52337551e-03, 2.45511532e-03,\n","       0.00000000e+00, 3.99744511e-03, 0.00000000e+00, 3.99696827e-03,\n","       3.99529934e-03, 3.57627869e-07, 1.19912624e-02, 3.99732590e-03,\n","       3.99804115e-03, 3.99577618e-03, 2.38418579e-07, 3.99768353e-03,\n","       0.00000000e+00, 0.00000000e+00, 4.41861153e-03, 3.99696827e-03,\n","       3.99732590e-03, 0.00000000e+00, 3.99756432e-03, 0.00000000e+00,\n","       3.99744511e-03, 0.00000000e+00, 4.66454029e-03, 0.00000000e+00,\n","       3.99756432e-03, 3.99756432e-03, 3.99851799e-03, 1.54030323e-03,\n","       3.99708748e-03, 3.57627869e-07, 0.00000000e+00, 3.99732590e-03,\n","       2.47228146e-03, 3.99816036e-03, 1.96182728e-03, 7.99489021e-03,\n","       1.19209290e-07, 3.99744511e-03, 3.99732590e-03, 3.99744511e-03,\n","       3.57627869e-07, 1.19209290e-07, 7.73072243e-04, 3.99744511e-03,\n","       3.99780273e-03, 3.57627869e-07, 3.99672985e-03, 3.99780273e-03,\n","       0.00000000e+00, 2.53081322e-04, 3.99756432e-03, 0.00000000e+00,\n","       0.00000000e+00, 3.99768353e-03, 3.99744511e-03, 3.99708748e-03,\n","       3.99780273e-03, 4.76837158e-07, 3.99971008e-03, 0.00000000e+00,\n","       3.99744511e-03, 1.95097923e-03, 3.57627869e-07, 1.19209290e-07,\n","       3.57627869e-07, 1.91247463e-03, 1.96146965e-03, 0.00000000e+00,\n","       4.66156006e-03, 1.26835108e-02, 3.99708748e-03, 3.99744511e-03,\n","       0.00000000e+00, 4.66275215e-03, 0.00000000e+00, 3.99732590e-03,\n","       4.05454636e-03, 7.51495361e-04, 0.00000000e+00, 0.00000000e+00,\n","       3.34095955e-03, 4.01985645e-03, 3.99756432e-03, 3.99732590e-03,\n","       4.65333462e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n","       0.00000000e+00, 3.99720669e-03, 1.19209290e-07, 0.00000000e+00,\n","       6.47544861e-04, 2.38418579e-07, 3.99672985e-03, 4.09650803e-03,\n","       3.99708748e-03, 4.66859341e-03, 0.00000000e+00, 8.61024857e-03,\n","       6.76512718e-04, 3.99756432e-03, 0.00000000e+00, 1.61378384e-02,\n","       2.04944611e-03, 3.99720669e-03, 2.62463093e-03, 1.05413198e-02,\n","       0.00000000e+00, 2.59709358e-03, 1.91414356e-03, 1.53148174e-03,\n","       1.01220608e-03, 0.00000000e+00, 2.61783600e-03, 0.00000000e+00,\n","       3.09944153e-06, 0.00000000e+00, 2.47204304e-03, 3.99696827e-03,\n","       3.99720669e-03, 1.19915009e-02, 2.04288960e-03, 2.13493109e-02,\n","       1.52933598e-03, 0.00000000e+00, 6.28948212e-04, 5.37395477e-04,\n","       3.99768353e-03, 4.76837158e-07, 4.00269032e-03, 4.76837158e-07,\n","       3.99923325e-03, 3.99732590e-03, 5.96046448e-07, 2.23302841e-03,\n","       0.00000000e+00, 3.99732590e-03, 3.99744511e-03, 4.36782837e-03,\n","       3.99732590e-03, 7.15255737e-07, 1.63774490e-02, 8.32450390e-03,\n","       4.76837158e-07, 3.99684906e-03, 4.33158875e-03, 3.99684906e-03,\n","       6.96063042e-04, 0.00000000e+00, 3.99756432e-03, 7.32719898e-03,\n","       1.19209290e-07, 1.03092194e-03, 3.99804115e-03, 0.00000000e+00,\n","       0.00000000e+00, 4.02009487e-03, 3.99637222e-03, 0.00000000e+00,\n","       2.99108028e-03, 0.00000000e+00, 3.99732590e-03, 1.45196915e-04,\n","       7.99560547e-03, 2.42018700e-03, 6.35385513e-04, 0.00000000e+00,\n","       0.00000000e+00, 3.99708748e-03, 1.19209290e-07, 3.99780273e-03,\n","       0.00000000e+00, 0.00000000e+00, 3.99756432e-03, 8.71026516e-03,\n","       0.00000000e+00, 3.99732590e-03, 3.99720669e-03, 0.00000000e+00,\n","       0.00000000e+00, 8.65435600e-03, 1.66655779e-02, 0.00000000e+00,\n","       0.00000000e+00, 3.57627869e-07, 3.99732590e-03, 3.99720669e-03,\n","       3.68285179e-03, 3.99744511e-03, 0.00000000e+00, 3.99804115e-03,\n","       9.16957855e-04, 3.99792194e-03, 0.00000000e+00, 3.99744511e-03,\n","       3.57627869e-07, 1.19209290e-07, 1.82652473e-03, 0.00000000e+00,\n","       4.38392162e-03, 3.99792194e-03, 4.35280800e-03, 9.53674316e-06,\n","       4.36162949e-03, 1.19209290e-07, 3.99804115e-03, 3.99708748e-03,\n","       4.37867641e-03, 4.35280800e-03, 0.00000000e+00, 3.99816036e-03,\n","       3.62396240e-04, 3.84926796e-04, 4.98294830e-05, 3.99827957e-03,\n","       0.00000000e+00, 3.38196754e-04, 1.17080212e-02, 3.99792194e-03,\n","       3.99792194e-03, 0.00000000e+00, 3.99780273e-03, 0.00000000e+00,\n","       0.00000000e+00, 0.00000000e+00, 1.94537640e-03, 0.00000000e+00,\n","       4.59229946e-03, 1.19917393e-02, 3.99720669e-03, 1.95336342e-03,\n","       5.87105751e-04, 1.52003765e-03, 6.83188438e-04, 0.00000000e+00,\n","       3.99696827e-03, 3.99696827e-03, 0.00000000e+00, 0.00000000e+00,\n","       4.27281857e-03, 0.00000000e+00, 0.00000000e+00, 3.99816036e-03,\n","       2.17556953e-03, 3.37600708e-04, 4.33409214e-03, 2.30562687e-03,\n","       2.22921371e-05, 2.90111303e-02, 4.33957577e-03, 4.76837158e-07,\n","       3.99744511e-03, 3.99720669e-03, 0.00000000e+00, 3.99780273e-03,\n","       1.19209290e-06, 3.99768353e-03, 0.00000000e+00, 3.99816036e-03,\n","       3.99744511e-03, 2.14183331e-03, 0.00000000e+00, 3.49569321e-03,\n","       3.99744511e-03, 2.11644173e-03, 6.58392906e-04, 3.99637222e-03,\n","       0.00000000e+00, 3.99684906e-03, 3.99792194e-03, 2.13178396e-02,\n","       3.99792194e-03, 3.99744511e-03, 3.99768353e-03, 3.99637222e-03,\n","       2.38418579e-07, 3.99792194e-03, 4.24420834e-03, 1.19209290e-07,\n","       4.63163853e-03, 6.75082207e-04, 4.69887257e-03, 2.55537033e-03,\n","       3.99816036e-03, 3.99756432e-03, 4.71544266e-03, 3.99732590e-03,\n","       0.00000000e+00, 3.99780273e-03, 0.00000000e+00, 3.57627869e-07,\n","       0.00000000e+00, 3.99780273e-03, 4.76837158e-07, 3.99744511e-03,\n","       3.99732590e-03, 0.00000000e+00, 3.99720669e-03, 0.00000000e+00,\n","       2.38418579e-07, 3.99780273e-03, 6.74962997e-04, 1.19920969e-02,\n","       3.99696827e-03, 0.00000000e+00, 0.00000000e+00, 1.19209290e-07,\n","       3.99684906e-03, 9.53674316e-07, 4.64284420e-03, 1.43051147e-06,\n","       4.58085537e-03, 3.51548195e-04, 3.99744511e-03, 4.34875488e-03,\n","       4.77242470e-03, 4.37176228e-03, 6.21557236e-04, 3.99816036e-03,\n","       3.99696827e-03, 0.00000000e+00, 1.19209290e-07, 3.99768353e-03,\n","       6.25133514e-04, 6.26564026e-04, 0.00000000e+00, 0.00000000e+00,\n","       0.00000000e+00, 3.99708748e-03, 0.00000000e+00, 5.88297844e-04,\n","       3.99756432e-03, 3.99720669e-03, 0.00000000e+00, 3.99708748e-03,\n","       2.08318233e-03, 2.47156620e-03, 2.59137154e-03, 5.88893890e-04,\n","       4.00090218e-03, 2.59518623e-03, 0.00000000e+00, 1.72567368e-03,\n","       6.83307648e-04, 5.96046448e-07, 5.96046448e-07, 2.38418579e-07,\n","       0.00000000e+00, 2.03204155e-03, 3.99756432e-03, 0.00000000e+00,\n","       3.99780273e-03, 4.65667248e-03, 0.00000000e+00, 1.55919790e-02,\n","       5.96046448e-07, 3.99744511e-03, 0.00000000e+00, 0.00000000e+00,\n","       3.99732590e-03, 3.99732590e-03, 3.99708748e-03, 5.96046448e-07,\n","       1.94692612e-03, 3.99768353e-03, 3.99684906e-03, 3.99732590e-03,\n","       3.99804115e-03, 0.00000000e+00, 3.99684906e-03, 0.00000000e+00,\n","       0.00000000e+00, 4.20451164e-03, 2.39610672e-05, 3.99708748e-03,\n","       4.02092934e-03, 1.43051147e-06, 4.76837158e-07, 4.33897972e-03,\n","       0.00000000e+00, 2.86102295e-06, 0.00000000e+00, 0.00000000e+00,\n","       3.99732590e-03, 5.95939159e-03, 2.03108788e-03, 1.19209290e-07,\n","       3.99732590e-03, 3.99744511e-03, 2.38418579e-07, 5.96046448e-07,\n","       7.15255737e-07, 3.99768353e-03, 3.99756432e-03, 3.99768353e-03,\n","       3.99720669e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n","       0.00000000e+00, 2.03406811e-03, 2.47013569e-03, 3.99696827e-03,\n","       3.99768353e-03, 0.00000000e+00, 0.00000000e+00, 3.99780273e-03,\n","       1.31130219e-06, 3.99744511e-03, 0.00000000e+00, 3.99792194e-03,\n","       3.99720669e-03, 3.57627869e-07, 1.52623653e-03, 2.47073174e-03,\n","       3.99744511e-03, 0.00000000e+00, 3.99744511e-03, 7.99512863e-03,\n","       3.99720669e-03, 3.99684906e-03, 3.99744511e-03, 3.99708748e-03,\n","       8.34465027e-07, 3.99744511e-03, 4.76837158e-07, 0.00000000e+00,\n","       7.99632072e-03, 3.99768353e-03, 3.99672985e-03, 3.99732590e-03,\n","       4.76837158e-07, 7.99465179e-03, 3.99744511e-03, 0.00000000e+00,\n","       9.53674316e-07, 1.19209290e-07, 3.99744511e-03, 2.03359127e-03,\n","       7.99489021e-03, 3.99804115e-03, 3.99672985e-03, 3.99756432e-03,\n","       0.00000000e+00, 3.31795216e-03, 1.00576878e-03, 1.69957876e-02,\n","       3.99708748e-03, 3.57627869e-07, 4.68254089e-03, 0.00000000e+00,\n","       6.66379929e-04, 4.37676907e-03, 4.76837158e-07, 0.00000000e+00,\n","       8.34465027e-07, 0.00000000e+00, 2.50339508e-06, 3.99684906e-03,\n","       0.00000000e+00, 0.00000000e+00, 3.99756432e-03, 2.38418579e-07,\n","       1.19209290e-07, 3.99816036e-03, 0.00000000e+00, 3.99684906e-03,\n","       3.99732590e-03, 3.99732590e-03, 0.00000000e+00, 3.99732590e-03,\n","       1.19209290e-07, 5.09500504e-04, 7.15255737e-07, 3.99768353e-03,\n","       0.00000000e+00, 3.99732590e-03, 3.99696827e-03, 3.99756432e-03,\n","       1.53279305e-03, 3.99708748e-03, 3.99720669e-03, 3.99768353e-03,\n","       0.00000000e+00, 1.19209290e-07, 3.99720669e-03, 0.00000000e+00,\n","       7.99477100e-03, 3.99756432e-03, 3.99684906e-03, 7.99500942e-03,\n","       3.99661064e-03, 3.99696827e-03, 3.99720669e-03, 3.99684906e-03,\n","       5.95688820e-03, 3.99684906e-03, 8.34465027e-07, 1.52826309e-03,\n","       4.00030613e-03, 3.99744511e-03, 0.00000000e+00, 3.99732590e-03,\n","       3.99744511e-03, 0.00000000e+00, 3.99744511e-03, 3.99720669e-03,\n","       0.00000000e+00, 3.99780273e-03, 0.00000000e+00, 0.00000000e+00,\n","       3.99756432e-03, 3.99720669e-03, 0.00000000e+00, 3.99768353e-03,\n","       3.99732590e-03, 0.00000000e+00, 3.99804115e-03, 2.03752518e-03,\n","       2.04062462e-03, 3.99720669e-03, 7.99429417e-03, 0.00000000e+00,\n","       2.47371197e-03, 2.47418880e-03, 0.00000000e+00, 0.00000000e+00,\n","       3.99720669e-03, 3.99732590e-03, 0.00000000e+00, 0.00000000e+00,\n","       0.00000000e+00, 3.99649143e-03, 3.99744511e-03, 0.00000000e+00,\n","       3.99732590e-03, 0.00000000e+00, 0.00000000e+00, 1.19209290e-07,\n","       2.50339508e-06, 0.00000000e+00, 7.99512863e-03, 0.00000000e+00,\n","       0.00000000e+00, 2.38418579e-07, 2.38418579e-07, 0.00000000e+00,\n","       3.99732590e-03, 3.99744511e-03, 3.99720669e-03, 2.38418579e-07,\n","       0.00000000e+00, 3.99732590e-03, 3.57627869e-07, 3.99708748e-03,\n","       3.99708748e-03, 0.00000000e+00, 8.34465027e-07, 0.00000000e+00,\n","       3.57627869e-07, 1.19920969e-02, 0.00000000e+00, 1.19209290e-06,\n","       3.99744511e-03, 0.00000000e+00, 3.99780273e-03, 1.19665861e-02,\n","       9.53674316e-07, 3.99708748e-03, 4.72187996e-03, 3.64065170e-04,\n","       0.00000000e+00, 3.99732590e-03, 4.72187996e-03, 3.99684906e-03,\n","       3.99756432e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n","       3.99768353e-03, 5.37025928e-03, 3.99792194e-03, 1.66893005e-06,\n","       1.19209290e-07, 3.07965279e-03, 4.14252281e-04, 0.00000000e+00,\n","       0.00000000e+00, 7.15255737e-07, 1.52730942e-03, 1.92105770e-03,\n","       1.19209290e-07, 1.53088570e-03, 1.19209290e-07, 4.76837158e-07,\n","       7.99465179e-03, 0.00000000e+00, 0.00000000e+00, 3.99661064e-03,\n","       3.99756432e-03, 3.99780273e-03, 8.34465027e-07, 3.99744511e-03,\n","       4.76837158e-07, 0.00000000e+00, 3.99768353e-03, 3.99708748e-03,\n","       0.00000000e+00, 3.99708748e-03, 1.19209290e-07, 2.38418579e-07,\n","       3.99684906e-03, 3.99768353e-03, 1.43051147e-06, 0.00000000e+00,\n","       1.53257847e-02, 3.99768353e-03, 3.28898430e-04, 1.52575970e-03,\n","       4.79412079e-03, 3.99708748e-03, 3.99529934e-03, 0.00000000e+00,\n","       3.99744511e-03, 0.00000000e+00, 3.99732590e-03, 0.00000000e+00,\n","       3.99768353e-03, 2.38418579e-07, 0.00000000e+00, 0.00000000e+00,\n","       4.02176380e-03, 7.51137733e-04, 3.99744511e-03, 0.00000000e+00,\n","       9.53674316e-07, 2.03514099e-03, 0.00000000e+00, 0.00000000e+00,\n","       0.00000000e+00, 0.00000000e+00, 3.99601460e-03, 3.99708748e-03,\n","       2.74181366e-06, 4.51445580e-04, 0.00000000e+00, 0.00000000e+00,\n","       0.00000000e+00, 3.48830223e-03, 4.34470177e-03, 8.60512257e-03,\n","       4.31621075e-03, 4.04798985e-03, 0.00000000e+00, 3.99827957e-03,\n","       3.99816036e-03, 8.63087177e-03, 4.33266163e-03, 3.99720669e-03,\n","       3.77714634e-03, 0.00000000e+00, 3.35931778e-04, 5.87344170e-04,\n","       2.46405602e-03, 1.34122372e-03, 3.99732590e-03, 5.96046448e-07]), 'param_learning_rate': masked_array(data=[0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n","                   0.01, 0.01, 0.01, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n","                   0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n","                   0.04, 0.04, 0.04],\n","             mask=[False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False],\n","       fill_value='?',\n","            dtype=object), 'param_max_depth': masked_array(data=[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","                   4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","                   4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6,\n","                   6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n","                   6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n","                   6, 6, 6, 6, 6, 6, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n","                   8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n","                   8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n","                   10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n","                   10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n","                   10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n","                   10, 10, 10, 10, 10, 10, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","                   4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","                   4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","                   4, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n","                   6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n","                   6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 8, 8, 8, 8,\n","                   8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n","                   8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n","                   8, 8, 8, 8, 8, 8, 8, 8, 10, 10, 10, 10, 10, 10, 10, 10,\n","                   10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n","                   10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n","                   10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 4, 4,\n","                   4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","                   4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","                   4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6, 6, 6,\n","                   6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n","                   6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n","                   6, 6, 6, 6, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n","                   8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n","                   8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 10, 10,\n","                   10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n","                   10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n","                   10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n","                   10, 10, 10, 10, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","                   4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","                   4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 6,\n","                   6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n","                   6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n","                   6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 8, 8, 8, 8, 8, 8, 8,\n","                   8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n","                   8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n","                   8, 8, 8, 8, 8, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n","                   10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n","                   10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n","                   10, 10, 10, 10, 10, 10, 10, 10, 10, 10],\n","             mask=[False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False],\n","       fill_value='?',\n","            dtype=object), 'param_max_features': masked_array(data=[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5,\n","                   5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 7, 7, 7, 7,\n","                   7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3,\n","                   3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5,\n","                   5, 5, 5, 5, 5, 5, 5, 5, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n","                   7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n","                   3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n","                   5, 5, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n","                   3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5,\n","                   5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 7, 7, 7, 7,\n","                   7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3,\n","                   3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5,\n","                   5, 5, 5, 5, 5, 5, 5, 5, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n","                   7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n","                   3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n","                   5, 5, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n","                   3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5,\n","                   5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 7, 7, 7, 7,\n","                   7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3,\n","                   3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5,\n","                   5, 5, 5, 5, 5, 5, 5, 5, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n","                   7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n","                   3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n","                   5, 5, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n","                   3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5,\n","                   5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 7, 7, 7, 7,\n","                   7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3,\n","                   3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5,\n","                   5, 5, 5, 5, 5, 5, 5, 5, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n","                   7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n","                   3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n","                   5, 5, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n","                   3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5,\n","                   5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 7, 7, 7, 7,\n","                   7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3,\n","                   3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5,\n","                   5, 5, 5, 5, 5, 5, 5, 5, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n","                   7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n","                   3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n","                   5, 5, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n","                   3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5,\n","                   5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 7, 7, 7, 7,\n","                   7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7],\n","             mask=[False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False],\n","       fill_value='?',\n","            dtype=object), 'param_n_estimators': masked_array(data=[20, 20, 20, 20, 50, 50, 50, 50, 70, 70, 70, 70, 100,\n","                   100, 100, 100, 20, 20, 20, 20, 50, 50, 50, 50, 70, 70,\n","                   70, 70, 100, 100, 100, 100, 20, 20, 20, 20, 50, 50, 50,\n","                   50, 70, 70, 70, 70, 100, 100, 100, 100, 20, 20, 20, 20,\n","                   50, 50, 50, 50, 70, 70, 70, 70, 100, 100, 100, 100, 20,\n","                   20, 20, 20, 50, 50, 50, 50, 70, 70, 70, 70, 100, 100,\n","                   100, 100, 20, 20, 20, 20, 50, 50, 50, 50, 70, 70, 70,\n","                   70, 100, 100, 100, 100, 20, 20, 20, 20, 50, 50, 50, 50,\n","                   70, 70, 70, 70, 100, 100, 100, 100, 20, 20, 20, 20, 50,\n","                   50, 50, 50, 70, 70, 70, 70, 100, 100, 100, 100, 20, 20,\n","                   20, 20, 50, 50, 50, 50, 70, 70, 70, 70, 100, 100, 100,\n","                   100, 20, 20, 20, 20, 50, 50, 50, 50, 70, 70, 70, 70,\n","                   100, 100, 100, 100, 20, 20, 20, 20, 50, 50, 50, 50, 70,\n","                   70, 70, 70, 100, 100, 100, 100, 20, 20, 20, 20, 50, 50,\n","                   50, 50, 70, 70, 70, 70, 100, 100, 100, 100, 20, 20, 20,\n","                   20, 50, 50, 50, 50, 70, 70, 70, 70, 100, 100, 100, 100,\n","                   20, 20, 20, 20, 50, 50, 50, 50, 70, 70, 70, 70, 100,\n","                   100, 100, 100, 20, 20, 20, 20, 50, 50, 50, 50, 70, 70,\n","                   70, 70, 100, 100, 100, 100, 20, 20, 20, 20, 50, 50, 50,\n","                   50, 70, 70, 70, 70, 100, 100, 100, 100, 20, 20, 20, 20,\n","                   50, 50, 50, 50, 70, 70, 70, 70, 100, 100, 100, 100, 20,\n","                   20, 20, 20, 50, 50, 50, 50, 70, 70, 70, 70, 100, 100,\n","                   100, 100, 20, 20, 20, 20, 50, 50, 50, 50, 70, 70, 70,\n","                   70, 100, 100, 100, 100, 20, 20, 20, 20, 50, 50, 50, 50,\n","                   70, 70, 70, 70, 100, 100, 100, 100, 20, 20, 20, 20, 50,\n","                   50, 50, 50, 70, 70, 70, 70, 100, 100, 100, 100, 20, 20,\n","                   20, 20, 50, 50, 50, 50, 70, 70, 70, 70, 100, 100, 100,\n","                   100, 20, 20, 20, 20, 50, 50, 50, 50, 70, 70, 70, 70,\n","                   100, 100, 100, 100, 20, 20, 20, 20, 50, 50, 50, 50, 70,\n","                   70, 70, 70, 100, 100, 100, 100, 20, 20, 20, 20, 50, 50,\n","                   50, 50, 70, 70, 70, 70, 100, 100, 100, 100, 20, 20, 20,\n","                   20, 50, 50, 50, 50, 70, 70, 70, 70, 100, 100, 100, 100,\n","                   20, 20, 20, 20, 50, 50, 50, 50, 70, 70, 70, 70, 100,\n","                   100, 100, 100, 20, 20, 20, 20, 50, 50, 50, 50, 70, 70,\n","                   70, 70, 100, 100, 100, 100, 20, 20, 20, 20, 50, 50, 50,\n","                   50, 70, 70, 70, 70, 100, 100, 100, 100, 20, 20, 20, 20,\n","                   50, 50, 50, 50, 70, 70, 70, 70, 100, 100, 100, 100, 20,\n","                   20, 20, 20, 50, 50, 50, 50, 70, 70, 70, 70, 100, 100,\n","                   100, 100, 20, 20, 20, 20, 50, 50, 50, 50, 70, 70, 70,\n","                   70, 100, 100, 100, 100, 20, 20, 20, 20, 50, 50, 50, 50,\n","                   70, 70, 70, 70, 100, 100, 100, 100, 20, 20, 20, 20, 50,\n","                   50, 50, 50, 70, 70, 70, 70, 100, 100, 100, 100, 20, 20,\n","                   20, 20, 50, 50, 50, 50, 70, 70, 70, 70, 100, 100, 100,\n","                   100, 20, 20, 20, 20, 50, 50, 50, 50, 70, 70, 70, 70,\n","                   100, 100, 100, 100, 20, 20, 20, 20, 50, 50, 50, 50, 70,\n","                   70, 70, 70, 100, 100, 100, 100, 20, 20, 20, 20, 50, 50,\n","                   50, 50, 70, 70, 70, 70, 100, 100, 100, 100, 20, 20, 20,\n","                   20, 50, 50, 50, 50, 70, 70, 70, 70, 100, 100, 100, 100,\n","                   20, 20, 20, 20, 50, 50, 50, 50, 70, 70, 70, 70, 100,\n","                   100, 100, 100, 20, 20, 20, 20, 50, 50, 50, 50, 70, 70,\n","                   70, 70, 100, 100, 100, 100, 20, 20, 20, 20, 50, 50, 50,\n","                   50, 70, 70, 70, 70, 100, 100, 100, 100, 20, 20, 20, 20,\n","                   50, 50, 50, 50, 70, 70, 70, 70, 100, 100, 100, 100, 20,\n","                   20, 20, 20, 50, 50, 50, 50, 70, 70, 70, 70, 100, 100,\n","                   100, 100, 20, 20, 20, 20, 50, 50, 50, 50, 70, 70, 70,\n","                   70, 100, 100, 100, 100, 20, 20, 20, 20, 50, 50, 50, 50,\n","                   70, 70, 70, 70, 100, 100, 100, 100, 20, 20, 20, 20, 50,\n","                   50, 50, 50, 70, 70, 70, 70, 100, 100, 100, 100, 20, 20,\n","                   20, 20, 50, 50, 50, 50, 70, 70, 70, 70, 100, 100, 100,\n","                   100],\n","             mask=[False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False],\n","       fill_value='?',\n","            dtype=object), 'param_subsample': masked_array(data=[0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5,\n","                   0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7,\n","                   0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9,\n","                   0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2,\n","                   0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5,\n","                   0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7,\n","                   0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9,\n","                   0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2,\n","                   0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5,\n","                   0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7,\n","                   0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9,\n","                   0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2,\n","                   0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5,\n","                   0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7,\n","                   0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9,\n","                   0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2,\n","                   0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5,\n","                   0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7,\n","                   0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9,\n","                   0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2,\n","                   0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5,\n","                   0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7,\n","                   0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9,\n","                   0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2,\n","                   0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5,\n","                   0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7,\n","                   0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9,\n","                   0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2,\n","                   0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5,\n","                   0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7,\n","                   0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9,\n","                   0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2,\n","                   0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5,\n","                   0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7,\n","                   0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9,\n","                   0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2,\n","                   0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5,\n","                   0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7,\n","                   0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9,\n","                   0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2,\n","                   0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5,\n","                   0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7,\n","                   0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9,\n","                   0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2,\n","                   0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5,\n","                   0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7,\n","                   0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9,\n","                   0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2,\n","                   0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5,\n","                   0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7,\n","                   0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9,\n","                   0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2,\n","                   0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5,\n","                   0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7,\n","                   0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9,\n","                   0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2,\n","                   0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5,\n","                   0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7,\n","                   0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9,\n","                   0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2,\n","                   0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5,\n","                   0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7,\n","                   0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9,\n","                   0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2,\n","                   0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5,\n","                   0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7,\n","                   0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9,\n","                   0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2,\n","                   0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5,\n","                   0.2, 0.9, 0.7, 0.5, 0.2, 0.9, 0.7, 0.5, 0.2],\n","             mask=[False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False],\n","       fill_value='?',\n","            dtype=object), 'params': [{'learning_rate': 0.01, 'max_depth': 4, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 6, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 8, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 4, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 6, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 8, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.02, 'max_depth': 10, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 4, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 6, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 8, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.03, 'max_depth': 10, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 4, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 6, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 8, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 3, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 3, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 3, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 3, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 5, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 5, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 5, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 5, 'n_estimators': 100, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 7, 'n_estimators': 20, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 7, 'n_estimators': 50, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 7, 'n_estimators': 70, 'subsample': 0.2}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.9}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.7}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.5}, {'learning_rate': 0.04, 'max_depth': 10, 'max_features': 7, 'n_estimators': 100, 'subsample': 0.2}], 'split0_test_score': array([0.66666667, 0.66666667, 0.66666667, 0.66666667, 0.88888889,\n","       0.88888889, 0.87407407, 0.83703704, 0.91111111, 0.9037037 ,\n","       0.9037037 , 0.86666667, 0.93333333, 0.91851852, 0.9037037 ,\n","       0.88148148, 0.66666667, 0.66666667, 0.66666667, 0.66666667,\n","       0.92592593, 0.91851852, 0.8962963 , 0.87407407, 0.91851852,\n","       0.93333333, 0.93333333, 0.86666667, 0.93333333, 0.94814815,\n","       0.94074074, 0.91851852, 0.66666667, 0.66666667, 0.66666667,\n","       0.66666667, 0.91111111, 0.93333333, 0.91111111, 0.82222222,\n","       0.94074074, 0.93333333, 0.9037037 , 0.88888889, 0.94074074,\n","       0.92592593, 0.92592593, 0.9037037 , 0.66666667, 0.66666667,\n","       0.66666667, 0.66666667, 0.88888889, 0.88148148, 0.88148148,\n","       0.87407407, 0.91851852, 0.9037037 , 0.8962963 , 0.85185185,\n","       0.91851852, 0.92592593, 0.91851852, 0.8962963 , 0.66666667,\n","       0.66666667, 0.66666667, 0.66666667, 0.91111111, 0.88888889,\n","       0.88148148, 0.82222222, 0.93333333, 0.93333333, 0.92592593,\n","       0.88148148, 0.94814815, 0.93333333, 0.94814815, 0.88888889,\n","       0.66666667, 0.66666667, 0.66666667, 0.66666667, 0.94074074,\n","       0.9037037 , 0.9037037 , 0.85925926, 0.93333333, 0.92592593,\n","       0.9037037 , 0.87407407, 0.95555556, 0.94814815, 0.94814815,\n","       0.88148148, 0.66666667, 0.66666667, 0.66666667, 0.66666667,\n","       0.8962963 , 0.88148148, 0.87407407, 0.87407407, 0.91851852,\n","       0.8962963 , 0.88888889, 0.86666667, 0.91851852, 0.91851852,\n","       0.93333333, 0.8962963 , 0.66666667, 0.66666667, 0.66666667,\n","       0.66666667, 0.93333333, 0.91111111, 0.88888889, 0.86666667,\n","       0.92592593, 0.92592593, 0.91851852, 0.88148148, 0.93333333,\n","       0.93333333, 0.96296296, 0.88888889, 0.66666667, 0.66666667,\n","       0.66666667, 0.66666667, 0.94814815, 0.92592593, 0.9037037 ,\n","       0.84444444, 0.94814815, 0.91111111, 0.93333333, 0.88148148,\n","       0.93333333, 0.93333333, 0.91851852, 0.88888889, 0.66666667,\n","       0.66666667, 0.66666667, 0.66666667, 0.88148148, 0.88148148,\n","       0.88888889, 0.82222222, 0.91111111, 0.9037037 , 0.9037037 ,\n","       0.88148148, 0.91851852, 0.91111111, 0.91851852, 0.88888889,\n","       0.66666667, 0.66666667, 0.66666667, 0.66666667, 0.91851852,\n","       0.91851852, 0.88888889, 0.85925926, 0.91851852, 0.93333333,\n","       0.91851852, 0.88888889, 0.93333333, 0.91851852, 0.93333333,\n","       0.8962963 , 0.66666667, 0.66666667, 0.66666667, 0.66666667,\n","       0.92592593, 0.91111111, 0.91111111, 0.84444444, 0.93333333,\n","       0.92592593, 0.91851852, 0.86666667, 0.95555556, 0.94074074,\n","       0.91111111, 0.8962963 , 0.88888889, 0.85185185, 0.85185185,\n","       0.8       , 0.94074074, 0.93333333, 0.9037037 , 0.8962963 ,\n","       0.93333333, 0.92592593, 0.9037037 , 0.88888889, 0.94814815,\n","       0.92592593, 0.91851852, 0.88888889, 0.8962963 , 0.91111111,\n","       0.88148148, 0.83703704, 0.95555556, 0.94074074, 0.92592593,\n","       0.9037037 , 0.94074074, 0.94814815, 0.93333333, 0.9037037 ,\n","       0.96296296, 0.97037037, 0.97037037, 0.9037037 , 0.94814815,\n","       0.91851852, 0.82962963, 0.80740741, 0.94074074, 0.93333333,\n","       0.93333333, 0.91851852, 0.92592593, 0.95555556, 0.95555556,\n","       0.94074074, 0.94074074, 0.96296296, 0.93333333, 0.94074074,\n","       0.86666667, 0.86666667, 0.87407407, 0.77777778, 0.91111111,\n","       0.91851852, 0.91851852, 0.88148148, 0.92592593, 0.92592593,\n","       0.91851852, 0.9037037 , 0.92592593, 0.94074074, 0.93333333,\n","       0.8962963 , 0.9037037 , 0.88888889, 0.87407407, 0.8       ,\n","       0.94814815, 0.94074074, 0.88888889, 0.88148148, 0.92592593,\n","       0.96296296, 0.94074074, 0.91111111, 0.94074074, 0.95555556,\n","       0.95555556, 0.91111111, 0.91851852, 0.91111111, 0.9037037 ,\n","       0.81481481, 0.92592593, 0.93333333, 0.94814815, 0.87407407,\n","       0.93333333, 0.94074074, 0.94074074, 0.8962963 , 0.93333333,\n","       0.95555556, 0.96296296, 0.93333333, 0.87407407, 0.88888889,\n","       0.82222222, 0.82962963, 0.92592593, 0.92592593, 0.9037037 ,\n","       0.9037037 , 0.91851852, 0.93333333, 0.94814815, 0.8962963 ,\n","       0.92592593, 0.92592593, 0.92592593, 0.92592593, 0.91111111,\n","       0.88148148, 0.86666667, 0.85925926, 0.92592593, 0.93333333,\n","       0.92592593, 0.8962963 , 0.95555556, 0.95555556, 0.94814815,\n","       0.87407407, 0.94074074, 0.95555556, 0.94814815, 0.9037037 ,\n","       0.91111111, 0.9037037 , 0.91111111, 0.8       , 0.94814815,\n","       0.91851852, 0.95555556, 0.88888889, 0.93333333, 0.94814815,\n","       0.94074074, 0.93333333, 0.94074074, 0.95555556, 0.97037037,\n","       0.94814815, 0.88148148, 0.87407407, 0.88148148, 0.75555556,\n","       0.91851852, 0.8962963 , 0.92592593, 0.88148148, 0.92592593,\n","       0.93333333, 0.92592593, 0.88888889, 0.94074074, 0.93333333,\n","       0.92592593, 0.93333333, 0.8962963 , 0.87407407, 0.86666667,\n","       0.82962963, 0.93333333, 0.93333333, 0.94814815, 0.86666667,\n","       0.94814815, 0.93333333, 0.94814815, 0.91111111, 0.94074074,\n","       0.95555556, 0.93333333, 0.92592593, 0.93333333, 0.9037037 ,\n","       0.8962963 , 0.84444444, 0.93333333, 0.92592593, 0.93333333,\n","       0.9037037 , 0.94074074, 0.92592593, 0.96296296, 0.92592593,\n","       0.94074074, 0.94814815, 0.95555556, 0.94074074, 0.9037037 ,\n","       0.88888889, 0.9037037 , 0.84444444, 0.9037037 , 0.93333333,\n","       0.93333333, 0.88888889, 0.94814815, 0.93333333, 0.91851852,\n","       0.91111111, 0.94074074, 0.94074074, 0.91851852, 0.91851852,\n","       0.93333333, 0.91851852, 0.8962963 , 0.82962963, 0.93333333,\n","       0.95555556, 0.95555556, 0.91111111, 0.96296296, 0.94814815,\n","       0.95555556, 0.9037037 , 0.96296296, 0.96296296, 0.96296296,\n","       0.91111111, 0.91851852, 0.91851852, 0.91111111, 0.82962963,\n","       0.94074074, 0.93333333, 0.94074074, 0.94814815, 0.93333333,\n","       0.96296296, 0.95555556, 0.93333333, 0.96296296, 0.96296296,\n","       0.97037037, 0.93333333, 0.91111111, 0.91111111, 0.8962963 ,\n","       0.85925926, 0.92592593, 0.94814815, 0.94814815, 0.88888889,\n","       0.93333333, 0.92592593, 0.92592593, 0.91111111, 0.94074074,\n","       0.93333333, 0.92592593, 0.91851852, 0.91851852, 0.91851852,\n","       0.8962963 , 0.87407407, 0.94814815, 0.94814815, 0.96296296,\n","       0.9037037 , 0.94074074, 0.97037037, 0.95555556, 0.91111111,\n","       0.94074074, 0.95555556, 0.96296296, 0.91111111, 0.95555556,\n","       0.91851852, 0.9037037 , 0.84444444, 0.93333333, 0.91851852,\n","       0.95555556, 0.92592593, 0.93333333, 0.96296296, 0.95555556,\n","       0.9037037 , 0.93333333, 0.95555556, 0.97037037, 0.91111111,\n","       0.91111111, 0.8962963 , 0.88148148, 0.85185185, 0.94074074,\n","       0.92592593, 0.94074074, 0.91111111, 0.93333333, 0.94074074,\n","       0.93333333, 0.8962963 , 0.92592593, 0.94074074, 0.92592593,\n","       0.92592593, 0.92592593, 0.9037037 , 0.91111111, 0.88888889,\n","       0.92592593, 0.94814815, 0.93333333, 0.88148148, 0.9037037 ,\n","       0.95555556, 0.95555556, 0.94074074, 0.94814815, 0.96296296,\n","       0.96296296, 0.91111111, 0.94074074, 0.92592593, 0.91111111,\n","       0.9037037 , 0.94074074, 0.96296296, 0.94814815, 0.94074074,\n","       0.94074074, 0.94074074, 0.94814815, 0.91111111, 0.93333333,\n","       0.96296296, 0.96296296, 0.93333333, 0.9037037 , 0.8962963 ,\n","       0.91111111, 0.85185185, 0.91851852, 0.94074074, 0.9037037 ,\n","       0.8962963 , 0.92592593, 0.92592593, 0.91851852, 0.8962963 ,\n","       0.92592593, 0.94814815, 0.94814815, 0.88888889, 0.93333333,\n","       0.95555556, 0.8962963 , 0.88148148, 0.94074074, 0.95555556,\n","       0.94814815, 0.91111111, 0.95555556, 0.95555556, 0.95555556,\n","       0.92592593, 0.92592593, 0.96296296, 0.95555556, 0.91851852,\n","       0.9037037 , 0.91111111, 0.9037037 , 0.82962963, 0.94074074,\n","       0.92592593, 0.97037037, 0.9037037 , 0.95555556, 0.95555556,\n","       0.96296296, 0.93333333, 0.94074074, 0.95555556, 0.96296296,\n","       0.93333333, 0.94814815, 0.91111111, 0.8962963 , 0.8962963 ,\n","       0.92592593, 0.92592593, 0.93333333, 0.91111111, 0.97037037,\n","       0.94814815, 0.95555556, 0.91851852, 0.93333333, 0.93333333,\n","       0.93333333, 0.9037037 , 0.92592593, 0.8962963 , 0.9037037 ,\n","       0.91111111, 0.96296296, 0.97037037, 0.96296296, 0.94074074,\n","       0.97037037, 0.97037037, 0.93333333, 0.92592593, 0.96296296,\n","       0.97037037, 0.96296296, 0.91851852, 0.94074074, 0.92592593,\n","       0.91851852, 0.84444444, 0.94074074, 0.96296296, 0.97037037,\n","       0.88888889, 0.96296296, 0.94814815, 0.94814815, 0.93333333,\n","       0.96296296, 0.96296296, 0.96296296, 0.94814815, 0.92592593,\n","       0.91851852, 0.8962963 , 0.8962963 , 0.92592593, 0.94074074,\n","       0.94074074, 0.93333333, 0.92592593, 0.92592593, 0.94814815,\n","       0.9037037 , 0.94814815, 0.94074074, 0.94814815, 0.91111111,\n","       0.91851852, 0.92592593, 0.91111111, 0.9037037 , 0.93333333,\n","       0.93333333, 0.96296296, 0.92592593, 0.93333333, 0.94814815,\n","       0.94814815, 0.94074074, 0.95555556, 0.97037037, 0.94814815,\n","       0.92592593, 0.94074074, 0.91851852, 0.91851852, 0.88148148,\n","       0.94074074, 0.95555556, 0.94814815, 0.93333333, 0.93333333,\n","       0.97037037, 0.97037037, 0.94814815, 0.92592593, 0.95555556,\n","       0.97037037, 0.93333333, 0.88888889, 0.8962963 , 0.91111111,\n","       0.87407407, 0.9037037 , 0.91851852, 0.91851852, 0.8962963 ,\n","       0.91111111, 0.93333333, 0.95555556, 0.91851852, 0.94074074,\n","       0.94814815, 0.92592593, 0.91111111, 0.91111111, 0.92592593,\n","       0.91111111, 0.91851852, 0.94074074, 0.95555556, 0.94814815,\n","       0.91111111, 0.93333333, 0.94814815, 0.94074074, 0.91851852,\n","       0.93333333, 0.96296296, 0.97037037, 0.93333333, 0.95555556,\n","       0.91851852, 0.91851852, 0.85925926, 0.94074074, 0.95555556,\n","       0.95555556, 0.9037037 , 0.95555556, 0.94074074, 0.94814815,\n","       0.9037037 , 0.94074074, 0.96296296, 0.97037037, 0.91851852,\n","       0.91111111, 0.92592593, 0.9037037 , 0.88148148, 0.8962963 ,\n","       0.93333333, 0.94074074, 0.94074074, 0.94074074, 0.91111111,\n","       0.91851852, 0.94814815, 0.94074074, 0.94814815, 0.92592593,\n","       0.92592593, 0.91111111, 0.91851852, 0.91111111, 0.9037037 ,\n","       0.94074074, 0.95555556, 0.94814815, 0.91111111, 0.94074074,\n","       0.96296296, 0.97037037, 0.91851852, 0.94814815, 0.97037037,\n","       0.96296296, 0.93333333, 0.95555556, 0.9037037 , 0.93333333,\n","       0.87407407, 0.94814815, 0.96296296, 0.92592593, 0.93333333,\n","       0.93333333, 0.97037037, 0.96296296, 0.92592593, 0.94074074,\n","       0.95555556, 0.97037037, 0.93333333]), 'split1_test_score': array([0.66666667, 0.66666667, 0.66666667, 0.66666667, 0.94814815,\n","       0.93333333, 0.94814815, 0.92592593, 0.93333333, 0.93333333,\n","       0.93333333, 0.93333333, 0.93333333, 0.93333333, 0.93333333,\n","       0.94814815, 0.66666667, 0.66666667, 0.66666667, 0.66666667,\n","       0.93333333, 0.93333333, 0.91851852, 0.93333333, 0.93333333,\n","       0.93333333, 0.93333333, 0.91851852, 0.93333333, 0.93333333,\n","       0.93333333, 0.91851852, 0.66666667, 0.66666667, 0.66666667,\n","       0.66666667, 0.93333333, 0.92592593, 0.91851852, 0.91111111,\n","       0.93333333, 0.92592593, 0.93333333, 0.93333333, 0.91111111,\n","       0.92592593, 0.92592593, 0.93333333, 0.66666667, 0.66666667,\n","       0.66666667, 0.66666667, 0.94814815, 0.93333333, 0.93333333,\n","       0.95555556, 0.93333333, 0.93333333, 0.94814815, 0.94814815,\n","       0.93333333, 0.93333333, 0.93333333, 0.93333333, 0.66666667,\n","       0.66666667, 0.66666667, 0.66666667, 0.92592593, 0.93333333,\n","       0.91851852, 0.93333333, 0.91851852, 0.93333333, 0.93333333,\n","       0.88888889, 0.86666667, 0.93333333, 0.93333333, 0.93333333,\n","       0.66666667, 0.66666667, 0.66666667, 0.66666667, 0.87407407,\n","       0.92592593, 0.91851852, 0.92592593, 0.87407407, 0.91111111,\n","       0.93333333, 0.91851852, 0.85185185, 0.86666667, 0.93333333,\n","       0.93333333, 0.66666667, 0.66666667, 0.66666667, 0.66666667,\n","       0.94814815, 0.94814815, 0.93333333, 0.8962963 , 0.93333333,\n","       0.93333333, 0.94814815, 0.94814815, 0.93333333, 0.93333333,\n","       0.93333333, 0.93333333, 0.66666667, 0.66666667, 0.66666667,\n","       0.66666667, 0.93333333, 0.93333333, 0.91851852, 0.93333333,\n","       0.93333333, 0.93333333, 0.93333333, 0.94814815, 0.93333333,\n","       0.92592593, 0.93333333, 0.93333333, 0.66666667, 0.66666667,\n","       0.66666667, 0.66666667, 0.87407407, 0.9037037 , 0.91851852,\n","       0.92592593, 0.86666667, 0.86666667, 0.91851852, 0.91851852,\n","       0.85925926, 0.86666667, 0.92592593, 0.91851852, 0.66666667,\n","       0.66666667, 0.66666667, 0.66666667, 0.93333333, 0.93333333,\n","       0.94814815, 0.96296296, 0.93333333, 0.93333333, 0.93333333,\n","       0.92592593, 0.93333333, 0.93333333, 0.93333333, 0.93333333,\n","       0.66666667, 0.66666667, 0.66666667, 0.66666667, 0.93333333,\n","       0.93333333, 0.91851852, 0.92592593, 0.92592593, 0.93333333,\n","       0.93333333, 0.91851852, 0.92592593, 0.93333333, 0.93333333,\n","       0.93333333, 0.66666667, 0.66666667, 0.66666667, 0.66666667,\n","       0.87407407, 0.93333333, 0.91851852, 0.95555556, 0.86666667,\n","       0.93333333, 0.91851852, 0.93333333, 0.85185185, 0.88148148,\n","       0.92592593, 0.91851852, 0.94074074, 0.95555556, 0.93333333,\n","       0.85925926, 0.93333333, 0.93333333, 0.93333333, 0.94814815,\n","       0.93333333, 0.93333333, 0.93333333, 0.93333333, 0.93333333,\n","       0.94074074, 0.93333333, 0.93333333, 0.93333333, 0.91851852,\n","       0.91111111, 0.91851852, 0.93333333, 0.92592593, 0.93333333,\n","       0.93333333, 0.92592593, 0.93333333, 0.93333333, 0.93333333,\n","       0.93333333, 0.93333333, 0.92592593, 0.93333333, 0.93333333,\n","       0.91851852, 0.91851852, 0.93333333, 0.92592593, 0.92592593,\n","       0.93333333, 0.91851852, 0.91111111, 0.93333333, 0.92592593,\n","       0.93333333, 0.86666667, 0.86666667, 0.93333333, 0.93333333,\n","       0.94814815, 0.97037037, 0.91851852, 0.88148148, 0.93333333,\n","       0.93333333, 0.93333333, 0.94814815, 0.93333333, 0.93333333,\n","       0.93333333, 0.93333333, 0.93333333, 0.93333333, 0.93333333,\n","       0.93333333, 0.91851852, 0.91851852, 0.93333333, 0.88888889,\n","       0.91851852, 0.93333333, 0.93333333, 0.94814815, 0.87407407,\n","       0.91851852, 0.93333333, 0.93333333, 0.86666667, 0.93333333,\n","       0.94074074, 0.93333333, 0.87407407, 0.91851852, 0.94814815,\n","       0.93333333, 0.86666667, 0.86666667, 0.92592593, 0.93333333,\n","       0.85185185, 0.91851852, 0.92592593, 0.93333333, 0.85185185,\n","       0.86666667, 0.92592593, 0.93333333, 0.94814815, 0.94074074,\n","       0.93333333, 0.85925926, 0.93333333, 0.93333333, 0.93333333,\n","       0.93333333, 0.93333333, 0.93333333, 0.93333333, 0.94814815,\n","       0.92592593, 0.93333333, 0.93333333, 0.93333333, 0.94074074,\n","       0.93333333, 0.93333333, 0.87407407, 0.91851852, 0.93333333,\n","       0.93333333, 0.93333333, 0.93333333, 0.92592593, 0.93333333,\n","       0.93333333, 0.92592593, 0.93333333, 0.93333333, 0.93333333,\n","       0.87407407, 0.91851852, 0.91851852, 0.94814815, 0.85185185,\n","       0.88148148, 0.93333333, 0.93333333, 0.86666667, 0.86666667,\n","       0.93333333, 0.93333333, 0.85925926, 0.86666667, 0.92592593,\n","       0.93333333, 0.93333333, 0.94814815, 0.93333333, 0.9037037 ,\n","       0.93333333, 0.93333333, 0.93333333, 0.94814815, 0.93333333,\n","       0.93333333, 0.93333333, 0.93333333, 0.93333333, 0.93333333,\n","       0.93333333, 0.93333333, 0.92592593, 0.93333333, 0.91851852,\n","       0.87407407, 0.9037037 , 0.92592593, 0.93333333, 0.91851852,\n","       0.93333333, 0.93333333, 0.93333333, 0.93333333, 0.92592593,\n","       0.92592593, 0.93333333, 0.93333333, 0.87407407, 0.92592593,\n","       0.91851852, 0.85185185, 0.85185185, 0.86666667, 0.93333333,\n","       0.93333333, 0.86666667, 0.91111111, 0.93333333, 0.93333333,\n","       0.85185185, 0.86666667, 0.91851852, 0.93333333, 0.93333333,\n","       0.94814815, 0.94814815, 0.92592593, 0.93333333, 0.94074074,\n","       0.93333333, 0.94814815, 0.94074074, 0.94074074, 0.93333333,\n","       0.93333333, 0.94074074, 0.94074074, 0.94074074, 0.93333333,\n","       0.92592593, 0.93333333, 0.93333333, 0.93333333, 0.92592593,\n","       0.93333333, 0.92592593, 0.93333333, 0.92592593, 0.94074074,\n","       0.93333333, 0.93333333, 0.91851852, 0.92592593, 0.93333333,\n","       0.93333333, 0.91111111, 0.92592593, 0.91851852, 0.91111111,\n","       0.91111111, 0.93333333, 0.92592593, 0.91851852, 0.91851852,\n","       0.91851852, 0.93333333, 0.93333333, 0.86666667, 0.92592593,\n","       0.91111111, 0.93333333, 0.93333333, 0.93333333, 0.92592593,\n","       0.88148148, 0.93333333, 0.93333333, 0.93333333, 0.93333333,\n","       0.93333333, 0.93333333, 0.93333333, 0.94814815, 0.93333333,\n","       0.93333333, 0.93333333, 0.93333333, 0.93333333, 0.93333333,\n","       0.91851852, 0.94814815, 0.86666667, 0.91111111, 0.92592593,\n","       0.93333333, 0.86666667, 0.92592593, 0.94074074, 0.93333333,\n","       0.86666667, 0.92592593, 0.93333333, 0.93333333, 0.87407407,\n","       0.93333333, 0.94814815, 0.91851852, 0.85185185, 0.86666667,\n","       0.92592593, 0.93333333, 0.85185185, 0.86666667, 0.8962963 ,\n","       0.93333333, 0.85925926, 0.86666667, 0.92592593, 0.93333333,\n","       0.94814815, 0.93333333, 0.93333333, 0.93333333, 0.93333333,\n","       0.93333333, 0.93333333, 0.93333333, 0.92592593, 0.93333333,\n","       0.93333333, 0.93333333, 0.93333333, 0.93333333, 0.93333333,\n","       0.93333333, 0.92592593, 0.93333333, 0.91851852, 0.93333333,\n","       0.92592593, 0.93333333, 0.93333333, 0.92592593, 0.8962963 ,\n","       0.92592593, 0.93333333, 0.93333333, 0.92592593, 0.94074074,\n","       0.93333333, 0.93333333, 0.86666667, 0.87407407, 0.91851852,\n","       0.94074074, 0.85185185, 0.91111111, 0.93333333, 0.93333333,\n","       0.86666667, 0.86666667, 0.93333333, 0.93333333, 0.85925926,\n","       0.86666667, 0.93333333, 0.93333333, 0.93333333, 0.94814815,\n","       0.93333333, 0.94074074, 0.93333333, 0.93333333, 0.93333333,\n","       0.93333333, 0.93333333, 0.93333333, 0.93333333, 0.93333333,\n","       0.93333333, 0.93333333, 0.93333333, 0.93333333, 0.93333333,\n","       0.93333333, 0.93333333, 0.93333333, 0.9037037 , 0.93333333,\n","       0.93333333, 0.93333333, 0.92592593, 0.93333333, 0.93333333,\n","       0.93333333, 0.91111111, 0.91851852, 0.93333333, 0.93333333,\n","       0.87407407, 0.93333333, 0.93333333, 0.91111111, 0.85925926,\n","       0.87407407, 0.94074074, 0.93333333, 0.85185185, 0.88148148,\n","       0.93333333, 0.93333333, 0.85925926, 0.86666667, 0.86666667,\n","       0.93333333, 0.94814815, 0.93333333, 0.93333333, 0.87407407,\n","       0.93333333, 0.93333333, 0.93333333, 0.93333333, 0.94074074,\n","       0.94074074, 0.93333333, 0.92592593, 0.93333333, 0.94074074,\n","       0.94074074, 0.93333333, 0.93333333, 0.93333333, 0.93333333,\n","       0.93333333, 0.92592593, 0.93333333, 0.92592593, 0.93333333,\n","       0.91851852, 0.93333333, 0.93333333, 0.93333333, 0.91111111,\n","       0.93333333, 0.93333333, 0.93333333, 0.92592593, 0.92592593,\n","       0.95555556, 0.91111111, 0.91851852, 0.93333333, 0.92592593,\n","       0.93333333, 0.85925926, 0.92592593, 0.93333333, 0.93333333,\n","       0.86666667, 0.88148148, 0.93333333, 0.93333333, 0.93333333,\n","       0.93333333, 0.94814815, 0.94814815, 0.94074074, 0.93333333,\n","       0.93333333, 0.93333333, 0.94074074, 0.93333333, 0.93333333,\n","       0.93333333, 0.94074074, 0.94074074, 0.93333333, 0.93333333,\n","       0.92592593, 0.91851852, 0.91851852, 0.93333333, 0.86666667,\n","       0.91851852, 0.92592593, 0.93333333, 0.86666667, 0.92592593,\n","       0.94074074, 0.93333333, 0.85925926, 0.93333333, 0.93333333,\n","       0.93333333, 0.86666667, 0.94074074, 0.93333333, 0.92592593,\n","       0.85185185, 0.86666667, 0.93333333, 0.93333333, 0.85185185,\n","       0.86666667, 0.92592593, 0.93333333, 0.85185185, 0.8962963 ,\n","       0.91111111, 0.93333333, 0.93333333, 0.93333333, 0.94814815,\n","       0.94814815, 0.93333333, 0.93333333, 0.93333333, 0.93333333,\n","       0.94074074, 0.93333333, 0.93333333, 0.92592593, 0.94074074,\n","       0.93333333, 0.93333333, 0.93333333, 0.91851852, 0.93333333,\n","       0.93333333, 0.93333333, 0.85925926, 0.8962963 , 0.93333333,\n","       0.93333333, 0.87407407, 0.93333333, 0.93333333, 0.93333333,\n","       0.86666667, 0.92592593, 0.92592593, 0.93333333, 0.86666667,\n","       0.85185185, 0.91111111, 0.92592593, 0.86666667, 0.86666667,\n","       0.93333333, 0.93333333, 0.85925926, 0.86666667, 0.88148148,\n","       0.93333333, 0.85925926, 0.91111111, 0.88148148, 0.93333333,\n","       0.93333333, 0.93333333, 0.93333333, 0.94814815, 0.93333333,\n","       0.93333333, 0.93333333, 0.93333333, 0.93333333, 0.93333333,\n","       0.93333333, 0.92592593, 0.93333333, 0.93333333, 0.93333333,\n","       0.92592593, 0.93333333, 0.93333333, 0.93333333, 0.92592593,\n","       0.86666667, 0.92592593, 0.93333333, 0.93333333, 0.9037037 ,\n","       0.93333333, 0.93333333, 0.93333333, 0.86666667, 0.93333333,\n","       0.94074074, 0.93333333, 0.87407407, 0.87407407, 0.94074074,\n","       0.91851852, 0.86666667, 0.86666667, 0.93333333, 0.93333333,\n","       0.85925926, 0.91851852, 0.88148148, 0.93333333, 0.85925926,\n","       0.86666667, 0.94074074, 0.91851852]), 'mean_test_score': array([0.66666667, 0.66666667, 0.66666667, 0.66666667, 0.91851852,\n","       0.91111111, 0.91111111, 0.88148148, 0.92222222, 0.91851852,\n","       0.91851852, 0.9       , 0.93333333, 0.92592593, 0.91851852,\n","       0.91481481, 0.66666667, 0.66666667, 0.66666667, 0.66666667,\n","       0.92962963, 0.92592593, 0.90740741, 0.9037037 , 0.92592593,\n","       0.93333333, 0.93333333, 0.89259259, 0.93333333, 0.94074074,\n","       0.93703704, 0.91851852, 0.66666667, 0.66666667, 0.66666667,\n","       0.66666667, 0.92222222, 0.92962963, 0.91481481, 0.86666667,\n","       0.93703704, 0.92962963, 0.91851852, 0.91111111, 0.92592593,\n","       0.92592593, 0.92592593, 0.91851852, 0.66666667, 0.66666667,\n","       0.66666667, 0.66666667, 0.91851852, 0.90740741, 0.90740741,\n","       0.91481481, 0.92592593, 0.91851852, 0.92222222, 0.9       ,\n","       0.92592593, 0.92962963, 0.92592593, 0.91481481, 0.66666667,\n","       0.66666667, 0.66666667, 0.66666667, 0.91851852, 0.91111111,\n","       0.9       , 0.87777778, 0.92592593, 0.93333333, 0.92962963,\n","       0.88518519, 0.90740741, 0.93333333, 0.94074074, 0.91111111,\n","       0.66666667, 0.66666667, 0.66666667, 0.66666667, 0.90740741,\n","       0.91481481, 0.91111111, 0.89259259, 0.9037037 , 0.91851852,\n","       0.91851852, 0.8962963 , 0.9037037 , 0.90740741, 0.94074074,\n","       0.90740741, 0.66666667, 0.66666667, 0.66666667, 0.66666667,\n","       0.92222222, 0.91481481, 0.9037037 , 0.88518519, 0.92592593,\n","       0.91481481, 0.91851852, 0.90740741, 0.92592593, 0.92592593,\n","       0.93333333, 0.91481481, 0.66666667, 0.66666667, 0.66666667,\n","       0.66666667, 0.93333333, 0.92222222, 0.9037037 , 0.9       ,\n","       0.92962963, 0.92962963, 0.92592593, 0.91481481, 0.93333333,\n","       0.92962963, 0.94814815, 0.91111111, 0.66666667, 0.66666667,\n","       0.66666667, 0.66666667, 0.91111111, 0.91481481, 0.91111111,\n","       0.88518519, 0.90740741, 0.88888889, 0.92592593, 0.9       ,\n","       0.8962963 , 0.9       , 0.92222222, 0.9037037 , 0.66666667,\n","       0.66666667, 0.66666667, 0.66666667, 0.90740741, 0.90740741,\n","       0.91851852, 0.89259259, 0.92222222, 0.91851852, 0.91851852,\n","       0.9037037 , 0.92592593, 0.92222222, 0.92592593, 0.91111111,\n","       0.66666667, 0.66666667, 0.66666667, 0.66666667, 0.92592593,\n","       0.92592593, 0.9037037 , 0.89259259, 0.92222222, 0.93333333,\n","       0.92592593, 0.9037037 , 0.92962963, 0.92592593, 0.93333333,\n","       0.91481481, 0.66666667, 0.66666667, 0.66666667, 0.66666667,\n","       0.9       , 0.92222222, 0.91481481, 0.9       , 0.9       ,\n","       0.92962963, 0.91851852, 0.9       , 0.9037037 , 0.91111111,\n","       0.91851852, 0.90740741, 0.91481481, 0.9037037 , 0.89259259,\n","       0.82962963, 0.93703704, 0.93333333, 0.91851852, 0.92222222,\n","       0.93333333, 0.92962963, 0.91851852, 0.91111111, 0.94074074,\n","       0.93333333, 0.92592593, 0.91111111, 0.91481481, 0.91481481,\n","       0.8962963 , 0.87777778, 0.94444444, 0.93333333, 0.92962963,\n","       0.91851852, 0.93333333, 0.94074074, 0.93333333, 0.91851852,\n","       0.94814815, 0.95185185, 0.94814815, 0.91851852, 0.94074074,\n","       0.91851852, 0.87407407, 0.87037037, 0.93333333, 0.92962963,\n","       0.93333333, 0.91851852, 0.91851852, 0.94444444, 0.94074074,\n","       0.93703704, 0.9037037 , 0.91481481, 0.93333333, 0.93703704,\n","       0.90740741, 0.91851852, 0.8962963 , 0.82962963, 0.92222222,\n","       0.92592593, 0.92592593, 0.91481481, 0.92962963, 0.92962963,\n","       0.92592593, 0.91851852, 0.92962963, 0.93703704, 0.93333333,\n","       0.91481481, 0.91111111, 0.9037037 , 0.9037037 , 0.84444444,\n","       0.93333333, 0.93703704, 0.91111111, 0.91481481, 0.9       ,\n","       0.94074074, 0.93703704, 0.92222222, 0.9037037 , 0.94444444,\n","       0.94814815, 0.92222222, 0.8962963 , 0.91481481, 0.92592593,\n","       0.87407407, 0.8962963 , 0.9       , 0.93703704, 0.9037037 ,\n","       0.89259259, 0.92962963, 0.93333333, 0.91481481, 0.89259259,\n","       0.91111111, 0.94444444, 0.93333333, 0.91111111, 0.91481481,\n","       0.87777778, 0.84444444, 0.92962963, 0.92962963, 0.91851852,\n","       0.91851852, 0.92592593, 0.93333333, 0.94074074, 0.92222222,\n","       0.92592593, 0.92962963, 0.92962963, 0.92962963, 0.92592593,\n","       0.90740741, 0.9       , 0.86666667, 0.92222222, 0.93333333,\n","       0.92962963, 0.91481481, 0.94444444, 0.94074074, 0.94074074,\n","       0.9037037 , 0.93333333, 0.94444444, 0.94074074, 0.91851852,\n","       0.89259259, 0.91111111, 0.91481481, 0.87407407, 0.9       ,\n","       0.9       , 0.94444444, 0.91111111, 0.9       , 0.90740741,\n","       0.93703704, 0.93333333, 0.9       , 0.91111111, 0.94814815,\n","       0.94074074, 0.90740741, 0.91111111, 0.90740741, 0.82962963,\n","       0.92592593, 0.91481481, 0.92962963, 0.91481481, 0.92962963,\n","       0.93333333, 0.92962963, 0.91111111, 0.93703704, 0.93333333,\n","       0.92962963, 0.93333333, 0.91111111, 0.9037037 , 0.89259259,\n","       0.85185185, 0.91851852, 0.92962963, 0.94074074, 0.89259259,\n","       0.94074074, 0.93333333, 0.94074074, 0.92222222, 0.93333333,\n","       0.94074074, 0.93333333, 0.92962963, 0.9037037 , 0.91481481,\n","       0.90740741, 0.84814815, 0.89259259, 0.8962963 , 0.93333333,\n","       0.91851852, 0.9037037 , 0.91851852, 0.94814815, 0.92962963,\n","       0.8962963 , 0.90740741, 0.93703704, 0.93703704, 0.91851852,\n","       0.91851852, 0.92592593, 0.88518519, 0.91851852, 0.93703704,\n","       0.93333333, 0.91851852, 0.94444444, 0.93703704, 0.92592593,\n","       0.92222222, 0.94074074, 0.94074074, 0.92962963, 0.92592593,\n","       0.92962963, 0.92592593, 0.91481481, 0.88148148, 0.92962963,\n","       0.94444444, 0.94074074, 0.92222222, 0.94444444, 0.94444444,\n","       0.94444444, 0.91851852, 0.94074074, 0.94444444, 0.94814815,\n","       0.92222222, 0.91481481, 0.92222222, 0.91481481, 0.87037037,\n","       0.92592593, 0.93333333, 0.93333333, 0.93333333, 0.92592593,\n","       0.94074074, 0.94444444, 0.93333333, 0.91481481, 0.94444444,\n","       0.94074074, 0.93333333, 0.92222222, 0.92222222, 0.91111111,\n","       0.87037037, 0.92962963, 0.94074074, 0.94074074, 0.91111111,\n","       0.93333333, 0.92962963, 0.92962963, 0.92962963, 0.93703704,\n","       0.93333333, 0.92962963, 0.92592593, 0.92592593, 0.92592593,\n","       0.90740741, 0.91111111, 0.90740741, 0.92962963, 0.94444444,\n","       0.91851852, 0.9037037 , 0.94814815, 0.94814815, 0.92222222,\n","       0.9037037 , 0.94074074, 0.94814815, 0.92222222, 0.91481481,\n","       0.92592593, 0.92592593, 0.88148148, 0.89259259, 0.89259259,\n","       0.94074074, 0.92962963, 0.89259259, 0.91481481, 0.92592593,\n","       0.91851852, 0.8962963 , 0.91111111, 0.94814815, 0.92222222,\n","       0.92962963, 0.91481481, 0.90740741, 0.89259259, 0.93703704,\n","       0.92962963, 0.93703704, 0.92222222, 0.92962963, 0.93703704,\n","       0.93333333, 0.91481481, 0.92962963, 0.93703704, 0.92962963,\n","       0.92962963, 0.92592593, 0.91851852, 0.91481481, 0.91111111,\n","       0.92592593, 0.94074074, 0.93333333, 0.9037037 , 0.9       ,\n","       0.94074074, 0.94444444, 0.93703704, 0.93703704, 0.95185185,\n","       0.94814815, 0.92222222, 0.9037037 , 0.9       , 0.91481481,\n","       0.92222222, 0.8962963 , 0.93703704, 0.94074074, 0.93703704,\n","       0.9037037 , 0.9037037 , 0.94074074, 0.92222222, 0.8962963 ,\n","       0.91481481, 0.94814815, 0.93333333, 0.91851852, 0.92222222,\n","       0.92222222, 0.8962963 , 0.92592593, 0.93703704, 0.91851852,\n","       0.91481481, 0.92962963, 0.92962963, 0.92592593, 0.91481481,\n","       0.92962963, 0.94074074, 0.94074074, 0.91111111, 0.93333333,\n","       0.94444444, 0.91481481, 0.90740741, 0.92222222, 0.94444444,\n","       0.94074074, 0.92222222, 0.94074074, 0.94444444, 0.94444444,\n","       0.92962963, 0.91851852, 0.94074074, 0.94444444, 0.92592593,\n","       0.88888889, 0.92222222, 0.91851852, 0.87037037, 0.9       ,\n","       0.9       , 0.95555556, 0.91851852, 0.9037037 , 0.91851852,\n","       0.94814815, 0.93333333, 0.9       , 0.91111111, 0.91481481,\n","       0.93333333, 0.94814815, 0.92222222, 0.91481481, 0.88518519,\n","       0.92962963, 0.92962963, 0.93333333, 0.92222222, 0.95555556,\n","       0.94444444, 0.94444444, 0.92222222, 0.93333333, 0.93703704,\n","       0.93703704, 0.91851852, 0.92962963, 0.91481481, 0.91851852,\n","       0.92222222, 0.94444444, 0.95185185, 0.94444444, 0.93703704,\n","       0.94444444, 0.95185185, 0.93333333, 0.92962963, 0.93703704,\n","       0.95185185, 0.94814815, 0.92592593, 0.93333333, 0.92592593,\n","       0.93703704, 0.87777778, 0.92962963, 0.94814815, 0.94814815,\n","       0.91111111, 0.91111111, 0.93703704, 0.94074074, 0.93333333,\n","       0.91481481, 0.92222222, 0.94814815, 0.94074074, 0.92962963,\n","       0.92592593, 0.92222222, 0.92222222, 0.93333333, 0.93703704,\n","       0.93703704, 0.93333333, 0.93333333, 0.92962963, 0.94074074,\n","       0.91851852, 0.94444444, 0.94074074, 0.94074074, 0.92222222,\n","       0.92222222, 0.92222222, 0.91481481, 0.91851852, 0.9       ,\n","       0.92592593, 0.94444444, 0.92962963, 0.9       , 0.93703704,\n","       0.94444444, 0.93703704, 0.90740741, 0.95185185, 0.94074074,\n","       0.92962963, 0.9037037 , 0.92962963, 0.92592593, 0.9037037 ,\n","       0.8962963 , 0.91111111, 0.94074074, 0.93333333, 0.89259259,\n","       0.91851852, 0.94814815, 0.94074074, 0.88888889, 0.92592593,\n","       0.94074074, 0.93333333, 0.91111111, 0.91481481, 0.92962963,\n","       0.91111111, 0.91851852, 0.92592593, 0.92592593, 0.91481481,\n","       0.92592593, 0.93333333, 0.94444444, 0.92222222, 0.94074074,\n","       0.94074074, 0.92962963, 0.92222222, 0.91481481, 0.92962963,\n","       0.92222222, 0.92592593, 0.9       , 0.92592593, 0.94074074,\n","       0.92222222, 0.9037037 , 0.94074074, 0.93703704, 0.92592593,\n","       0.9       , 0.94444444, 0.94814815, 0.93333333, 0.91111111,\n","       0.88518519, 0.91481481, 0.89259259, 0.9037037 , 0.91111111,\n","       0.94444444, 0.91851852, 0.90740741, 0.9037037 , 0.91481481,\n","       0.91851852, 0.9       , 0.93703704, 0.92592593, 0.92592593,\n","       0.92222222, 0.92962963, 0.91851852, 0.91481481, 0.91481481,\n","       0.93333333, 0.93703704, 0.93703704, 0.93703704, 0.92222222,\n","       0.92592593, 0.93703704, 0.93703704, 0.94074074, 0.92962963,\n","       0.92592593, 0.92222222, 0.92592593, 0.92222222, 0.91481481,\n","       0.9037037 , 0.94074074, 0.94074074, 0.92222222, 0.92222222,\n","       0.94814815, 0.95185185, 0.92592593, 0.90740741, 0.95185185,\n","       0.95185185, 0.93333333, 0.91481481, 0.88888889, 0.93703704,\n","       0.8962963 , 0.90740741, 0.91481481, 0.92962963, 0.93333333,\n","       0.8962963 , 0.94444444, 0.92222222, 0.92962963, 0.9       ,\n","       0.91111111, 0.95555556, 0.92592593]), 'std_test_score': array([0.        , 0.        , 0.        , 0.        , 0.02962963,\n","       0.02222222, 0.03703704, 0.04444444, 0.01111111, 0.01481481,\n","       0.01481481, 0.03333333, 0.        , 0.00740741, 0.01481481,\n","       0.03333333, 0.        , 0.        , 0.        , 0.        ,\n","       0.0037037 , 0.00740741, 0.01111111, 0.02962963, 0.00740741,\n","       0.        , 0.        , 0.02592593, 0.        , 0.00740741,\n","       0.0037037 , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.01111111, 0.0037037 , 0.0037037 , 0.04444444,\n","       0.0037037 , 0.0037037 , 0.01481481, 0.02222222, 0.01481481,\n","       0.        , 0.        , 0.01481481, 0.        , 0.        ,\n","       0.        , 0.        , 0.02962963, 0.02592593, 0.02592593,\n","       0.04074074, 0.00740741, 0.01481481, 0.02592593, 0.04814815,\n","       0.00740741, 0.0037037 , 0.00740741, 0.01851852, 0.        ,\n","       0.        , 0.        , 0.        , 0.00740741, 0.02222222,\n","       0.01851852, 0.05555556, 0.00740741, 0.        , 0.0037037 ,\n","       0.0037037 , 0.04074074, 0.        , 0.00740741, 0.02222222,\n","       0.        , 0.        , 0.        , 0.        , 0.03333333,\n","       0.01111111, 0.00740741, 0.03333333, 0.02962963, 0.00740741,\n","       0.01481481, 0.02222222, 0.05185185, 0.04074074, 0.00740741,\n","       0.02592593, 0.        , 0.        , 0.        , 0.        ,\n","       0.02592593, 0.03333333, 0.02962963, 0.01111111, 0.00740741,\n","       0.01851852, 0.02962963, 0.04074074, 0.00740741, 0.00740741,\n","       0.        , 0.01851852, 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.01111111, 0.01481481, 0.03333333,\n","       0.0037037 , 0.0037037 , 0.00740741, 0.03333333, 0.        ,\n","       0.0037037 , 0.01481481, 0.02222222, 0.        , 0.        ,\n","       0.        , 0.        , 0.03703704, 0.01111111, 0.00740741,\n","       0.04074074, 0.04074074, 0.02222222, 0.00740741, 0.01851852,\n","       0.03703704, 0.03333333, 0.0037037 , 0.01481481, 0.        ,\n","       0.        , 0.        , 0.        , 0.02592593, 0.02592593,\n","       0.02962963, 0.07037037, 0.01111111, 0.01481481, 0.01481481,\n","       0.02222222, 0.00740741, 0.01111111, 0.00740741, 0.02222222,\n","       0.        , 0.        , 0.        , 0.        , 0.00740741,\n","       0.00740741, 0.01481481, 0.03333333, 0.0037037 , 0.        ,\n","       0.00740741, 0.01481481, 0.0037037 , 0.00740741, 0.        ,\n","       0.01851852, 0.        , 0.        , 0.        , 0.        ,\n","       0.02592593, 0.01111111, 0.0037037 , 0.05555556, 0.03333333,\n","       0.0037037 , 0.        , 0.03333333, 0.05185185, 0.02962963,\n","       0.00740741, 0.01111111, 0.02592593, 0.05185185, 0.04074074,\n","       0.02962963, 0.0037037 , 0.        , 0.01481481, 0.02592593,\n","       0.        , 0.0037037 , 0.01481481, 0.02222222, 0.00740741,\n","       0.00740741, 0.00740741, 0.02222222, 0.01851852, 0.0037037 ,\n","       0.01481481, 0.04074074, 0.01111111, 0.00740741, 0.0037037 ,\n","       0.01481481, 0.00740741, 0.00740741, 0.        , 0.01481481,\n","       0.01481481, 0.01851852, 0.02222222, 0.01481481, 0.00740741,\n","       0.        , 0.04444444, 0.06296296, 0.00740741, 0.0037037 ,\n","       0.        , 0.        , 0.00740741, 0.01111111, 0.01481481,\n","       0.0037037 , 0.03703704, 0.04814815, 0.        , 0.0037037 ,\n","       0.04074074, 0.05185185, 0.02222222, 0.05185185, 0.01111111,\n","       0.00740741, 0.00740741, 0.03333333, 0.0037037 , 0.0037037 ,\n","       0.00740741, 0.01481481, 0.0037037 , 0.0037037 , 0.        ,\n","       0.01851852, 0.00740741, 0.01481481, 0.02962963, 0.04444444,\n","       0.01481481, 0.0037037 , 0.02222222, 0.03333333, 0.02592593,\n","       0.02222222, 0.0037037 , 0.01111111, 0.03703704, 0.01111111,\n","       0.00740741, 0.01111111, 0.02222222, 0.0037037 , 0.02222222,\n","       0.05925926, 0.02962963, 0.03333333, 0.01111111, 0.02962963,\n","       0.04074074, 0.01111111, 0.00740741, 0.01851852, 0.04074074,\n","       0.04444444, 0.01851852, 0.        , 0.03703704, 0.02592593,\n","       0.05555556, 0.01481481, 0.0037037 , 0.0037037 , 0.01481481,\n","       0.01481481, 0.00740741, 0.        , 0.00740741, 0.02592593,\n","       0.        , 0.0037037 , 0.0037037 , 0.0037037 , 0.01481481,\n","       0.02592593, 0.03333333, 0.00740741, 0.0037037 , 0.        ,\n","       0.0037037 , 0.01851852, 0.01111111, 0.01481481, 0.00740741,\n","       0.02962963, 0.00740741, 0.01111111, 0.00740741, 0.01481481,\n","       0.01851852, 0.00740741, 0.0037037 , 0.07407407, 0.04814815,\n","       0.01851852, 0.01111111, 0.02222222, 0.03333333, 0.04074074,\n","       0.0037037 , 0.        , 0.04074074, 0.04444444, 0.02222222,\n","       0.00740741, 0.02592593, 0.03703704, 0.02592593, 0.07407407,\n","       0.00740741, 0.01851852, 0.0037037 , 0.03333333, 0.0037037 ,\n","       0.        , 0.0037037 , 0.02222222, 0.0037037 , 0.        ,\n","       0.0037037 , 0.        , 0.01481481, 0.02962963, 0.02592593,\n","       0.02222222, 0.01481481, 0.0037037 , 0.00740741, 0.02592593,\n","       0.00740741, 0.        , 0.00740741, 0.01111111, 0.00740741,\n","       0.01481481, 0.        , 0.0037037 , 0.02962963, 0.01111111,\n","       0.01111111, 0.0037037 , 0.04074074, 0.02962963, 0.        ,\n","       0.01481481, 0.03703704, 0.00740741, 0.01481481, 0.0037037 ,\n","       0.04444444, 0.04074074, 0.01851852, 0.0037037 , 0.01481481,\n","       0.02962963, 0.02222222, 0.04074074, 0.01481481, 0.0037037 ,\n","       0.        , 0.02962963, 0.0037037 , 0.0037037 , 0.00740741,\n","       0.01111111, 0.        , 0.        , 0.01111111, 0.00740741,\n","       0.0037037 , 0.00740741, 0.01851852, 0.05185185, 0.0037037 ,\n","       0.01111111, 0.01481481, 0.01111111, 0.01851852, 0.0037037 ,\n","       0.01111111, 0.01481481, 0.02222222, 0.01851852, 0.01481481,\n","       0.01111111, 0.0037037 , 0.0037037 , 0.0037037 , 0.04074074,\n","       0.01481481, 0.        , 0.00740741, 0.01481481, 0.00740741,\n","       0.02222222, 0.01111111, 0.        , 0.04814815, 0.01851852,\n","       0.02962963, 0.        , 0.01111111, 0.01111111, 0.01481481,\n","       0.01111111, 0.0037037 , 0.00740741, 0.00740741, 0.02222222,\n","       0.        , 0.0037037 , 0.0037037 , 0.01851852, 0.0037037 ,\n","       0.        , 0.0037037 , 0.00740741, 0.00740741, 0.00740741,\n","       0.01111111, 0.03703704, 0.04074074, 0.01851852, 0.01851852,\n","       0.01481481, 0.03703704, 0.02222222, 0.00740741, 0.01111111,\n","       0.03703704, 0.01481481, 0.01481481, 0.01111111, 0.04074074,\n","       0.00740741, 0.02222222, 0.03703704, 0.04074074, 0.02592593,\n","       0.01481481, 0.0037037 , 0.04074074, 0.04814815, 0.02962963,\n","       0.01481481, 0.03703704, 0.04444444, 0.02222222, 0.01111111,\n","       0.01851852, 0.01851852, 0.02592593, 0.04074074, 0.0037037 ,\n","       0.0037037 , 0.0037037 , 0.01111111, 0.0037037 , 0.0037037 ,\n","       0.        , 0.01851852, 0.0037037 , 0.0037037 , 0.0037037 ,\n","       0.0037037 , 0.        , 0.01481481, 0.0037037 , 0.02222222,\n","       0.        , 0.00740741, 0.        , 0.02222222, 0.0037037 ,\n","       0.01481481, 0.01111111, 0.0037037 , 0.01111111, 0.01111111,\n","       0.01481481, 0.01111111, 0.03703704, 0.02592593, 0.0037037 ,\n","       0.01851852, 0.04444444, 0.02592593, 0.00740741, 0.0037037 ,\n","       0.03703704, 0.03703704, 0.00740741, 0.01111111, 0.03703704,\n","       0.04814815, 0.01481481, 0.        , 0.01481481, 0.02592593,\n","       0.01111111, 0.04444444, 0.00740741, 0.0037037 , 0.01481481,\n","       0.01851852, 0.0037037 , 0.0037037 , 0.00740741, 0.01851852,\n","       0.0037037 , 0.00740741, 0.00740741, 0.02222222, 0.        ,\n","       0.01111111, 0.01851852, 0.02592593, 0.01851852, 0.01111111,\n","       0.00740741, 0.01111111, 0.01481481, 0.01111111, 0.01111111,\n","       0.0037037 , 0.00740741, 0.02222222, 0.01111111, 0.00740741,\n","       0.01481481, 0.01111111, 0.01481481, 0.04074074, 0.04074074,\n","       0.02592593, 0.01481481, 0.01481481, 0.05185185, 0.03703704,\n","       0.01481481, 0.        , 0.04074074, 0.04444444, 0.04814815,\n","       0.        , 0.        , 0.01111111, 0.01851852, 0.01111111,\n","       0.0037037 , 0.0037037 , 0.        , 0.01111111, 0.01481481,\n","       0.0037037 , 0.01111111, 0.0037037 , 0.        , 0.0037037 ,\n","       0.0037037 , 0.01481481, 0.0037037 , 0.01851852, 0.01481481,\n","       0.01111111, 0.01851852, 0.01851852, 0.01851852, 0.0037037 ,\n","       0.02592593, 0.01851852, 0.        , 0.0037037 , 0.02592593,\n","       0.01851852, 0.01481481, 0.00740741, 0.00740741, 0.        ,\n","       0.01851852, 0.03333333, 0.01111111, 0.01481481, 0.02222222,\n","       0.02222222, 0.05185185, 0.01111111, 0.00740741, 0.        ,\n","       0.04814815, 0.04074074, 0.01481481, 0.00740741, 0.0037037 ,\n","       0.00740741, 0.02592593, 0.02592593, 0.00740741, 0.0037037 ,\n","       0.0037037 , 0.        , 0.00740741, 0.0037037 , 0.00740741,\n","       0.01481481, 0.0037037 , 0.        , 0.00740741, 0.01111111,\n","       0.0037037 , 0.0037037 , 0.0037037 , 0.01481481, 0.03333333,\n","       0.00740741, 0.01851852, 0.0037037 , 0.03333333, 0.01111111,\n","       0.0037037 , 0.0037037 , 0.04814815, 0.01851852, 0.00740741,\n","       0.0037037 , 0.03703704, 0.01111111, 0.00740741, 0.02222222,\n","       0.04444444, 0.04444444, 0.00740741, 0.        , 0.04074074,\n","       0.05185185, 0.02222222, 0.00740741, 0.03703704, 0.02962963,\n","       0.02962963, 0.        , 0.02222222, 0.01851852, 0.01851852,\n","       0.03703704, 0.01481481, 0.00740741, 0.00740741, 0.01851852,\n","       0.01481481, 0.        , 0.01111111, 0.0037037 , 0.        ,\n","       0.00740741, 0.0037037 , 0.01111111, 0.0037037 , 0.0037037 ,\n","       0.01111111, 0.00740741, 0.04074074, 0.02962963, 0.00740741,\n","       0.01111111, 0.02962963, 0.00740741, 0.0037037 , 0.00740741,\n","       0.03333333, 0.01851852, 0.02222222, 0.        , 0.04444444,\n","       0.03333333, 0.0037037 , 0.03333333, 0.03703704, 0.04444444,\n","       0.01111111, 0.01481481, 0.04814815, 0.03703704, 0.03333333,\n","       0.01481481, 0.04074074, 0.02592593, 0.04444444, 0.00740741,\n","       0.01111111, 0.0037037 , 0.01481481, 0.03333333, 0.01851852,\n","       0.        , 0.0037037 , 0.0037037 , 0.0037037 , 0.01111111,\n","       0.00740741, 0.01111111, 0.0037037 , 0.00740741, 0.0037037 ,\n","       0.        , 0.01111111, 0.00740741, 0.01111111, 0.01111111,\n","       0.03703704, 0.01481481, 0.00740741, 0.01111111, 0.01851852,\n","       0.01481481, 0.01851852, 0.00740741, 0.04074074, 0.01851852,\n","       0.01111111, 0.        , 0.04074074, 0.01481481, 0.0037037 ,\n","       0.02222222, 0.04074074, 0.04814815, 0.0037037 , 0.        ,\n","       0.03703704, 0.02592593, 0.04074074, 0.0037037 , 0.04074074,\n","       0.04444444, 0.01481481, 0.00740741]), 'rank_test_score': array([721, 721, 721, 721, 417, 545, 530, 698, 361, 417, 417, 628, 164,\n","       295, 417, 474, 721, 721, 721, 721, 228, 295, 568, 595, 295, 164,\n","       164, 671, 164,  69, 121, 417, 721, 721, 721, 721, 361, 228, 485,\n","       712, 121, 228, 417, 545, 295, 295, 295, 417, 721, 721, 721, 721,\n","       417, 568, 568, 474, 295, 417, 361, 628, 295, 228, 295, 485, 721,\n","       721, 721, 721, 417, 545, 628, 701, 295, 164, 228, 692, 568, 164,\n","        69, 545, 721, 721, 721, 721, 568, 485, 545, 671, 595, 417, 417,\n","       656, 595, 568,  69, 568, 721, 721, 721, 721, 361, 474, 595, 692,\n","       295, 485, 417, 568, 295, 295, 164, 485, 721, 721, 721, 721, 164,\n","       361, 595, 628, 228, 228, 295, 474, 164, 228,  13, 545, 721, 721,\n","       721, 721, 530, 485, 545, 692, 568, 688, 295, 628, 656, 628, 361,\n","       595, 721, 721, 721, 721, 568, 568, 417, 671, 361, 417, 417, 595,\n","       295, 361, 295, 545, 721, 721, 721, 721, 295, 295, 595, 671, 361,\n","       164, 295, 595, 228, 295, 164, 485, 721, 721, 721, 721, 628, 361,\n","       485, 628, 628, 228, 417, 628, 595, 530, 417, 568, 485, 595, 671,\n","       718, 121, 164, 417, 361, 164, 228, 417, 545,  69, 164, 295, 545,\n","       485, 485, 656, 701,  35, 164, 228, 417, 164,  69, 164, 417,  13,\n","         4,  13, 417,  69, 417, 706, 708, 164, 228, 164, 417, 417,  35,\n","        69, 121, 595, 485, 164, 121, 568, 417, 656, 718, 361, 295, 295,\n","       474, 228, 228, 295, 417, 228, 121, 164, 485, 545, 595, 595, 716,\n","       164, 121, 545, 474, 628, 115, 121, 361, 595,  35,  13, 361, 656,\n","       485, 295, 706, 656, 628, 121, 595, 671, 228, 164, 485, 671, 530,\n","        35, 164, 530, 485, 701, 716, 228, 228, 417, 417, 295, 164,  69,\n","       361, 295, 228, 228, 228, 295, 568, 628, 712, 361, 164, 228, 485,\n","        35,  69,  69, 595, 164,  35,  69, 417, 671, 545, 485, 705, 628,\n","       628,  35, 545, 628, 568, 121, 164, 628, 530,  13,  69, 568, 530,\n","       568, 720, 295, 485, 228, 474, 228, 164, 228, 545, 121, 164, 228,\n","       164, 545, 595, 671, 714, 417, 228,  69, 671,  69, 164,  69, 361,\n","       164,  69, 164, 228, 595, 485, 568, 715, 671, 656, 164, 417, 595,\n","       417,  13, 228, 656, 568, 121, 121, 417, 417, 295, 692, 417, 121,\n","       164, 417,  35, 121, 295, 361,  69,  69, 228, 295, 228, 295, 485,\n","       698, 228,  35,  69, 361,  35,  35,  35, 417, 115,  35,  13, 361,\n","       485, 361, 485, 710, 295, 164, 164, 164, 295, 115,  35, 164, 485,\n","        35, 115, 164, 361, 361, 545, 708, 228,  69,  69, 545, 164, 228,\n","       228, 228, 121, 164, 228, 295, 295, 295, 568, 530, 568, 228,  35,\n","       417, 595,  13,  13, 361, 595,  69,  13, 361, 474, 295, 295, 698,\n","       671, 671,  69, 228, 671, 485, 295, 417, 656, 530,  13, 361, 228,\n","       485, 568, 671, 121, 228, 121, 361, 228, 121, 164, 485, 228, 121,\n","       228, 228, 295, 417, 485, 545, 295,  69, 164, 595, 655,  69,  35,\n","       121, 121,   4,  13, 361, 595, 628, 485, 361, 656, 161,  69, 121,\n","       595, 595,  69, 361, 656, 485,  13, 164, 417, 361, 361, 656, 295,\n","       121, 417, 485, 228, 228, 295, 485, 228,  69,  69, 545, 164,  35,\n","       485, 568, 361,  35,  69, 361,  69,  35,  35, 228, 417, 115,  35,\n","       295, 688, 361, 417, 710, 628, 628,   1, 417, 595, 417,  13, 164,\n","       628, 530, 485, 164,  13, 361, 485, 692, 228, 228, 164, 361,   1,\n","        35,  35, 361, 164, 121, 121, 417, 228, 485, 417, 361,  35,   4,\n","        35, 121,  35,   4, 164, 228, 161,   4,  13, 295, 164, 295, 121,\n","       701, 228,  13,  13, 545, 545, 121,  69, 164, 485, 361,  13,  69,\n","       228, 295, 361, 361, 164, 121, 121, 164, 164, 228,  69, 417,  35,\n","        69,  69, 361, 361, 361, 485, 417, 628, 295,  35, 228, 628, 121,\n","        35, 121, 568,   4,  69, 228, 595, 228, 295, 595, 656, 530,  69,\n","       164, 671, 417,  13,  69, 688, 295, 115, 164, 545, 485, 228, 530,\n","       417, 295, 295, 485, 295, 164,  35, 361,  69,  69, 228, 361, 485,\n","       228, 361, 295, 628, 295,  69, 361, 595,  69, 121, 295, 628,  35,\n","        13, 164, 530, 692, 485, 671, 595, 530,  35, 417, 568, 595, 474,\n","       417, 628, 161, 295, 295, 361, 228, 417, 474, 485, 164, 121, 121,\n","       121, 361, 295, 121, 121,  69, 228, 295, 361, 295, 361, 485, 595,\n","        69,  69, 361, 361,  13,   4, 295, 568,   4,   4, 164, 474, 688,\n","       121, 656, 568, 485, 228, 164, 656,  35, 361, 228, 628, 530,   1,\n","       295])}\n"]}],"source":["print(grid.best_estimator_)\n","print(grid.best_score_)\n","print(grid.best_params_)\n","print(grid.cv_results_)"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy scores for each training fold:  [0.9838709677419355, 0.9839357429718876, 0.9919678714859438, 0.9839357429718876, 0.9959839357429718]\n","Accuracy scores for each testing fold:  [0.9523809523809523, 1.0, 0.7903225806451613, 0.9838709677419355, 0.7580645161290323]\n","Traning Mean accuracy score:  0.9879388521829252\n","Testing Mean accuracy score:  0.8969278033794164\n"]}],"source":["opt_gb_clf = GradientBoostingClassifier(learning_rate=0.02, \n","                                        max_depth= 6, \n","                                        subsample=0.5, \n","                                        n_estimators = 70, \n","                                        max_features = 7)\n","\n","\n","opt_gb_clf.fit(X_train, y_train)\n","splitSmote(opt_gb_clf)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["the best until now\n","\n","learning_rate=0.03, \n","                                        max_depth= 6, \n","                                        subsample=0.5, \n","                                        n_estimators = 50, \n","                                        max_features = 7)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 4.4 LogisticRegression"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Before optimization"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy scores for each training fold:  [0.9314516129032258, 0.8955823293172691, 0.9598393574297188, 0.8995983935742972, 0.927710843373494]\n","Accuracy scores for each testing fold:  [0.8888888888888888, 1.0, 0.7903225806451613, 0.967741935483871, 0.6451612903225806]\n","Traning Mean accuracy score:  0.922836507319601\n","Testing Mean accuracy score:  0.8584229390681004\n"]}],"source":["#LogisticRegression\n","\n","#Grid search cross validation\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.linear_model import LogisticRegression\n","grid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\",\"elasticnet\"], 'solver'  : ['newton-cg', 'lbfgs', 'liblinear']}\n","logreg=LogisticRegression()\n","lr=GridSearchCV(logreg,grid,cv=10)\n","# lr.fit(X_train,y_train)\n","\n","# a=lr.best_params_\n","# b=lr.best_score_\n","# print(\"tuned hpyerparameters :(best parameters) \",lr.best_params_)\n","# print(\"accuracy :\",lr.best_score_)\n","# print(\"Best estimator: \",lr.best_estimator_ )\n","\n","#y_pred.append(pd.Series(lr.predict(X_test), name='LogisticRegression'))\n","\n","# precision, recall, fscore, _ = score(y_test, lr.predict(X_test), average='weighted')\n","# accuracy = accuracy_score(y_test, lr.predict(X_test))\n","# metrics.append(pd.Series({'precision':precision, 'recall':recall,'fscore':fscore, 'accuracy':accuracy}))\n","\n","#LogisticRegression\n","#from sklearn.linear_model import LogisticRegressionCV\n","#lr= LogisticRegressionCV(Cs=a['C'], penalty=a['penalty'], solver=a['solver']).fit(X_train, y_train)\n","\n","splitSmote (lr)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["After optimization"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#TODO: write LogisticRegression optimization code here"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 4.5 Random Forest"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Before optimization"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy scores for each training fold:  [1.0, 1.0, 1.0, 1.0, 1.0]\n","Accuracy scores for each testing fold:  [0.9523809523809523, 1.0, 0.7903225806451613, 0.967741935483871, 0.7741935483870968]\n","Traning Mean accuracy score:  1.0\n","Testing Mean accuracy score:  0.8969278033794164\n"]}],"source":["#RandomForestClassifier\n","RF= RandomForestClassifier(random_state= 0)\n","# RF.fit(X_train,y_train)\n","\n","# #y_pred.append(pd.Series(RF.predict(X_test), name='RandomForestClassifier'))\n","\n","# precision, recall, fscore, _ = score(y_test, RF.predict(X_test), average='weighted')\n","# accuracy = accuracy_score(y_test, RF.predict(X_test))\n","# metrics.append(pd.Series({'precision':precision, 'recall':recall,'fscore':fscore, 'accuracy':accuracy}))\n","\n","\n","splitSmote (RF)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["After optimization"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["random_search\n","Accuracy scores for each training fold:  [0.9516129032258065, 0.9799196787148594, 0.9879518072289156, 0.9236947791164659, 0.9839357429718876]\n","Accuracy scores for each testing fold:  [0.9365079365079365, 1.0, 0.7903225806451613, 0.9516129032258065, 0.7580645161290323]\n","Traning Mean accuracy score:  0.965422982251587\n","Testing Mean accuracy score:  0.8873015873015874\n"]}],"source":["#TODO: Write RandomForest optimization code here\n","\n","#RandomForest optimization\n","random_grid={\n","\"max_depth\":[2,4,6],\n","\"min_samples_split\":[2,5,10],\n","\"min_samples_leaf\":[1,2,4]}\n","random_search= GridSearchCV(estimator=RF,param_grid=random_grid,cv=5)\n","\n","\n","print(\"random_search\")\n","random_search.fit(X_train, y_train)\n","splitSmote(random_search)\n"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.9407407407407407\n","{'max_depth': 6, 'min_samples_leaf': 2, 'min_samples_split': 5}\n","{'mean_fit_time': array([0.10915833, 0.10873075, 0.10442495, 0.10606613, 0.10393457,\n","       0.1043633 , 0.10886307, 0.11078372, 0.11789246, 0.11568594,\n","       0.11351151, 0.10705094, 0.10543561, 0.11611819, 0.10590014,\n","       0.10392418, 0.10913391, 0.10713654, 0.10808372, 0.10603871,\n","       0.10830426, 0.10976267, 0.10553489, 0.1068552 , 0.10562015,\n","       0.10437803, 0.10872912]), 'std_fit_time': array([3.45143893e-03, 3.92021144e-03, 1.02617642e-03, 3.10341403e-03,\n","       1.48566726e-05, 8.66248013e-04, 6.30879043e-03, 9.06395404e-03,\n","       5.92463911e-03, 1.45218046e-02, 5.98650097e-03, 4.61625434e-03,\n","       4.36142252e-03, 1.61829348e-02, 3.09842999e-03, 1.34283643e-05,\n","       3.46748557e-03, 3.92682770e-03, 3.28357460e-03, 3.09782790e-03,\n","       5.66847922e-03, 5.94912407e-03, 3.20990102e-03, 3.60520352e-03,\n","       3.15721474e-03, 8.79943959e-04, 3.93145805e-03]), 'mean_score_time': array([0.00800824, 0.00959697, 0.00800362, 0.00959997, 0.01278086,\n","       0.0096015 , 0.0095933 , 0.0095942 , 0.00798049, 0.01280465,\n","       0.01178083, 0.0127912 , 0.01279249, 0.01279125, 0.0079947 ,\n","       0.00799408, 0.00800071, 0.00799274, 0.00978241, 0.00959201,\n","       0.00959373, 0.01278591, 0.01118035, 0.01011348, 0.0114224 ,\n","       0.0111927 , 0.00799351]), 'std_score_time': array([1.52311970e-05, 3.19831398e-03, 1.23652619e-05, 3.19290539e-03,\n","       3.92012673e-03, 3.19464244e-03, 3.19860038e-03, 3.19743179e-03,\n","       1.69586517e-05, 3.91365894e-03, 3.60910852e-03, 3.93022503e-03,\n","       3.91722584e-03, 3.91640795e-03, 7.44843452e-07, 1.90734863e-07,\n","       1.29482143e-05, 3.68431926e-06, 3.12611059e-03, 3.19864819e-03,\n","       3.19802765e-03, 3.92428364e-03, 3.91347565e-03, 3.10513115e-03,\n","       4.21411502e-03, 3.91631048e-03, 3.04728419e-06]), 'param_max_depth': masked_array(data=[2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","                   6, 6, 6, 6, 6, 6, 6, 6, 6],\n","             mask=[False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False],\n","       fill_value='?',\n","            dtype=object), 'param_min_samples_leaf': masked_array(data=[1, 1, 1, 2, 2, 2, 4, 4, 4, 1, 1, 1, 2, 2, 2, 4, 4, 4,\n","                   1, 1, 1, 2, 2, 2, 4, 4, 4],\n","             mask=[False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False],\n","       fill_value='?',\n","            dtype=object), 'param_min_samples_split': masked_array(data=[2, 5, 10, 2, 5, 10, 2, 5, 10, 2, 5, 10, 2, 5, 10, 2, 5,\n","                   10, 2, 5, 10, 2, 5, 10, 2, 5, 10],\n","             mask=[False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False, False, False, False, False, False,\n","                   False, False, False],\n","       fill_value='?',\n","            dtype=object), 'params': [{'max_depth': 2, 'min_samples_leaf': 1, 'min_samples_split': 2}, {'max_depth': 2, 'min_samples_leaf': 1, 'min_samples_split': 5}, {'max_depth': 2, 'min_samples_leaf': 1, 'min_samples_split': 10}, {'max_depth': 2, 'min_samples_leaf': 2, 'min_samples_split': 2}, {'max_depth': 2, 'min_samples_leaf': 2, 'min_samples_split': 5}, {'max_depth': 2, 'min_samples_leaf': 2, 'min_samples_split': 10}, {'max_depth': 2, 'min_samples_leaf': 4, 'min_samples_split': 2}, {'max_depth': 2, 'min_samples_leaf': 4, 'min_samples_split': 5}, {'max_depth': 2, 'min_samples_leaf': 4, 'min_samples_split': 10}, {'max_depth': 4, 'min_samples_leaf': 1, 'min_samples_split': 2}, {'max_depth': 4, 'min_samples_leaf': 1, 'min_samples_split': 5}, {'max_depth': 4, 'min_samples_leaf': 1, 'min_samples_split': 10}, {'max_depth': 4, 'min_samples_leaf': 2, 'min_samples_split': 2}, {'max_depth': 4, 'min_samples_leaf': 2, 'min_samples_split': 5}, {'max_depth': 4, 'min_samples_leaf': 2, 'min_samples_split': 10}, {'max_depth': 4, 'min_samples_leaf': 4, 'min_samples_split': 2}, {'max_depth': 4, 'min_samples_leaf': 4, 'min_samples_split': 5}, {'max_depth': 4, 'min_samples_leaf': 4, 'min_samples_split': 10}, {'max_depth': 6, 'min_samples_leaf': 1, 'min_samples_split': 2}, {'max_depth': 6, 'min_samples_leaf': 1, 'min_samples_split': 5}, {'max_depth': 6, 'min_samples_leaf': 1, 'min_samples_split': 10}, {'max_depth': 6, 'min_samples_leaf': 2, 'min_samples_split': 2}, {'max_depth': 6, 'min_samples_leaf': 2, 'min_samples_split': 5}, {'max_depth': 6, 'min_samples_leaf': 2, 'min_samples_split': 10}, {'max_depth': 6, 'min_samples_leaf': 4, 'min_samples_split': 2}, {'max_depth': 6, 'min_samples_leaf': 4, 'min_samples_split': 5}, {'max_depth': 6, 'min_samples_leaf': 4, 'min_samples_split': 10}], 'split0_test_score': array([0.92592593, 0.92592593, 0.92592593, 0.92592593, 0.92592593,\n","       0.92592593, 0.92592593, 0.92592593, 0.92592593, 0.98148148,\n","       0.98148148, 0.98148148, 0.98148148, 0.98148148, 1.        ,\n","       1.        , 1.        , 0.98148148, 0.98148148, 0.98148148,\n","       0.98148148, 0.98148148, 1.        , 1.        , 1.        ,\n","       1.        , 1.        ]), 'split1_test_score': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 'split2_test_score': array([0.7962963 , 0.7962963 , 0.7962963 , 0.7962963 , 0.7962963 ,\n","       0.7962963 , 0.7962963 , 0.7962963 , 0.7962963 , 0.87037037,\n","       0.87037037, 0.88888889, 0.87037037, 0.87037037, 0.87037037,\n","       0.77777778, 0.77777778, 0.77777778, 0.88888889, 0.88888889,\n","       0.87037037, 0.87037037, 0.88888889, 0.87037037, 0.88888889,\n","       0.88888889, 0.88888889]), 'split3_test_score': array([0.96296296, 0.96296296, 0.96296296, 0.96296296, 0.96296296,\n","       0.96296296, 0.96296296, 0.96296296, 0.96296296, 0.98148148,\n","       0.98148148, 0.98148148, 0.98148148, 0.98148148, 0.98148148,\n","       0.98148148, 0.98148148, 0.98148148, 0.98148148, 0.98148148,\n","       0.98148148, 0.98148148, 0.98148148, 0.98148148, 0.98148148,\n","       0.98148148, 0.98148148]), 'split4_test_score': array([0.83333333, 0.83333333, 0.83333333, 0.83333333, 0.83333333,\n","       0.83333333, 0.83333333, 0.83333333, 0.83333333, 0.83333333,\n","       0.83333333, 0.83333333, 0.83333333, 0.83333333, 0.83333333,\n","       0.83333333, 0.83333333, 0.83333333, 0.81481481, 0.81481481,\n","       0.81481481, 0.83333333, 0.83333333, 0.83333333, 0.83333333,\n","       0.83333333, 0.83333333]), 'mean_test_score': array([0.9037037 , 0.9037037 , 0.9037037 , 0.9037037 , 0.9037037 ,\n","       0.9037037 , 0.9037037 , 0.9037037 , 0.9037037 , 0.93333333,\n","       0.93333333, 0.93703704, 0.93333333, 0.93333333, 0.93703704,\n","       0.91851852, 0.91851852, 0.91481481, 0.93333333, 0.93333333,\n","       0.92962963, 0.93333333, 0.94074074, 0.93703704, 0.94074074,\n","       0.94074074, 0.94074074]), 'std_test_score': array([0.07715802, 0.07715802, 0.07715802, 0.07715802, 0.07715802,\n","       0.07715802, 0.07715802, 0.07715802, 0.07715802, 0.06789001,\n","       0.06789001, 0.06478835, 0.06789001, 0.06789001, 0.07085602,\n","       0.0941353 , 0.0941353 , 0.09117432, 0.07085602, 0.07085602,\n","       0.07351642, 0.06789001, 0.06768766, 0.07085602, 0.06768766,\n","       0.06768766, 0.06768766]), 'rank_test_score': array([19, 19, 19, 19, 19, 19, 19, 19, 19,  8,  8,  5,  8,  8,  5, 16, 16,\n","       18,  8,  8, 15,  8,  1,  5,  1,  1,  1])}\n"]}],"source":["#print(random_search.best_estimator_)\n","print(random_search.best_score_)\n","print(random_search.best_params_)\n","print(random_search.cv_results_)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 5. Training"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 5.1 Before Optimazation "]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["KNN\n","Accuracy scores for each fold:  [0.9365079365079365, 0.9193548387096774, 0.7903225806451613, 0.9516129032258065, 0.7741935483870968]\n","Mean accuracy score:  0.8743983614951356\n","GradientBoosting\n","Accuracy scores for each fold:  [0.9682539682539683, 1.0, 0.7903225806451613, 0.9838709677419355, 0.7419354838709677]\n","Mean accuracy score:  0.8968766001024064\n","LogisticRegression\n","Accuracy scores for each fold:  [0.9047619047619048, 1.0, 0.8225806451612904, 0.967741935483871, 0.6612903225806451]\n","Mean accuracy score:  0.8712749615975423\n","RandomForest\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\rayoo\\AppData\\Local\\Temp\\ipykernel_45220\\3765096017.py:21: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  model.fit(x_sm, y_sm)\n","C:\\Users\\rayoo\\AppData\\Local\\Temp\\ipykernel_45220\\3765096017.py:21: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  model.fit(x_sm, y_sm)\n","C:\\Users\\rayoo\\AppData\\Local\\Temp\\ipykernel_45220\\3765096017.py:21: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  model.fit(x_sm, y_sm)\n","C:\\Users\\rayoo\\AppData\\Local\\Temp\\ipykernel_45220\\3765096017.py:21: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  model.fit(x_sm, y_sm)\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy scores for each fold:  [0.9523809523809523, 1.0, 0.7903225806451613, 0.967741935483871, 0.7741935483870968]\n","Mean accuracy score:  0.8969278033794164\n","DecisionTree\n","Accuracy scores for each fold:  [0.9841269841269841, 0.967741935483871, 0.7903225806451613, 0.9516129032258065, 0.7096774193548387]\n","Mean accuracy score:  0.8806963645673322\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\rayoo\\AppData\\Local\\Temp\\ipykernel_45220\\3765096017.py:21: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  model.fit(x_sm, y_sm)\n"]}],"source":["# Base models \n","kCls=KNeighborsClassifier()\n","print(\"KNN\")\n","splitSmote (kCls)\n","\n","gb_clf2 = GradientBoostingClassifier()\n","print(\"GradientBoosting\")\n","splitSmote (gb_clf2)\n","\n","logreg=LogisticRegression()\n","print(\"LogisticRegression\")\n","splitSmote (logreg)\n","\n","RF= RandomForestClassifier()\n","print(\"RandomForest\")\n","splitSmote (RF)\n","\n","clf=DecisionTreeClassifier()\n","print(\"DecisionTree\")\n","splitSmote (clf)\n","\n","\n","\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### After Optimization"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#TODO: write the code here"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 5. Evaluation"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 5.1 Before Optimization"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["\n","def report( y_test, pred ):\n","    #report \n","    print(classification_report(y_test,pred,target_names=['0','1','2','3','4','5']))\n","\n","\n","def confusionMatrix():\n","    #confusion_matrix\n","    #the result will show how mwny sucessful predition and wrong from each class\n","\n","    cm = confusion_matrix(y_test, pred)\n","    plt.figure(figsize=(10,7))\n","\n","    sns.heatmap(cm, annot=True, fmt='.2g', cmap='Blues')\n","\n","    # TN   FP\n","    # FN   TP"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"ename":"TypeError","evalue":"first argument must be an iterable of pandas objects, you passed an object of type \"DataFrame\"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m metrics \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mconcat(metrics, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,names\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mDecisionTree\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mDecisionTreeOpt\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mKNN\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mGradientBoosting\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mLogisticRegression\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mRandomForest\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m      2\u001b[0m metrics\u001b[39m.\u001b[39mcolumns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mDecisionTree\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mDecisionTreeOpt\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mKNN\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mGradientBoosting\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mLogisticRegression\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mRandomForest\u001b[39m\u001b[39m'\u001b[39m]\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/reshape/concat.py:368\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[39m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, allowed_args\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mobjs\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    147\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconcat\u001b[39m(\n\u001b[1;32m    148\u001b[0m     objs: Iterable[NDFrame] \u001b[39m|\u001b[39m Mapping[HashableT, NDFrame],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    157\u001b[0m     copy: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    158\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m    159\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[39m    Concatenate pandas objects along a particular axis.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[39m    1   3   4\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 368\u001b[0m     op \u001b[39m=\u001b[39m _Concatenator(\n\u001b[1;32m    369\u001b[0m         objs,\n\u001b[1;32m    370\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[1;32m    371\u001b[0m         ignore_index\u001b[39m=\u001b[39;49mignore_index,\n\u001b[1;32m    372\u001b[0m         join\u001b[39m=\u001b[39;49mjoin,\n\u001b[1;32m    373\u001b[0m         keys\u001b[39m=\u001b[39;49mkeys,\n\u001b[1;32m    374\u001b[0m         levels\u001b[39m=\u001b[39;49mlevels,\n\u001b[1;32m    375\u001b[0m         names\u001b[39m=\u001b[39;49mnames,\n\u001b[1;32m    376\u001b[0m         verify_integrity\u001b[39m=\u001b[39;49mverify_integrity,\n\u001b[1;32m    377\u001b[0m         copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m    378\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[1;32m    379\u001b[0m     )\n\u001b[1;32m    381\u001b[0m     \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39mget_result()\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/reshape/concat.py:403\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    390\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    391\u001b[0m     objs: Iterable[NDFrame] \u001b[39m|\u001b[39m Mapping[HashableT, NDFrame],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    400\u001b[0m     sort\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    401\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(objs, (ABCSeries, ABCDataFrame, \u001b[39mstr\u001b[39m)):\n\u001b[0;32m--> 403\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    404\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mfirst argument must be an iterable of pandas \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    405\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mobjects, you passed an object of type \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(objs)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    406\u001b[0m         )\n\u001b[1;32m    408\u001b[0m     \u001b[39mif\u001b[39;00m join \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mouter\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    409\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintersect \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n","\u001b[0;31mTypeError\u001b[0m: first argument must be an iterable of pandas objects, you passed an object of type \"DataFrame\""]}],"source":["metrics = pd.concat(metrics, axis=1,names=['DecisionTree','DecisionTreeOpt','KNN','GradientBoosting','LogisticRegression','RandomForest'])\n"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>DecisionTree</th>\n","      <th>DecisionTreeOpt</th>\n","      <th>KNN</th>\n","      <th>GradientBoosting</th>\n","      <th>LogisticRegression</th>\n","      <th>RandomForest</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>precision</th>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.733490</td>\n","      <td>0.963374</td>\n","      <td>0.961147</td>\n","      <td>0.944874</td>\n","    </tr>\n","    <tr>\n","      <th>recall</th>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.776596</td>\n","      <td>0.968085</td>\n","      <td>0.968085</td>\n","      <td>0.968085</td>\n","    </tr>\n","    <tr>\n","      <th>fscore</th>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.749230</td>\n","      <td>0.963933</td>\n","      <td>0.962621</td>\n","      <td>0.954647</td>\n","    </tr>\n","    <tr>\n","      <th>accuracy</th>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.776596</td>\n","      <td>0.968085</td>\n","      <td>0.968085</td>\n","      <td>0.968085</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["           DecisionTree  DecisionTreeOpt       KNN  GradientBoosting  \\\n","precision           1.0              1.0  0.733490          0.963374   \n","recall              1.0              1.0  0.776596          0.968085   \n","fscore              1.0              1.0  0.749230          0.963933   \n","accuracy            1.0              1.0  0.776596          0.968085   \n","\n","           LogisticRegression  RandomForest  \n","precision            0.961147      0.944874  \n","recall               0.968085      0.968085  \n","fscore               0.962621      0.954647  \n","accuracy             0.968085      0.968085  "]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["metrics.columns=['DecisionTree','DecisionTreeOpt','KNN','GradientBoosting','LogisticRegression','RandomForest']\n","metrics"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 5.2 After Optimization"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#TODO: write the code here"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
