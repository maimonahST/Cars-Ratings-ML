{"cells":[{"cell_type":"code","execution_count":97,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","import seaborn as sns\n","import matplotlib.pylab as plt\n","#%matplotlib inline\n","\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn import preprocessing\n","from sklearn.preprocessing import OrdinalEncoder\n","from sklearn.impute import SimpleImputer\n","\n","from scipy.stats import norm\n","from scipy import stats\n","from sklearn.impute import KNNImputer\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import precision_recall_fscore_support as score\n","\n","#DS\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import GridSearchCV, cross_val_score\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import classification_report\n","\n","#KNN\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","#GradientBoostingClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","\n","#LogisticRegressionCV\n","from sklearn.linear_model import LogisticRegressionCV\n","\n","#RandomForestClassifier\n","from sklearn.ensemble import RandomForestClassifier\n"]},{"cell_type":"code","execution_count":98,"metadata":{},"outputs":[],"source":["#loading the data from CSV file \n","data=pd.read_csv('finalDataset.csv')\n"]},{"cell_type":"code","execution_count":99,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["The range of feature inputs are within 0.0 to 1.0\n"]}],"source":["# Create a MinMaxScaler object for numrical data\n","scaler = MinMaxScaler()\n","\n","# Scaling the raw input features \n","feature_cols=data.columns[:-1]\n","X= scaler.fit_transform(data[feature_cols])\n","\n","print(f\"The range of feature inputs are within {X.min()} to {X.max()}\")"]},{"cell_type":"code","execution_count":100,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Training dataset shape, X_train: (217, 19), y_train: (217,)\n","Testing dataset shape, X_test: (94, 19), y_test: (94,)\n"]}],"source":["\n","from sklearn.model_selection import StratifiedShuffleSplit\n","\n","# Get the split indexes\n","strat_shuf_split = StratifiedShuffleSplit(n_splits=1, \n","                                          test_size=0.3, random_state=0)\n","\n","train_idx, test_idx = next(strat_shuf_split.split(data[feature_cols], data['Rate']))\n","\n","# Create the dataframes\n","X_train = data.loc[train_idx, feature_cols]\n","y_train = data.loc[train_idx, 'Rate']\n","\n","X_test  = data.loc[test_idx, feature_cols]\n","y_test  = data.loc[test_idx, 'Rate']\n","\n","print(f\"Training dataset shape, X_train: {X_train.shape}, y_train: {y_train.shape}\")\n","print(f\"Testing dataset shape, X_test: {X_test.shape}, y_test: {y_test.shape}\")"]},{"cell_type":"code","execution_count":101,"metadata":{},"outputs":[{"data":{"text/plain":["5    0.718894\n","4    0.161290\n","3    0.092166\n","1    0.009217\n","0    0.009217\n","2    0.009217\n","Name: Rate, dtype: float64"]},"execution_count":101,"metadata":{},"output_type":"execute_result"}],"source":["y_train.value_counts(normalize=True)"]},{"cell_type":"code","execution_count":102,"metadata":{},"outputs":[{"data":{"text/plain":["5    0.723404\n","4    0.159574\n","3    0.085106\n","2    0.010638\n","1    0.010638\n","0    0.010638\n","Name: Rate, dtype: float64"]},"execution_count":102,"metadata":{},"output_type":"execute_result"}],"source":["y_test.value_counts(normalize=True)"]},{"cell_type":"code","execution_count":103,"metadata":{},"outputs":[],"source":["# DecisionTreeClassifier\n","#importing the classfier\n","clf=DecisionTreeClassifier(random_state=0)\n","clf2=clf.fit(X_train,y_train)\n","\n","pred = clf2.predict(X_test)"]},{"cell_type":"code","execution_count":104,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\reham\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Best hyper-param:  {'max_depth': 6, 'min_samples_leaf': 1, 'min_samples_split': 2}\n","Best estimator:  DecisionTreeClassifier(max_depth=6, random_state=0)\n","Best score:  0.977061310782241\n"]}],"source":["# DecisionTree opt\n","\n","\n","#optimization\n","param_grid={\n","\"max_depth\":[2,4,6],\n","\"min_samples_split\":[2,5,10],\n","\"min_samples_leaf\":[1,2,4]}\n","grid_search= GridSearchCV(estimator=clf,param_grid=param_grid,cv=5)\n","\n","grid_search.fit(X_train,y_train)\n","print(\"Best hyper-param: \",grid_search.best_params_ )\n","print(\"Best estimator: \",grid_search.best_estimator_ )\n","print(\"Best score: \",grid_search.best_score_ )"]},{"cell_type":"code","execution_count":105,"metadata":{},"outputs":[{"data":{"text/plain":["KNeighborsClassifier(n_neighbors=9)"]},"execution_count":105,"metadata":{},"output_type":"execute_result"}],"source":["# KNN\n","#importing the classfier\n","\n","kCls=KNeighborsClassifier(n_neighbors=9)\n","kCls.fit(X_train,y_train)"]},{"cell_type":"code","execution_count":106,"metadata":{},"outputs":[],"source":["# GradientBoostingClassifier\n","#importing the classfier\n","\n","gb_clf2 = GradientBoostingClassifier(n_estimators=20, learning_rate=0.1, max_features=4, max_depth=2, random_state=0)\n","gb_clf2.fit(X_train, y_train)\n","predictions = gb_clf2.predict(X_test)"]},{"cell_type":"code","execution_count":107,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\reham\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=4.\n","  warnings.warn(\n","c:\\Users\\reham\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","c:\\Users\\reham\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","c:\\Users\\reham\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n"]}],"source":["#LogisticRegressionCV\n","lr_l1 = LogisticRegressionCV(Cs=10, cv=4, penalty='l1', solver='liblinear').fit(X_train, y_train)"]},{"cell_type":"code","execution_count":108,"metadata":{},"outputs":[],"source":["#RandomForestClassifier\n","RF= RandomForestClassifier(criterion=\"gini\",\n","                           max_depth=8,\n","                           min_samples_split=10,\n","                           random_state= 200)"]},{"cell_type":"code","execution_count":109,"metadata":{},"outputs":[],"source":["\n","def report( y_test, pred ):\n","    #report \n","    print(classification_report(y_test,pred,target_names=['0','1','2','3','4','5']))\n","\n","\n","def confusionMatrix():\n","    #confusion_matrix\n","    #the result will show how mwny sucessful predition and wrong from each class\n","\n","    cm = confusion_matrix(y_test, pred)\n","    plt.figure(figsize=(10,7))\n","\n","    sns.heatmap(cm, annot=True, fmt='.2g', cmap='Blues')\n","\n","    # TN   FP\n","    # FN   TP"]},{"cell_type":"code","execution_count":110,"metadata":{},"outputs":[{"ename":"KeyError","evalue":"DecisionTreeClassifier(random_state=0)","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)","File \u001b[1;32mc:\\Users\\reham\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3620\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3621\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n","File \u001b[1;32mc:\\Users\\reham\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n","File \u001b[1;32mc:\\Users\\reham\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n","File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n","File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n","\u001b[1;31mKeyError\u001b[0m: DecisionTreeClassifier(random_state=0)","\nThe above exception was the direct cause of the following exception:\n","\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[1;32mc:\\Users\\reham\\OneDrive\\Desktop\\git\\Cars-Ratings-ML-2\\Models.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/reham/OneDrive/Desktop/git/Cars-Ratings-ML-2/Models.ipynb#X11sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     metrics \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat(metrics, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/reham/OneDrive/Desktop/git/Cars-Ratings-ML-2/Models.ipynb#X11sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     metrics\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/reham/OneDrive/Desktop/git/Cars-Ratings-ML-2/Models.ipynb#X11sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m PredictionAndPerformance()\n","\u001b[1;32mc:\\Users\\reham\\OneDrive\\Desktop\\git\\Cars-Ratings-ML-2\\Models.ipynb Cell 14\u001b[0m in \u001b[0;36mPredictionAndPerformance\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/reham/OneDrive/Desktop/git/Cars-Ratings-ML-2/Models.ipynb#X11sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m y_pred \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat(y_pred, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/reham/OneDrive/Desktop/git/Cars-Ratings-ML-2/Models.ipynb#X11sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m models:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/reham/OneDrive/Desktop/git/Cars-Ratings-ML-2/Models.ipynb#X11sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39m# Preciision, recall, f-score from the multi-class support function\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/reham/OneDrive/Desktop/git/Cars-Ratings-ML-2/Models.ipynb#X11sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     precision, recall, fscore, _ \u001b[39m=\u001b[39m score(y_test, y_pred[i], average\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mweighted\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/reham/OneDrive/Desktop/git/Cars-Ratings-ML-2/Models.ipynb#X11sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39m# The usual way to calculate accuracy\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/reham/OneDrive/Desktop/git/Cars-Ratings-ML-2/Models.ipynb#X11sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     accuracy \u001b[39m=\u001b[39m accuracy_score(y_test, y_pred[i])\n","File \u001b[1;32mc:\\Users\\reham\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3504\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3505\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3506\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3507\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n","File \u001b[1;32mc:\\Users\\reham\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3621\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3623\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3624\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3625\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3626\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3627\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3628\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n","\u001b[1;31mKeyError\u001b[0m: DecisionTreeClassifier(random_state=0)"]}],"source":["\n","\n","models=[clf2,kCls,gb_clf2,gb_clf2,lr_l1,RF]\n","\n","labels=['DecisionTreeClassifier','KNeighborsClassifier','GradientBoostingClassifier','LogisticRegressionCV','RandomForestClassifier']\n","\n","metrics=[]\n","\n","\n","def PredictionAndPerformance():\n","    y_pred =[]\n","    \n","    for lab,mod in zip(labels, models):\n","        y_pred.append(pd.Series(mod.predict(X_test), name=lab))\n","\n","    y_pred = pd.concat(y_pred, axis=1)\n","\n","    for i in models:\n","        # Preciision, recall, f-score from the multi-class support function\n","        precision, recall, fscore, _ = score(y_test, y_pred[i], average='weighted')\n","\n","        # The usual way to calculate accuracy\n","        accuracy = accuracy_score(y_test, y_pred[i])\n","        metrics.append(pd.Series({'precision':precision, 'recall':recall,'fscore':fscore, 'accuracy':accuracy}))\n","\n","    metrics = pd.concat(metrics, axis=1)\n","    metrics\n","\n","PredictionAndPerformance()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
